---
layout: post
title:  "最近阅读文章小抄"
date: 2019-03-06 21:35:10 +0800
categories: notes
tags: nlp
author: Henryzhou
---

#### 2019.1.21--《NLP 领域的 ImageNet 时代到来：词嵌入「已死」，语言模型当立》

计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。



长期以来，词向量一直是自然语言处理的核心表征技术。然而，其统治地位正在被一系列令人振奋的新挑战所动摇，如：ELMo、ULMFiT 及 OpenAI transformer。这些方法因证明预训练的语言模型可以在一大批 NLP 任务中达到当前最优水平而吸引了很多目光。这些方法预示着一个分水岭：它们在 NLP 中拥有的影响，可能和预训练的 ImageNet 模型在计算机视觉中的作用一样广泛。



当前常见的NLP任务:

+ 阅读理解是回答关于一个段落自然语言问题的任务。该任务最流行的数据集是 Stanford Question Answering Dataset (SQuAD)，其中包含了超过 10 万个问答对，并通过突出显示段落中的几个单词来让模型回答一个问题
+ 自然语言推理是识别一段文本和一个假设之间关系（蕴涵、矛盾和中性等）的任务。该任务最流行的数据集是 Stanford Natural Language Inference (SNLI) Corpus，包含 57 万个人类写的英语句子对。
+ 机器翻译，即将文本从一种语言转换到另一种语言，是 NLP 中研究最充分的任务之一。并且多年来，人们为常用的语言对累积了大量的训练数据，例如 WMT2014 的 4 千万个英语法语句子对。
+ 选区解析（Constituency parsing）以（线性化）解析树的形式提取句子的句法结构，如下图所示。在过去，人们在该任务中使用数百万个弱标记解析来训练序列到序列的模型。
+ 语言建模（LM）在给定前一个单词的情况下尝试预测下一个单词。已有的基准数据集由大约 10 亿个单词构成，但由于该任务是无监督的，因此可以使用任意数量的单词来训练。常见的数据集是由维基百科文章构成的WikiText-2 数据集。
+ 文本分类
+ 问答系统
+ 序列标注





#### 2019.1.21--《Transformer 一统江湖：自然语言处理三大特征抽取器比较》

+ **为何 RNN 能够成为解决 NLP 问题的主流特征抽取器**
+ **RNN 系面临的严重问题**
  + RNN 本身的序列依赖结构对于大规模并行计算来说相当之不友好。
+ **如何改造 RNN 使其具备并行计算能力？**
  + SRU（Simple Recurrent Units for Highly Parallelizable Recurrence）把隐层之间的神经元依赖由全连接改成了哈达马乘积，这样 T 时刻隐层单元本来对 T-1 时刻所有隐层单元的依赖，改成了只是对 T-1 时刻对应单元的依赖，于是可以在隐层单元之间进行并行计算
  + Sliced RNN：部分打断隐层之间的连接以实现不同时间步输入之间进行并行计算，比如每隔 2 个时间步打断一次，通过加深层深来建立远距离特征之间的联系。
+ **CNN 的进化：如何增加CNN提取长期依赖的能力**
  + Dilated CNN：卷积核可以进行跳步覆盖更大的区域
  + 将卷积的深度提高，高层的卷积层能更宏观的覆盖序列

两个NLP领域的语言：1.Bert 这种两阶段的模式（预训练 + Finetuning）必将成为 NLP 领域研究和工业应用的流行方法；2.从 NLP 领域的特征抽取器角度来说，Transformer 会逐步取代 RNN 成为最主流的的特征抽取器。



如果对目前 **NLP 里的三大特征抽取器的未来走向趋势**做个宏观判断的话，我的判断是这样的：

- **RNN** 人老珠黄，已经基本完成它的历史使命，将来会逐步退出历史舞台；
- **CNN** 如果改造得当，将来还是有希望有自己在 NLP 领域的一席之地，如果改造成功程度超出期望，那么还有一丝可能作为割据一方的军阀，继续生存壮大，当然我认为这个希望不大，可能跟宋小宝打篮球把姚明打哭的概率相当；
- 而新欢 **Transformer** 明显会很快成为 NLP 里担当大任的最主流的特征抽取器。

*注：Recursive NN和Memory Network这两种技术不被看好*



通常而言，绝大部分 NLP 问题可以归入上图所示的**四类任务**中：

- 一类是**序列标注**，这是最典型的 NLP 任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
- 第二类是**分类任务**，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
- 第三类任务是**句子关系判断**，比如 Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
- 第四类是**生成式任务**，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。



身为资深 Bug 制造者和算法工程师，你现在需要做的事情就是：选择一个好的特征抽取器，选择一个好的特征抽取器，选择一个好的特征抽取器，喂给它大量的训练数据，设定好优化目标（loss function），告诉它你想让它干嘛…….. 然后你觉得你啥也不用干等结果就行了是吧？那你是我见过的整个宇宙中最乐观的人……. 你大量时间其实是用在调参上…….。从这个过程可以看出，如果我们有个强大的特征抽取器，那么中初级算法工程师沦为调参侠也就是个必然了，在 AutoML（自动那啥）流行的年代，也许以后你想当调参侠而不得，李斯说的 “吾欲与若复牵黄犬，俱出上蔡东门逐狡兔，岂可得乎！” 请了解一下。所以请珍惜你半夜两点还在调整超参的日子吧，因为对于你来说有一个好消息一个坏消息，好消息是：对于你来说可能这样**辛苦**的日子不多了！坏消息是：对于你来说可能这样辛苦的日子**不多了**！！！那么怎么才能成为算法高手？你去设计一个更强大的特征抽取器呀。

