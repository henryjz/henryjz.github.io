<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>深入解析torch.nn</title>
    <meta name="description" content="  转载自：https://blog.csdn.net/weixin_36811328/article/details/87905208原文地址：WHAT IS TORCH.NN REALLY?本人英语学渣，如有错误请及时指出以便更正，使用的源码可点击原文地址进行下载。pytorch提供了许多优雅的类和模块帮助我...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>深入解析torch.nn</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2019-03-12
            </div>

            <div class="label-card">
                <i class="fa fa-user"></i>Henryzhou
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a-->
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <blockquote>
  <p>转载自：https://blog.csdn.net/weixin_36811328/article/details/87905208</p>
</blockquote>

<p>原文地址：<a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#sphx-glr-download-beginner-nn-tutorial-py">WHAT IS TORCH.NN REALLY?</a>
本人英语学渣，如有错误请及时指出以便更正，使用的源码可点击原文地址进行下载。</p>

<hr />

<p>pytorch提供了许多优雅的类和模块帮助我们构建与训练网络，比如 <code class="highlighter-rouge">torch.nn</code>, <code class="highlighter-rouge">torch.optim</code>,<code class="highlighter-rouge">Dataset</code>等。为了充分利用这些模块的功能，灵活操作它们解决各种不同的问题，我们需要更好地理解当我们调用这些模块时它们到底干了些什么，为此，我们首先不调用这些模块实现<strong>MNIST</strong>手写字识别，仅使用最基本的 pytorch 张量函数。然后，我们逐渐增加 <code class="highlighter-rouge">torch.nn</code>, <code class="highlighter-rouge">torch.optim</code>, <code class="highlighter-rouge">Dataset</code>, or <code class="highlighter-rouge">DataLoader</code>,具体地展示每个模块具体干了些什么，展示这些模块是怎样使代码变得更加优雅灵活。
<strong>此教程适用范围：熟悉pytorch的张量操作</strong></p>

<h1 id="加载-mnist-数据集">加载 MNIST 数据集</h1>

<p>我们使用经典的 <code class="highlighter-rouge">MNIST</code> 数据集，一个包含了0-9数字的二值图像库。</p>

<p>还会用到 <code class="highlighter-rouge">pathlib</code> 库用于目录操作，一个python3自带的标准库。使用 <code class="highlighter-rouge">requests</code> 下载数据集。当用到一个模块时才会进行导入，而不会一开始全部导入，以便更好地理解每个步骤。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'data'</span><span class="p">)</span>
<span class="n">PATH</span> <span class="o">=</span> <span class="n">DATA_PATH</span> <span class="o">/</span> <span class="s">"mnist"</span>

<span class="n">PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">exit_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">URL</span> <span class="o">=</span> <span class="s">"http://deeplearning.net/data/mnist/"</span>
<span class="n">FILENAME</span> <span class="o">=</span> <span class="s">"mnist.pkl.gz"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
	<span class="n">content</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span> <span class="o">+</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
	<span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"wb"</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
<span class="mi">1234567891011121314</span>
</code></pre></div></div>

<p>该数据集采用numpy数组格式，并使用pickle存储，pickle是一种特定于python的格式，用于序列化数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">gzip</span>

<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="nb">open</span><span class="p">((</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">(),</span><span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
	<span class="p">((</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),(</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span><span class="p">),</span><span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="s">"latin-1"</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>每张训练图片分辨率为 28x28， 被存储为 784(=28x28) 的一行。我们输出看一下数据，首先需要转换回 28x28的图像。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">form</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span><span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p><img src="https://img-blog.csdnimg.cn/20190224165051244.png" alt="img" /></p>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50000,784)
1
</code></pre></div></div>

<p>PyTorch使用 torch.tensor ，所以我们需要对numpy类型数据进行转换</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
	<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span><span class="p">)</span>
	<span class="p">)</span>
<span class="n">n</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y_train</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span><span class="n">y_train</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span><span class="n">y_train</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="mi">123456789</span>
</code></pre></div></div>

<h1 id="从头创建神经网络不使用torchnn">从头创建神经网络（不使用torch.nn）</h1>

<p>让我们仅仅使用 pytorch 中的张量操作来创建模型，假设你已经熟悉神经网络的基础知识（不熟悉请参考<a href="https://course.fast.ai/">corse.fast.ai</a> ）</p>

<p>pytorch提供了很多创建张量的操作，我们将用这些方法来初始化权值weights和偏置 bais来创建一个线性模型。这些只是常规张量，有一个非常特别的补充：我们告诉PyTorch这些张量需要支持求导(requires_grad=True)。这样PyTorch将记录在张量上完成的所有操作，以便它可以在反向传播过程中自动计算梯度！</p>

<p>对于权值weights，我们再初始化<strong>之后</strong>再设置 <code class="highlighter-rouge">requires_grad</code>,因为我们不想这一步包含在梯度的计算中（注：pytorch中以 <code class="highlighter-rouge">_</code> 结尾的操作都是在原变量中(in-place)执行的）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">780</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>多亏了pytorch的自动求导功能，我们可以使用python的所有标准函数来构建模型。 我们这儿利用矩阵乘法，加法来构建线性模型。我们编写 <code class="highlighter-rouge">log_softmax</code>函数作为激活函数。 虽然pytorch提供了大量写好的损失函数，激活函数，你依然可以自由地编写自己的函数替代它们。 pytorch 甚至支持创建自己的 GPU函数或者CPU矢量函数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>           <span class="c1"># python的广播机制
</span><span class="mi">12345</span>
</code></pre></div></div>

<p>上面的 <code class="highlighter-rouge">@</code> 符号表示向量的点乘，接下来我们会调用一批数据（batch，64张图片）输入此模型。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span> 					<span class="c1"># batch size
</span><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>    		<span class="c1"># a mini-batch from x
</span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>   		<span class="c1"># predictions
</span><span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-2.4513, -2.5024, -2.0599, -3.1052, -3.2918, -2.2665, -1.9007, -2.2588,
        -2.0149, -2.0287], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])
12
</code></pre></div></div>

<p>正如我们看到的，<code class="highlighter-rouge">preds</code> 张量不仅包含了一组张量，还包含了求导函数。反向传播的时候会用到此函数。让我们使用标准的python语句接着来实现 negative log likelihood loss 损失函数（译者加：也被称为交叉熵损失函数）：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">target</span><span class="p">):</span>
	<span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nll</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>现在用我们的损失函数来检查我们随机初始化的模型，待会就能看到再反向传播之后是否会改善模型性能。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.3620, grad_fn=&lt;NegBackward&gt;)
1
</code></pre></div></div>

<p>接下来定义一个计算准确度的函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span><span class="err">：</span>
	<span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># 得到最大值的索引
</span>	<span class="k">return</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">yb</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>检查模型的准确度：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0938)
1
</code></pre></div></div>

<p>现在我们开始循环训练模型，每一步我们执行以下操作：</p>

<ul>
  <li>选择一批数据（a batch）</li>
  <li>使用模型进行预测</li>
  <li>计算损失函数</li>
  <li>反向传播更新参数 weights 和 bias</li>
</ul>

<p>我们现在使用 <code class="highlighter-rouge">torch.no_grad()</code> 更新参数，以避免参数更新过程被记录入求导函数中。</p>

<p>然后我们清零导数，以便开始下一轮循环，否则导数会在原来的基础上累加，而非替代原来的数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.core.debugger</span> <span class="kn">import</span> <span class="n">set_trace</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># learning rate
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># how many epochs to train for
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#         set_trace()
</span>        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="mi">123456789101112131415161718192021</span>
</code></pre></div></div>

<p>目前为止，我们从头创建一个迷你版的神经网络</p>

<p>让我们来检查一下损失和准确率，并于迭代更新参数之前进行比较，我们期望得到更小的损失于更高的准确率。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0822, grad_fn=&lt;NegBackward&gt;) tensor(1.)
1
</code></pre></div></div>

<h1 id="使用-torchnnfunctional-简化代码">使用 torch.nn.functional 简化代码</h1>

<p>现在我们使用<code class="highlighter-rouge">torch.nn.functional</code>重构之前的代码，这样会使代码变得更加简洁与灵活，更易理解。</p>

<p>首先最简单的一步是，用 <code class="highlighter-rouge">torch.nn.functional</code>( 为了方便后面统一称作F) 中带有的损失函数来代替我们自己编写的函数，使得代码变得更简短。这些函数都包包含于模块 <code class="highlighter-rouge">torch.nn</code>里面，除了大量的损失函数与激活函数，里面还包含了大量用于构建网络的函数。</p>

<p>如果我们的网络中使用 negative log likelihood loss 作为损失函数， log softmax activation 作为激活函数 （即我们上面实现的损失函数与激活函数）。在pytorch中我们直接使用函数 <code class="highlighter-rouge">F.cross_entropy</code> 便可实现上面两个函数的功能。所以我们可以用此函数代替上面实现的激活函数与损失函数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
<span class="mi">123456</span>
</code></pre></div></div>

<p>让我测试一下是否和上面自己实现的函数效果一致：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)
1
</code></pre></div></div>

<h1 id="引入-nnmodule-重构代码">引入 nn.Module 重构代码</h1>

<p>接下来我们引入 <code class="highlighter-rouge">nn.Module</code>和<code class="highlighter-rouge">nn.Parameter</code> 改进代码。我们创建 <code class="highlighter-rouge">nn.Module</code>的子类。这个例子中我们创建一个包含权重，偏置，以及包含前向传播的类。<code class="highlighter-rouge">nn.Module</code>含有许多的属性与方法可供调用 （比如： <code class="highlighter-rouge">.parameters</code> <code class="highlighter-rouge">.zero_grad()</code>）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="n">sefl</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">xb</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
<span class="mi">12345678910</span>
</code></pre></div></div>

<p>接下来实例化我们的模型：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>现在我们可以和之前一样使用损失函数了。注意：<code class="highlighter-rouge">nn.Module</code> 对象可以像函数一样调用，但实际上是自动调用了对象内部的函数 <code class="highlighter-rouge">forward</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.2082, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>在之前，我们必须进行如下得操作对权重，偏置进行更新，梯度清零：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
	<span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>现在我们可以充分利用 <code class="highlighter-rouge">nn.Module</code> 的方法属性更简单地完成这些操作，如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>现在我们将整个训练过程写进函数 <code class="highlighter-rouge">fit</code>中。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
			<span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
			<span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
			<span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
			<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
			<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span>
		
			<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
			<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
				<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
				<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">fit</span><span class="p">()</span>
<span class="mi">123456789101112131415</span>
</code></pre></div></div>

<p>让我们再一次确认损失情况：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0812, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入-nnlinear-重构代码">引入 nn.Linear 重构代码</h1>

<p>比起手动定义 权重 与 偏置，并且使用 <code class="highlighter-rouge">self.weights</code>和 <code class="highlighter-rouge">self.bias</code> 来计算 <code class="highlighter-rouge">xb @ self.weights + self.bias</code>的方式，我们可以使用pytorch中的 <code class="highlighter-rouge">nn.Linear</code>来定义线性层，他自动为我们实现以上权重参数的定义以及计算的过程。除了线性模型之外，pytorch还有一系列的其它网络层供我们使用，大大简化了我们的编程过程。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">xb</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">1234567</span>
</code></pre></div></div>

<p>同上面一样实例化模型，计算损失</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.2731, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>训练，并查看训练之后的损失</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0820, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入-optim-重构代码">引入 optim 重构代码</h1>

<p>接下来使用<code class="highlighter-rouge">torch.optim</code>改进训练过程，而不用手动更新参数</p>

<p>之前的手动优化过程如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>使用如下代码替代手动的参数更新：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="c1"># optim.zero_grad() resets the gradient to 0 and we need to call it 
# before computing the gradient for the next minibatch.
</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
<span class="mi">1234</span>
</code></pre></div></div>

<p>结合之前的完整跟新代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">get_model</span><span class="p">():</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
	<span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
		<span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span><span class="n">bs</span>
		<span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
		<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
		<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span>

		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1234567891011121314151617181920212223</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.3785, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0802, grad_fn=&lt;NllLossBackward&gt;)
12
</code></pre></div></div>

<h1 id="引入-dataset-处理数据">引入 Dataset 处理数据</h1>

<p>pytorch定义了 Dataset 类，其中主要包含了 <code class="highlighter-rouge">__len__</code> 函数与 <code class="highlighter-rouge">__getitem__</code>函数。<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">此教程</a>以创建 <code class="highlighter-rouge">FacialLandmarkDataset</code> 为例详细地介绍了Dataset类的使用。</p>

<p>pytorch的 <code class="highlighter-rouge">TensorDataset</code> 是一个包含张量的数据集。通过定义长度索引等方式，使我们更好地利用索引，切片等方法迭代数据。这会让我们很容易地在一行代码中获取我们地数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">form</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span> <span class="kn">import</span> <span class="nn">TensorDataset</span>
<span class="mi">1</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">x_train</code> <code class="highlighter-rouge">y_train</code>可以被组合进一个<code class="highlighter-rouge">TensorDataset</code>中，这会使得迭代切片更加简单。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>之前我们获取数据的方法如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>现在我们可以使用更简单的方法：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="o">+</span><span class="n">bs</span><span class="p">]</span>
<span class="mi">1</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bs</span><span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">+</span> <span class="n">bs</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">12345678910111213</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入dataloader加载数据">引入DataLoader加载数据</h1>

<p><code class="highlighter-rouge">DataLoader</code> 用于批量加载数据，你可以用他来加载任何来自 <code class="highlighter-rouge">Dataset</code>的数据，它使得数据的批量加载十分容易。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>之前我们读取数据的方式：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="o">+</span><span class="n">bs</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>现在使用dataloader加载数据：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">12</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">123456789101112</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>目前为止训练模型部分我们就已经完成了，通过使用<code class="highlighter-rouge">nn.Module</code>, <code class="highlighter-rouge">nn.Parameter</code>, <code class="highlighter-rouge">DataLoader</code>, 我们的训练模型以及得到了很大的改进。接下来让我们开始模型的测试部分。</p>

<h1 id="添加测试集">添加测试集</h1>

<p>在前一部分，我们尝试了使用训练集训练网络。实际工作中，我们还会使用测试集来观察训练的模型是否过拟合。</p>

<p>打乱数据的分布有助于减小每一批(batch)数据间的关联，有利于模型的泛化。但对于测试集来说，是否打乱数据对结果并没有影响，反而会花费多余的时间，所以我们没有必要打乱测试集的数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">bs</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>在每训练完一轮数据（epoch）后我们输出测试得到的损失值。
(注：如下代码中，我们调用<code class="highlighter-rouge">model.train()</code>和<code class="highlighter-rouge">model.eval</code>表示进入训练模式与测试模式，以保证模型运行的准确性)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">,</span><span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
	<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
	<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
		<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
		
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

	<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
		<span class="n">valid_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">)</span>

	<span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">valid_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">))</span>
<span class="mi">1234567891011121314151617</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 tensor(0.3456)
1 tensor(0.2988)
12
</code></pre></div></div>

<h1 id="创建-fit-和-get_data-优化代码">创建 fit() 和 get_data() 优化代码</h1>

<p>我们再继续做一点改进。因为我们再计算训练损失和验证损失时执行了两次相同的操作，所以我们用一个计算每一个batch损失的函数封装这部分代码。</p>

<p>我们为训练集添加优化器，并执行反向传播。对于训练集我们不添加优化器，当然也不会执行反向传播。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span> <span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
	<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">)</span>

	<span class="k">if</span> <span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
	
	<span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">123456789</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">fit</code>执行每一个epoch过程中训练和验证的必要操作</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
		<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
			<span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
		
		<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
			<span class="n">losses</span><span class="p">,</span> <span class="n">nums</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
				<span class="o">*</span><span class="p">[</span><span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">]</span>
			<span class="p">)</span>
		<span class="n">val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span><span class="o">.</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">nums</span><span class="p">))</span>

		<span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">)</span>
<span class="mi">12345678910111213141516</span>
</code></pre></div></div>

<p>现在，获取数据加载模型进行训练的整个过程只需要三行代码便能实现了</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="n">fit</span><span class="p">(</span><span class="n">epoches</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.2961075816631317
1 0.28558296990394594
12
</code></pre></div></div>

<p>我们可以用这简单的三行代码训练各种模型。下面让我们看看怎么用它训练一个卷积神经网络。</p>

<h1 id="使用卷积神经网络">使用卷积神经网络</h1>

<p>现在我们用三个卷积层来构造我们的卷积网络。因为之前的实现的函数都没有假定模型形式，这儿我们依然可以使用它们而不需要任何修改。</p>

<p>我们pytorch预定义的<code class="highlighter-rouge">Conv2d</code>类来构建我们的卷积层。我们模型有三层，每一层卷积之后都跟一个 ReLU，然后跟一个平均池化层。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="mi">12345678910111213141516</span>
</code></pre></div></div>

<p>动量momentum是随机梯度下降的一个参数，它考虑到了之前的梯度值使得训练更快。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_CNN</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.3829730714321136
1 0.2258522843360901
12
</code></pre></div></div>

<h1 id="使用-nnsequential-搭建网络">使用 nn.Sequential 搭建网络</h1>

<p><code class="highlighter-rouge">torch.nn</code>还有另外一个方便的类可以简化我们的代码：<code class="highlighter-rouge">Sequential</code>, 一个<code class="highlighter-rouge">Sequential</code>对象</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Lambda</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="mi">12345678910</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">Sequential</code>是一种简化代码的好方法。 一个<code class="highlighter-rouge">Sequential</code>对象按顺序执行包含在内的每一个module，使用它可以很方便地建立一个网络。</p>

<p>为了更好地使用<code class="highlighter-rouge">Sequential</code>模块，我们需要自定义 pytorch中没实现地module。例如pytorch中没有自带 改变张量形状地层，我们创建 <code class="highlighter-rouge">Lambda</code>层，以便在<code class="highlighter-rouge">Sequential</code>中调用。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="n">preprocess</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">123456789101112131415</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.32739396529197695
1 0.25574398956298827
12
</code></pre></div></div>

<h1 id="简易的dataloader">简易的DataLoader</h1>

<p>我们的网络以及足够精简了，但是只能适用于MNIST数据集，因为</p>

<ul>
  <li>网络默认输入为 28x28 的张量</li>
  <li>网络默认最后一个卷积层大小为 4x4 （因为我们的池化层大小为4x4）</li>
</ul>

<p>现在我们去除这两个假设，使得网络可以适用于所有的二维图像。首先我们移除最初的 <code class="highlighter-rouge">Lambda</code>层，用数据预处理层替代。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">WrappedDataLoader</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">dl</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

	<span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">batches</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
			<span class="k">yield</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="p">))</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="mi">12345678910111213141516171819</span>
</code></pre></div></div>

<p>然后，我们使用<code class="highlighter-rouge">nn.AdaptiveAvgPool2d</code>代替<code class="highlighter-rouge">nn.AvgPool2d</code>。它允许我们自定义输出张量的维度，而于输入的张量无关。这样我们的网络便可以适用于各种size的网络。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernal_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
	<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="mi">123456789101112</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.32888883714675904
1 0.31000419993400574
12
</code></pre></div></div>

<h1 id="使用gpu">使用GPU</h1>

<p>如果你的电脑有支持CUDA的GPU（你可以很方便地以 0.5美元/小时 的价格租到支持的云服务器），便可以使用GPU加速训练过程。首先检测设备是否正常支持GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ture
1
</code></pre></div></div>

<p>接着创建一个设备对象：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
	<span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>更新 <code class="highlighter-rouge">preprocess(x,y)</code>把数据移到GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="mi">123456</span>
</code></pre></div></div>

<p>最后移动网络模型到GPU：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>进行训练，能发现速度快了很多：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.21190375366210937
1 0.18018000435829162
12
</code></pre></div></div>

<h1 id="总结">总结</h1>

<p>我们现在得到了一个通用的数据加载和模型训练方法，我们可以在pytorch种用这种方法训练大多的模型。想知道训练一个模型有多简单，回顾一下本次的代码便可以了。</p>

<p>当然，除此之外本篇内容还有很多需求没有讲到，比如数据增强，超参调试，数据监控(monitoring training),迁移学习等。这些特点都以与本篇教程相似的设计方法包含于 fastai库中。</p>

<p>本篇教程开头我们承诺将会通过例程解释 <code class="highlighter-rouge">torch.nn</code> <code class="highlighter-rouge">torch.optim</code> <code class="highlighter-rouge">Dataset</code> <code class="highlighter-rouge">DataLoader</code>等模块，下面我们就这些模型进行总结。</p>

<ul>
  <li>torch.nn
    <ul>
      <li>Module: 创建一个可以像函数一样调用地对象，包含了网络的各种状态，可以使用<code class="highlighter-rouge">parameter</code>方便地获取模型地参数，并有清零梯度，循环更新参数等功能。</li>
      <li>Parameter: 将模型中需要更新的参数全部打包，方便反向传播过程中进行更新。有 <code class="highlighter-rouge">requires_grad</code>属性的参数才会被更新。</li>
      <li>functional：通常导入为<code class="highlighter-rouge">F</code>，包含了许多激活函数，损失函数等。</li>
    </ul>
  </li>
  <li>torch.optim: 包含了很多诸如<code class="highlighter-rouge">SGD</code>一样的优化器，用来在反向传播中跟新参数</li>
  <li>Dataset: 一个带有 <code class="highlighter-rouge">__len__</code> <code class="highlighter-rouge">__getitem__</code>等函数的抽象接口。里面包含了 <code class="highlighter-rouge">TensorDataset</code>等类。</li>
  <li>DataLoader: 输入任意的 <code class="highlighter-rouge">Dataset</code> 并按批(batch)迭代输出数据。</li>
</ul>

        </article>
        <hr>

        
        
            
            
                
                    
                        
                        <h2 id="similar_posts">Similar Posts</h2>
                        <ul>
                        
                        <li class="relatedPost">
                            <a href="/2019/03/13/fine-tuning/">使用pytorch进行fine-tune
                            
                            </a>
                        </li>
                        
                        
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                        
                        <li class="relatedPost">
                            <a href="/2019/03/11/Pytorch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Pytorch环境搭建
                            
                            </a>
                        </li>
                        
                        
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
        
            
            
                
                    
                
            
        
        
            </ul>
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/">爬虫环境安装</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2019/03/13/fine-tuning/">使用pytorch进行fine-tune</a></p>
        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        


<div id="disqus_thread"></div>
<script>
    /**
     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function() {
        this.page.url = 'http://localhost:4000/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://localhost:4000/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document,
            s = d.createElement('script');

        s.src = '//henry.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#similar_posts">Similar Posts</a></li>
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
