<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/page2/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
    <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
    	tex2jax: {
      	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      	inlineMath: [['$','$']]
    	}
  	});
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/">华为李航-NLP有个基本问题，深度学习4个做的很好</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>对于自然语言理解，有两种定义。第一种是计算机能够将所说的语言映射到计算机内部表示；另一种是基于行为的，你说了一句话，计算机做出了相应行为，就认为计算机理解了自然语言。后者的定义，更广为采用。</p>

<p>为什么自然语言理解很难？其本质原因是语言是一种复杂的现象。自然语言有 5 个重要特点，使得计算机实现自然语言处理很困难：</p>

<ol>
  <li>语言是不完全有规律的，规律是错综复杂的。有一定的规律，也有很多例外。因为语言是经过上万年的时间发明的，这一过程类似于建立维基百科。因此，一定会出现功能冗余、逻辑不一致等现象。但是语言依旧有一定的规律，若不遵循一定的规范，交流会比较困难；</li>
  <li>语言是可以组合的。语言的重要特点是能够将词语组合起来形成句子，能够组成复杂的语言表达；</li>
  <li>语言是一个开放的集合。我们可以任意地发明创造一些新的表达。比如，微信中 “潜水” 的表达就是一种比喻。一旦形成之后，大家都会使用，形成固定说法。语言本质的发明创造就是通过比喻扩展出来的；</li>
  <li>语言需要联系到实践知识；</li>
  <li>
    <p>语言的使用要基于环境。在人与人之间的互动中被使用。如果在外语的语言环境里去学习外语，人们就会学习得非常快，理解得非常深。</p>

    <p>这些现象都说明，在计算机里去实现与人一样的语言使用能力是一件非常具有挑战性的事情。首先，语言的不完全规律性和组合性，就意味着如果在目前的计算机上去实现，会产生组合爆炸；还有，如果需要语言做比喻，去联系到实践环境，就意味着要做全局的、穷举的计算。如果通过现代计算机来做，非常复杂，几乎不太可能。所以，如果想让计算机像人一样使用语言，原理上需要完全不同的、与人脑更接近的计算机体系<a href="http://lib.csdn.net/base/16" title="大型网站架构知识库">架构</a>。</p>
  </li>
</ol>

<p>其本质原因是，目前在计算机上去实现东西一定需要数学模型。换句话说，计算机能够做的事情要通过数学形式化。但是，到目前为止，语言的使用还不清楚是否能够用数学模型去刻画。<a href="http://lib.csdn.net/base/2" title="机器学习知识库">人工智能</a>的终极挑战就是自然语言理解。现实当中，不能因为自然语言理解非常困难就放弃。我们还是希望能够使计算机越来越智能化，能够部分使用语言。因此，就形成了所谓自然语言处理这一领域。我们叫自然语言处理，而不是自然语言理解，因为真正的理解是太难了。</p>

<p>自然语言处理做的第一件事情就是把问题简化。比如，知识问答中，问姚明身高是多少？朋友告诉你是 2 米 26。这是人与人之间的知识问答。那么，这其中有哪些步骤呢？首先是听，然后去理解问题，然后去做一定的推理，然后再去做信息检索，最后判断怎么去做回答，整个过程相当复杂。我们现在做自然语言处理时，也做这种知识问答，包括有名的 IBM 的 Watson，其整个步骤也是简化了自然语言处理的过程。一般而言，就是这几个步骤，先分析一下问句，接着去检索相关的知识或者信息，然后产生答案。</p>

<p>目前，所有的自然语言处理的问题都可以分类成为五大统计自然语言处理的方法或者模型，即分类、匹配、翻译、结构预测，马尔可夫决策过程。各种各样的自然语言处理的应用，都可以模型化为这五大基本问题，基本能够涵盖自然语言处理相当一部分或者大部分的技术。主要采用统计<a href="http://lib.csdn.net/base/2" title="机器学习知识库">机器学习</a>的方法来解决。第一是分类，就是你给我一个字符串，我给你一个标签，这个字符串可以是一个文本，一句话或者其他的自然语言单元；其次是匹配，两个字符串，两句话或者两段文章去做一个匹配，判断这两个字符串的相关度是多少；第三就是翻译，即更广义的翻译或者转换，把一个字符串转换成另外一个字符串；第四是结构预测，即找到字符串里面的一定结构；第五是马可夫决策过程，在处理一些事情的时候有很多状态，基于现在的状态，来决定采取什么样的行动，然后去判断下一个状态。我们也可以采用这样的模型，去刻画自然语言处理的一些任务。</p>

<p>分类主要有文本分类和情感分类，匹配主要有搜索、问题回答、对话（主要是单轮对话）；翻译主要有机器翻译，语音识别，手写识别，单轮对话；结构预测主要有专门识别，词性标注，句法分析，文本的语义分析；马可夫决策过程可以用于多轮对话。我们可以看到，自然语言处理里面有很多任务，在现实中我们已经开始使用最基本这五种最基本的模型它都去可以去刻画的。</p>

<p>语言处理，在一定程度上需要考虑技术上界和性能下界的关系。现在的自然语言处理，最本质是用数据驱动的方法去模拟人，通过人工智能闭环去逼近人的语言使用能力。但是，这种技术并没有真正实现人的语言理解机制。可能会有这样的情况，这个技术的准确率（绿线）画了一个上界。比如，语音识别的上届是 95%，我们希望不断把这个技术做好，比如通过人工智能闭环，更好的深度学习方法，从而使得上界不断提高。但是，不可能一下子达到百分之百对，或者达到完全与人一样的水平。每个应用，对于下界的要求是不一样的。比如，在葡萄牙问路，对方也不会英语，我也不会葡萄牙语，交流非常困难，在这种环境下我其实就是听懂几个单词，让机器翻译给我翻译几个单词就行了，对性能的要求其实是比较低的，不需要去翻译一大段话。我们可以看到，不同的应用，用户对使用性能的要求不同，如果下界达到这个水平，用户就用了。再比如互联网搜索中排序第一的准确率不高，60% 多 - 70% 多，大家往往觉得，互联网搜索引擎已经达到要求了。当然因为搜索的时候，通过排序展示给用户多个结果，用户可以去逐个去看，一定程度上解决一些问题，这时候对性能要求下界相对就比较低。如果，现在的技术上届达到了用户要求的下界，就能够使用。所以，哪些自然语言处理的技术未来能够起飞，能够真正实用化，就可以通过这种关系来看。还是要看具体的应用的场景。在一些特定场景下，准确率达到 99% 都不行。我们相信，自然语言处理的技术会不断提高，但是是不是都能够达到我们每一个应用要求的性能的下界，就不好说了，要看未来的发展了。这是自然语言处理技术整个发展情况。</p>

<p>下面，给大家一起看一下我们自然语言处理领域里面都有哪些技术，有代表性的技术都大概达到什么样的水平，都是什么样的一些基础。假设大家对深度学习有一定的了解，如果这方面的知识还不够也没关系，过后你可以去再去看书看论文，去了解一些相关的技术情况。</p>

<p>刚才，我介绍到站在一个很抽象的角度来看，自然语言处理就是五个问题。如果用各种方法包括深度学习把这五个问题做好了，就能够把自然语言做得很好。现实当中，我们就是通过深度学习，达到自然语言处理技术比较好的水平。</p>

<p>首先，问答系统有很多，包括 IBM 的 Watson 也是一个问答系统，有大量的知识或者信息放在知识库。典型的办法就是把问答用 FAQ 索引起来，与搜索引擎相似，如果来了一个新问题，有一大堆已经索引好的 FAQ，然后去做一个检索（字符上的匹配），之后逐个去做匹配，判断问句与回答的匹配如何。往往匹配的模型有多个，再去将候补做一个排序，把最有可能的答案排在前面，往往就取第一个作为答案返回给用户。</p>

<p>这里面牵扯到几个技术，我们在在线的时候要做匹配和排序，现在最先进的技术都是用机器学习，用深度学习技术。就是把问句和回答的可能的候选，用向量来表示，问句的每一个单词都可以用向量来表示。每一个词的语义都可以用一个实数值向量赖表示，问句和候补都是实数值向量的序列。然后，用一个二维的卷积神经网络来判断两句话在语义上是不是相关，候选是否是很好的答案。通过二维卷积神经网络，可以判断两句话里面哪一些词语、词组是可以相互对应，最后可以做一个判断这两句话是不是相关的。整个模型的学习通过大量的数据、句对，去训练。如果卷积神经网络的参数学好，就可以判断任何给定的两句话是不是能够构成一轮问答。</p>

<p>这样的模型不仅仅可以用到文本问答（知识问答）上，也可以用到图像检索上面。给大家演示一个 demo。</p>

<p>这种模型，可以跨模态的把文本和图片联系起来。在深度学习技术出现之前的话，这件事情是不可能的。因为他们是不同的模态。一个是符号表示的信息，一个是像素表示的信息，那么我们可以用深度学习的模型去做这种跨模态的匹配。比如，左边有一个卷积神经网络，他能够抽出左边图片的语意表示，表示成一个向量；右边也是一个卷积神经网络，能够把一段文字的内容抽取出来，表示成为一个向量，还有一个网络判断这两个向量在语义上是否能够匹配。这个模型可以通过大量的数据去训练。假设每一个照片有 3 到 5 个人给出描述。我们用大量这样的数据就可以学这样的神经网络，神经网络可以帮助我们，就是说任何给定的一句话，要去查找一个图片的内容，它就可以在这个图片库里帮你去匹配到最相关的图片，给你返回来。这个技术也是在深度学习出现之前应用的，因为我们不知道怎样把图片和文字匹配到一起。有了深度学习技术，我们可以做这样的事情。</p>

<p>自然语言对话是用另外一种技术，用生成式的模型去做自然语言对话。大量的聊天系统是这么做的，输入一句话，里面准备了大量的 FAQ，搜索到一个最相关的回答，反馈给你。这叫做基于检索的自然语言问答系统。</p>

<p>我们这里看到是一种产生式，经过大量数据训练之后，输入一句话系统自动的产生一个回复，理论上产生出无穷多的不同的回复。下面先看一个实际系统的录像。</p>

<p>这个系统，我们在微博上爬了四百万的微博数据，微博数据可以看作是一种简单的单轮对话。我们用 400 万数据训练了这样一个系统能够去自动产生对话。（系统演示）</p>

<p>理论上它可以回答任何你输入的对话。用 400 万的微博数据就可以训练这样一个模型。系统产生一句话的比例是 96%，真正形成一个有意义的单轮对话的比例是 76% 左右。这个系统的一大特点是，可以回答没有见过的一句话。</p>

<p>第二个特点是能够记住训练数据。发现深度网络有一个共同的特点就是能够记住训练数据，同时也有去泛化的能力，能针对未知的新见到的东西去自动组织出一句话，并返回给你。这种能力很令人惊叹，是否实用并不清楚。在一个很固定的场景里，比如话务中心，如果话务员跟客户之间的交互都是简单的重复，大量的类似数据可以构建一个产生自动的回复系统，而且跟人的回复非常接近。</p>

<p>大家如果熟悉深度学习的话，刚才说自然语言处理有很多问题都是翻译的问题，即把一个文字的字符翻译成另外一个文字字符，那么单轮对话的产生也可以看成是机器翻译。序列对序列学习，sequenceto sequence learning，可以用到这种单轮对话中。每个单词其实是用一个实数值向量表示，就是编码，之后用实数值向量分解成一个回复的一句，叫做解码。通过这种编码、解码这两个过程的话，我们把原始的数据转化成中间表示，再把中间表示，转换成为应该回复的话，可以产生对话系统。</p>

<p>谷歌的神经机器翻译系统是一个非常强大的系统，需要很多训练数据和强大计算资源。这个 seqto seq 模型有八层的编码器和八层的解码器，整个网络非常深。它还用了各种这个新的技术，比如注意力技术，并行处理技术，还有模型分割和数据分割等。目前，翻译的准确率已经超过了传统的统计机器翻译。</p>

<p>下面，再看一下未来自然语言处理技术发展的前景和趋势。刚才我们看到技术上界和用户对于性能要求的下界，碰到一起就看到技术的使用化。那么，就预测一下未来自然语言处理技术的发展。目前，几个最基本的应用，包括语音识别，就是一个序列对序列学习的问题，就是翻译的问题，目前准确率是 95% 左右，那么已经比较实用了。单轮对话往往可以变成一个分类问题，或者结构预测问题，就是通过手写一些规则或者建一些分类器，可以做的比较准。很多手机上应用或者是语音助手像 siri，就是用这样的技术；多轮对话还很不成熟，准确率还远远达不到一般期待的要求，只有在特定场景下能做的比较好。单轮问答已经开始实用化，准确率一般来百分之七十八十，自动问答系统没有超过 80% 的这个准确率的情况。去年，我有一个报告就讲鲁棒的自动问答或者知识问答，并不要求准确率是百分之百。单轮自动问答会马上越来越实用化，因为我们看到很多成功的例子包括 Alexa 往往都是用单轮对话技术来做的。文本的机器翻译水平在不断提高，深度学习在不断进步，越来越接近人的专业水平，但只是在一些特定场景下。完全去替代人，还是不太可能。人的语言理解是一个非常复杂的过程，序列对序列实际上是一种近似，现在这种技术能够去无穷尽的逼近人，但是本质上还是跟人的做法不一样的。即使是这样，准确率可以达到百分之七八十。在某些场景下，用户对性能要求并不是特别高。</p>

<p>总而言之，语音识别、机器翻译已经起飞，大家现在开始慢慢在用，但是真正对话的翻译还很困难，还有很长的路要走，但是也说不定能够做得很好。并不是说序列对序列就没有问题需要解决了，还有细致的问题。一个典型的问题就是长尾现象。不常用的单词、语音识别、翻译还是做得不是很好。比如用中文语音输入，人名、地名这种专有名词识别率一下就下降，特殊的专业术语识也不好，讲中文中间夹杂一些英文单词也是一种长尾现象。因为现在机器学习的方法是基于统计的，原则上就是看到数据里面的规律，掌握数据的规律。需要看到甚至多次重复看到一些东西，才能够掌握这些规律。这块相信有很多技术能帮助解决一些问题，使得机器翻译或语音识别技术不断提高，但是完全彻底的解决还是比较困难，因为这是这种方法带来的一个局限性。</p>

<p>单轮的问答，特别是场景驱动的单轮的问答，可能慢慢会开始使用。但是多轮对话技术还是比较难。马尔可夫决策过程实际上是还是个统计学习模型，本质特点就是需要有大量的数据去学习。其实我们人在做多轮对话的时候，并不需要重复才能掌握这种天生能力。这些是否能够用马尔科夫决策过程去模拟或者近似还不是很清楚。还有一个重要的问题就是多轮对话的数据不够，不能够很好地去学习这样的模型，去研究这些问题。即使是特定任务，多轮对话还比较困难，如果是任务不特定，比如聊天机器人就更难了，我们都不知道该怎么去做，马尔科夫决策过程都用不上。现实当中的一些聊天机器人就是用单轮技术堆起来，但是形成不了一个很自然合理的多轮对话，变成大家用起来觉得很奇怪的东西。总结起来就是多轮对话，在任务驱动的简单场景，有了更多的数据，我们是有可能做的越来越好。</p>

<p>给今天的讲座大概做一个总结，自然语言理解很难，自然语言处理现在用数据驱动的办法去做，有五个最基本的问题，即分类、匹配、翻译、结构预测和马尔可夫决策过程。在具体的问题上，有了数据就可以跑 AI 的闭环，就可以不断提高系统的性能、算法的能力。深度学习在我刚说的五个大任务里的前四个都能做得很好，特别是运用 seq toseq 的翻译和语音识别。单论对话也能做的越来越好，但是多轮对话需要去研究和解决。</p>

<p>自然语言概括的那部分其实我也写过一些文章，大家感兴趣的话也可以去看一看，网上也能搜得到，然后还有就是我们相关的工作论文，包括谷歌的工作论文，我在这里列出来了。</p>

<p>最后，欢迎大家加入我们的实验室。方向有语音、语言处理、推荐搜索、<a href="http://lib.csdn.net/base/20" title="Hadoop知识库">大数据</a>分析、智能通讯网络、计算机视觉、物联网、智能城市、智能终端。谢谢大家。</p>

<p>李航 VS 雷鸣 对话部分</p>

<p>雷鸣：特别感谢李老师精彩的讲座。今天这个讲座基本上对自然语言的整个发展能解决什么问题做了一个综述，而且对于技术、挑战和未来展望讲得特别全面，以致于我想到一个问题，后面就已经在回答了，今天讲得非常全面，非常仔细。我们想跟李航老师再探讨一下应用方面，我们知道华为的诺亚方舟做了很多计算语言方面的工作，能大概讲一下在落地方面做了哪些产品，大概现在处在一个什么水平上吗？</p>

<p>李航：好的。我们在语音、语言这方面做了两个应用，一个是机器翻译，一个是自然语言对话。机器翻译在我们公司内已经广泛使用，没有推到外面做产品。因为华为是一个非常国际化的公司，大概有不止三、四万的非中国籍员工，所以中翻英，英翻中在公司内使用，我们的技术都应用在里面。还有云对话的应用场景就是手机。</p>

<p>雷鸣：助手。</p>

<p>李航：对，助手。手机的东西一直在做，现在不太方便说，欢迎大家到我们实验室访问，我们可以做进一步介绍。</p>

<p>雷鸣：李航老师也组织一次北大、清华、中科院的同学一起去参观，关起门来比较好说。提到对话这块很有意思，比较早的商用系统是 Siri, 包括在《生活大爆炸》里也看到调笑 Siri 的场景，最后发现也就是大家稍微玩一玩，后面就没有后面了。我们发现比较实用化的对话系统反而是亚马逊做的 Echo，客观地讲亚马逊在自然语言的技术积累以前没有见太多，比起苹果、谷歌、甚至百度都不那么强，那为什么它能先做出来一个特别落地化的东西？它走了一个什么样的路径？对我们技术落地有什么启发？能大概解释一下吗？</p>

<p>李航：好。据我所知道的情况，亚马逊收购了几个公司，问答那部分是英国的剑桥做的，他们已经做了多年的这种问答，它们做得好的地方在于细节处理得非常好。也就是说问答的技术是亚马逊买来的。还有麦克阵列那些好像也不是自己开发的，细节我不是很清楚。但是自动问答我很清楚，就是收购的技术。</p>

<p>问答，或者说是广义的对话，刚才笼统地讲，有三类不同的技术，他们之间其实并不是包含和被包含的关系，是相互独立的。一种是分类或者结构预测，直观来说就是人手写规则，Siri 或者以前典型的场景都是基于这种技术做的。写好规则，这句话匹配上了，或者叫分类，分类对了，就去做了。还有一种技术就是问答，这种基于搜索、检索技术的比较多，有索引、排序这套东西，这你也是专家。再有就是多轮对话，比如说强化学习。这三套技术其实相互都比较独立。</p>

<p>说到匹配的话，模板、规则，或者说分类比较适合命令型的东西，未来就是家居各种场景，包括手机的命令，这种场景里面准确率相对也比较高，因为相对场景比较局限，能够达到百分之八、九十的识别准确率，甚至更高一些，应该是能够比较多地实用化。我们看到未来这是一个很好的场景。还有就是你刚才讲到 Alexa 的问答已经做得很好，这也是我觉得未来能够去实用化的一块，因为 Alexa 已经迈出非常好的一步了。它可以不断地跑人工智能闭环，收集更多的数据去把这个东西做得越来越好，就是内容不够填内容。如果是说 “理解” 用户的问法，还是用我们这种匹配、排序的技术，能看到哪个地方有问题就可以去改进，能够不断地把对话、问答这种东西做得越来越好。这两块我觉得都是未来能够起飞，能够用起来的技术。</p>

<p>多轮对话，刚才也说到，就是还需要很多研究了。数据也不够，大家都没有数据，是这样的一个状况。</p>

<p>雷鸣：可不可以理解为，他们虽然没什么积累，但是买的公司还挺厉害的。</p>

<p>李航：对，还挺厉害的。</p>

<p>雷鸣：有很牛的技术。第二点来讲，它进入到家庭场景里面，这个场景本身的限制导致说这个问题被降维了。</p>

<p>李航：对，就是实现做得比较好。</p>

<p>雷鸣：Siri 相当于一个开放式的，所以难度比较大一些。做到大家都满意就比较难一些，因为技术并不是一步到位的，选择技术能解决的问题去解决，解决实际问题然后再落地是更现实的事，可能会比较有挑战。谷歌成立很久，其实为全球培养了大量的科学家，出来了无数的创业公司，但是到现在好像还没有一个商业化成功的案例，所以他们走得有点儿太远了。</p>

<p>李航：是的。</p>

<p>雷鸣：刚才李航老师讲了好几次关于人类的语言，就是自然语言这一块它的复杂度是蛮高的，可能超过了用数学公式表达，或者是用概率就能搞定的。现在由于大量的数据积累，包括深度学习，使得在简单的语言问题上我们看到一种可解性。面对将来复杂的语言问题，我们现在能不能看到一个路径将来能够走到那一天？比如说这个路径大概是什么样子？现在在学术界，包括您这里有什么看法？比如说刚才讲这个多轮对话是个非常有挑战的问题，今天我们看到确实挺难的，多轮开放就更难了。但是有没有一种方向性的东西使得我们能够往那个方向走？</p>

<p>李航：这个问题问得很好，大家现在往往就是对人工智能过于乐观，包括自然语言处理也是一样，整个人工智能也是这样，还需要很多努力，还有漫长的路要走。面向未来的话，我们诺亚方舟实验室做研究一个大的方向就是怎么把知识和深度学习这样的技术结合起来。换一个角度就是说把符号处理 symbolic processing 和神经处理 neural processing 结合起来，这能够帮助我们做很多事。不一定能解决你说的多轮对话的问题，但是能帮助我们人类做很多事情。</p>

<p>你们可以这样想，计算机第一个是计算比咱们人类厉害，第二就是存储比人厉害。现在有互联网，各种信息库、知识库，但是我们觉得用起来还不是很方便，很多问题不是简单地通过搜索引擎就能去做。一个关于 symbolic neural processing 就是神经符号处理的想法就是，给计算机大量的文本，都是用符号表示的知识和信息，让它不加休息地去读，然后结合深度学习的技术，让它学到更好的知识表示、语义表示。对于每个人来说就是一个智能助手，帮助你去记忆各种东西，包括具体的信息，包括知识，这对我们人的能力是一种更大的延伸。不一定是多轮，但是是简单的多轮。实际上是把我们整个知识、信息的获取、检索的这样一个大的任务都解决，对我们每个人的能力是一种延伸。不光是知识，信息也可以融合起来。比如说，上次我和雷老师见面谈了什么都可以很快地导出来。这就扩大了我整个的能力，记忆的能力，存储的能力。这方面我们看到一些可能性，不好说是不是能突破，这也是自然语言知识问答的延伸，目前还做不到，做得不好，我们正在往这个方向一步步走，这是我们希望有突破的。</p>

<p>今年 1 月份我们去蒙特利尔访问了深度学习三大牛人之一 Bengio，请教了他关于 neuralsymbolic processing 的看法，他也是比较认可。他认可这种意义的结合，但是在一般意义上，如在深度网络里加一些 symbol，他认为是不对的。刚才说的这种意义上的结合他觉得还是有道理的，当然还有很多未知的问题，很多挑战，但是是值得进一步去探索的。我们现实中也开始在做一些研究。</p>

<p>雷鸣：这个回答大家去品味一下，技术发展无止境，有些时候一些特定的技术方式能解决一些特定的问题，但有时候也不是完全通用的。今天深度学习虽然很火，但是现在也有些反思，它是不是能解决所有问题。不同的学者有不同的想法，大家要用自己的智慧去理解。</p>

<p>刚才李航老师讲到 symbolic neural processing，从产业里我们有时候会从工程看问题。比如说自然语言这方面，第一是从语音到文字，如语音识别，现在专门有人做这个，做得还不错。第二就是从文字到语义，就是我知道你在说什么。第三层就是 response，就是我知道你在说什么，我再给你一个有效的反馈。从这三层来看，第一层做得还不错，第二层就是机器看到一段文字，到底理解没理解这个是否有个定义，咱们待会儿探讨一下。第三层就是来一个问题给一个正确的反馈，这和问题的理解层面纠缠有多深？因为我看到现在都是把问题和答案对着训，把内涵加进去了。就比如说对牛弹琴有两种，一个是它根本听不懂你谈的东西，第二是听懂了但它不知道是什么意思。聊天也一样，对方说的每一个字我都懂，但是合起来不懂，或者说合起来说的我也都懂，但是不知道怎么回答。从学术上看，刚才讲到语音分开得比较清楚，就是语义理解和回答这两块的研究我看一体化比较严重，从长远来看应该是分开还是合并的？每一块有什么挑战？</p>

<p>李航：这个问题问得非常好。这就牵扯到自然语言处理的本质的问题。我个人观点，说到人工智能、自然语言处理还有很长的路要走的意思就在这儿。一个就是要任务驱动才能去做，撇开任务单纯讲语义这是很难的，包括回答。深度学习的好处就是我们现在能做端到端的学习，输入、输出，里面都是黑箱，学习就好了。不好的地方就是中间发生什么都不知道。你刚才说对应人的语义，这个都不知道。深度学习肯定就有局限了。理想就是能够把人的支持加进来帮助语义的理解。刚才讲到 Bengio 也觉得这个事情不太好做，当然他也不一定绝对就对，但是这块是挺有挑战的，有太多不知道的事情了。</p>

<p>雷鸣：有一次我看到你提到谷歌的翻译，他们做的其实还是很牛的。以前我们都是端对端，英汉、汉英训练一个模型，诸如此类很多模型。但是谷歌是训练了个挺通用的模型，比如说英中对译训好了，中法对译训好了，然后英法之间就直接可以开始对译了。</p>

<p>李航：那是另外的一些工作，但是都是相通的。我今天介绍的不是太一样。业内有这样技术，有这样的研究。</p>

<p>雷鸣：这个我觉得挺有意思的，核心是我们没有训过英法之间的对译。这也就意味着，某种意义上来说，我们感觉机器在深度网络学习的时候对人类的语言做了一个内隐的表达。它在英中、中英、英法学习中间有个层面学会了英语的表达，然后就直接对上了。是不是说它找到了人类语言的某种内在表达方法？但是因为刚才说到的深度学习的特性导致我们读不出来，可不可以这样理解？</p>

<p>李航：我同意你的观点。就是这是做了一种表达。比如人做翻译其实也是一个很复杂的过程。你们有没有观察过专业的同声翻译，我只是从旁观察或跟他们交流，发现他们其实不思考的，他们已经形成了一种模式训练。不是所有外语好的人都能做同声翻译的，需要做一些特殊的训练。我的解释是他们其实是学了各种模式，但是他们很快，并不需要理解。他们就是有一种中间表示，很快就能转换成目标语言，我们现在的深度学习多少有点儿像那样一个机制，就是大量数据去训练。但往往我们一般人做翻译事实上是有语言的理解的，同声翻译至少局部就是一个模式。所以我同意你的观点，就是让中法、中英这些都一起训练，可能针对某些语言映射到内部有一些表示，对于语义的理解是比较 universal 的，就可以通用。</p>

<p>雷鸣：您刚才提到语义这一块您想说一下。</p>

<p>李航：对。有一篇文章是《迎接自然语言处理新时代》里面有写这个事情。你刚才谈到语音，语音只是人大脑里的一个模块，语言处理是整个大脑都会参与的，所以说语言本身就是很复杂。语音就是一个模块，所以语音处理不是一个人工智能完全的问题。语言处理是需要很多语言的知识模块参与在一起去做，而且大家可能也知道，脑区里面负责语言的部分都不止一个，多个脑区同时参与做这个事情。</p>

<p>雷鸣：占的面积也挺大的。</p>

<p>李航：对。现在大脑很多事情我们不知道。我们最早知道的一个有意思的事情是什么呢，就是给猴子的 premotor cortex 插上电极，发现有一个脑细胞在猴子自己吃香蕉和看到别人吃香蕉时都会有反应，说明在猴子的前运动域有一个脑细胞对应吃香蕉这个概念。然后人去做核磁共振这个实验，发现也是跟运动相关，比如说张开嘴或想象张开嘴，通过核磁共振去看脑区的反应的地方是一样的。让人去做某个动作和想象做某个动作是在大脑前运动皮质，而不是小脑，小脑是指挥你怎么样去运动，但是对应的大脑皮质有些运动的概念。有个假说就是有一个或多个脑细胞就是对应那个动作的概念。</p>

<p>现在有个我比较喜欢的假说，就是其实我们大脑皮质的一些细胞对应一些概念。比如喝水，看到人喝水，这个脑细胞可能就被激活，读小说读到有人在喝水，这个脑细胞也会被激活。所以说为什么读小说大家有身临其境的感觉，就是说视觉刺激和文字刺激都能刺激那个脑细胞，那一个或多个脑细胞就是对应这个概念。每个人理解语言的时候肯定是不完全一样的，因为每个人经验不一样。脑细胞被激活的过程是，成长过程中你的经历形成那些脑细胞激活的机制，相关的一些概念容易被联想出来，每个人容易联想的事情肯定是不一样的。但大家肯定有一些共性，如果差太多就没法交流了。脑细胞表示的概念还有很多共性的东西，使得我们能够做交流，能共同去做事情。既有共性，又有个性。</p>

<p>整个理解语言的过程就是激活相关的所有脑细胞对应的概念，把它们联系起来，然后还有自身体验的这种联系，这就是每个人对语言的理解。这个过程其实是非常复杂的。有人说意识占我们大脑处理的 2%，有个人极端的说法是下意识占 98%，就是说对语言的理解和处理实际上是在下意识中进行的，整个过程非常复杂，而且是并行处理，牵扯到到里面的很多个模块，达到了所谓对语言的理解。说话、写和看到的东西都是 symbol，都是非常表层的东西。它背后牵扯到，产生或理解这个文字符号时背后的大部分东西都是在我们大脑里无意识的情况下进行的。这是非常复杂的，怎么去把它发掘出来，到底是怎么一回事儿，非常复杂。本身大脑规模又非常大，大家知道大脑有 10 的 11 次方的个神经元，15 次方的连接。</p>

<p>雷鸣：对，1000 亿的这么一个复杂的系统。</p>

<p>李航：对，就是这么一个复杂的系统，这么一个复杂的现象，我们要去再现这种理解的过程，从现在的技术来说是非常困难了。</p>

<p>雷鸣：其实对大脑的研究现在在相当初级的阶段，我也关注比较久，基本上就跟你说的一样，大概到脑细胞激活这个研究，只能对特别简单的一些低等生物做一做，对人还是做一些脑区分化和相关性的一些研究。</p>

<p>其实深度学习的发展跟对大脑的理解有很大关系。现在不知道还算不算，一段时间以前我记得还有两个派别，一个态度是说尽量要了解清楚大脑的结构，然后进入模拟大脑的过程，模拟得足够快，就会产生通用智能。还有一个就是说，造飞机不需要造出一个会扇翅膀的。我们大概了解清楚了，用机器的告诉照样可以超过。我们不知道哪个是最终答案，但我觉得研究大脑的结构肯定能够促进人工智能的发展，人工智能的发展反过来也会促进我们对大脑的研究，这是相辅相成的。</p>

<p>如果在座的大家对人工智能感兴趣的，还是比较建议大家去读一读神经生物学。神经的结构、大脑这些看一看会对大家很有帮助。大脑是怎么做决策的，意识、直觉、痛苦，行动这些都会讲到，挺有意思的。比如说我说一句话，不要想一个红色的苹果，你们脑子里有没有一个红色的苹果？所以劝人时不要说，你不要哭了，你别难受了，这完全起不到任何作用，因为他听到的就是难受，就是哭。小小地讲一下大脑很有意思的这一点。</p>

<p>雷鸣：自然语言研究里还有很大一部分，叫做知识库，就是尝试建立一个知识表达。现在知识库的研究在整个 NLP 中处于一个什么位置呢？在以深度学习方法为主流的 NLP 中，还会有它的位置吗？</p>

<p>李航：这是很热的一个领域，研究很多。但是这方面我看的东西不是特别多。我更关心的是结合应用。这跟你刚才提的另外一个问题相关，就是怎么样去定义知识。如果不是应用驱动的，而是纯粹去定义知识的话，到目前为止我们看到的结果都不理想。你建完了很大的知识库，也不知道该怎么用。就是说，知识表示是不是合理，如果没有一个明确的应用，就很难判断。目前对人类语言机制的理解并不清楚。应用驱动、数据驱动是我们的主要想法，知识库建设也应该是这样。大家现在在朝这个方向走，有一个大趋势。我不知道你们注意到没有，大概 4、5 年前知识图谱这个概念就很火。很多公司都在做。那时有些很有野心的项目，比如要做巨大的知识图谱等等。这些项目后来基本都停顿了。大家发现，真正要做这种通用的知识库还是很难的。就我了解的范围内的共识，是说其实可以做一些领域知识库，比如说医疗知识库。又比如说我们在华为做了通讯领域知识库。这是更现实的。在应用里面去使用，能够解决实际的问题。</p>

<p>这是知识库方面的一个趋势，或者说是动向吧。应该结合到实际的应用里面。你建了知识库无外乎就是希望大家去使用这些知识库。我们希望，如果 Neural Symbolic Processing 这种新的技术有突破的话，如果知识的获取和检索能够解决的话，大家就能够更好地去使用知识库。这是最基本最重要的应用。从这一角度来看，我们没有特别关注知识库本身，而还是以应用驱动、数据驱动，看神经和符号的结合。</p>

<p>雷鸣：李航老师从科研界出来，在产业界摸爬滚了一段时间，观点是典型的以应用为驱动。用得着的项目，我们就放进去用；用不着的，就先放在那里，先慢慢研究。我也是这样的人。（笑）</p>

<p>另外，刚才李航老师也说到，有一些我们认为很高大上的技术，现在作为通用的解决方案可能还非常困难。可能由于运算能力不够、数据不够，甚至是模型的复杂度不够，或是理论模型还不能支撑。这时我们可以在一个小领域里先应用它，降难度，限场景，先把它用起来，有时发现，在 vertical 的小领域里面，它还是挺好用的。</p>

<p>好，下面我们看一下同学、观众在我们的<a href="http://lib.csdn.net/base/5" title="微信开发知识库">微信</a>群、公众号和网上直播中提出的问题。一个同学问，主流的聊天机器人，比如小冰、小娜等等，现在主要是用什么技术实现现在的结果的？</p>

<p>李航：不说具体的系统，聊天的技术一般来说主要还是基于检索的。产生式的对话系统真正到了实用阶段的我们还没有看到，或者说看到的不多。因为有很多风险。有一个风险是，它说的话可能都是对的，但是它说的事实是错的。比如你问它，姚明身高多少？它说，1 米 2。这种时候你还能判断出它是错的。但有时候无法判断，就很码放了。深度学习不知道怎么去控制这个系统。不知道在什么时候能让它说出准确的答案。</p>

<p>我们现在其实也做了一些研究，还都是比较偏基础的一些东西，到使用阶段还有一些距离。即使聊天机器人，里面也还不是一些实用的技术，基本上都是基于检索的多轮对话的技术。简单说就是上下文对齐、指代消歧，这样的事情也能局部地做一做，但缺少一个整体的多轮对话的模型。我刚才也说到，现在任务驱动的时候，有马尔科夫决策过程，但如果是闲聊，是 open 的，那么都没有一个很好的数学模型去刻画这一过程。所以这还是非常难的一个事情。</p>

<p>雷鸣：下一个问题。为什么国际会议上都是用英文的数据集，大家比来比去，为什么中文的 NLP 研究相对就少一些？</p>

<p>李航：没有啊，现在越来越多了。这是研究者的话语权的问题。现在做中文研究的学者越来越多，而且中文现在越来越重要。20 年前，中文的数据就更少了，现在中文数据已经越来越多了。同时英语也是作为国际性的语言，大家更容易去用。这不是什么大的问题。</p>

<p>雷鸣：好，有同学问，NLP 和创业相结合的话，有什么比较值得做的东西？</p>

<p>李航：这是个很好的问题。人工智能还是要跟具体的业务结合起来。自然语言处理也一样。这是第一个要定。第二个要点我刚才其实也讲到了，你需要去判断，你做的 NLP 系统其性能能达到的上界，和你面对的需求所要求的性能的下届是否能对上？这是非常重要的一个判断。如果你预测未来技术发展到某个阶段，能使上界提高到满足或超过需求下界的水平，就可以考虑结合实际的应用了。这个场景是非常多的。有很多场景里，我们都可以使用 NLP，来把它做得更好。其实我整晚的课都在强调这两点：一个是应用驱动，或者说需求驱动；另一个就是这个上界下界的事情。</p>

<p>雷鸣：李航老师高屋建瓴。我对创业这块比较熟悉一点。我们现在能看到 NLP 相关的创业，大体说有两类。一类是指令式的，比如智能家居，你下达指令，让它开灯关灯；车载环境下也是指令类。车载环境真的不方便用手了，智能用嘴去下达指令，你可能会说，给我老妈拨个电话，或是把刚刚收到的微信信息读一下。另一类是 QA 类的，基本应用在客服上。这一块最近用得非常多。大企业会自己做客服系统，借助自身大量的客户积累，去实现问题和回答的匹配。当然有一些中小企业和传统企业，没有这个能力，所以有一些创业公司就切进去了。这种情况也不少。另外还有一类，就是利用 NLP 技术对以前积累的数据和知识进行分析和处理，比如文本构成的知识库，像卷宗、病历等等，用 Watson 这样的系统去分析它，尝试发掘一些规则的知识。</p>

<p>大体上来讲，大公司，比如华为，有人力、财力、物力，再加上有数据，同时还有应用场景，你去跟它抢这个市场难度很大。也不是说完全没有可能，但难度很大。而有一些行业，比如医疗，大公司也没有数据，都要去抢。这相应来讲还有一定机会。</p>

<p>再问一个问题。上节课上徐伟老师讲过，有一个调研，调研了一百位科学家，这些人中有一半人都认为，2050 年之前，强人工智能有超过 50% 的可能性会实现。当然你可能不同意这种说法。显然，通用人工智能应该能理解人类语言。这是不是意味着自然语言处理在接下来的三十年间也会有很大的发展？或者说，到最后，NLP 的问题会等价于通用人工智能的问题？</p>

<p>李航：有一个说法我比较认可，就是未来的 5 年、10 年，我们可以预测。20 年的话，基本上就不能预测了。2050 年的事情，真的谁都很难预测了。返回头说，十年前我们能预料到语音识别会达到现在的水平吗？十年前可能没有人能够预测得到。很多东西不好预测，预测未来是一个挺难的事情。尤其是现在技术突飞猛进，发展这么快，各种路数都出来，你知道哪边突破了？这都很难说。</p>

<p>还有，通用人工智能也没有一个准确的定义。在我看来，通用人工智能会在未来 10 年、20 年在一定程度上有突破，我也持这种观点。如果我们把通用人工智能定义为把语言、视觉、听觉等所有这些能力综合起来的一种能力，那么是完全有可能的。因为传统上认为人工智能太难了，所以大家把它分而治之，研究视觉的，研究听觉的，等等。现在如果说要通过类似深度学习的方法把这些串起来，这是可能的。比如说你把图像识别和语言处理放在一起做。其实人在成长过程中，学习最基本概念的时候，图像、语言等等也是同时来学习的。从这种意义上说，未来甚至不用那么长的时间，就能看到一些成功的案例了。现实中我们已经看到一些多模态的智能结合了。但是这还不能说是达到语言理解了。我觉得从这个意义上说，语言理解就更难了。</p>

<p>雷鸣：对。上一次我跟徐伟探讨时，也说到了通用人工智能的定义问题。我们当时有一个简单的小共识，认为它不是用解决什么问题来定义的，而是可能要看它的学习能力。比如说跟我们人类一样的通用人工智能，你让它学开车，人用一个月学会，它也可以用一个月学会；然后还是同样的一段程序，去让它学围棋，它和人类一样，花了三年时间，变成了业余几段。从学习能力来考察，可以认为它是实现了通用人工智能。</p>

<p>李航：对对，这个我同意。可以从学习能力来判断。</p>

<p>雷鸣：对，就是同一套程序，干啥都行。这是蛮有意思的探讨。</p>

<p>李航：对，我同意。</p>

<p>雷鸣：未来的事情确实真的都不好说，你刚才说返回头看，其实就算往回推一年，去年的 4 月份，谁也不知道 AlphaGo 能赢人啊。那时候大部分人认为还是赢不了的。</p>

<p>再问一个问题的，刚才咱们也提了关于知识的问题。我们知道，神经网络没有一个明确的存储，虽然它存起来了，但不知道存在哪里了。现在有一种网络，叫 memory network，里面加了内存。未来发展它的潜力会不会比较大？加了 memory 之后，包括推理、自然语言处理等方面的能力会不会有一个比较大的提升？还是说现在看也就是一个架构而已？</p>

<p>李航：刚才我说的神经符号处理，就是其中一部分，我觉得这个东西是非常非常重要的。时间关系我今天不能讲太多。我今年还会做一两个报告来讲这个事情。人的一个很重要的特点是能够把过去的事情都记忆起来，可以把记忆里面这些知识和事实的信息都能够串联起来。机器现在就没有这样的能力。AlphaGo 就没有一个记忆的能力。如果能够把知识、信息不断往里存储，根据需要检索，如果有了这个能力，对机器智能来说是非常大的提升。</p>

<p>雷鸣：我们现在看不到 neural network 里显式的记忆点在哪里，但是我们发现它是有隐式的记忆的。就像你刚才说的，问答在它的里面跑了几回，它就记住了，好像是记在什么地方了。那么是不是一定要显式的记忆才会有效？还是说只要网络足够的复杂，其实它也通过网络结构实现了记忆呢？</p>

<p>李航：是这样的，它在记语言的使用模式的时候，比如句法啊、回答问题的反式啊，等等，隐式记忆也许是可以的。但像知识啊、信息啊，这种事实性的东西，我们还是希望它是显式的，让人能够看到，能够检验。</p>

<p>雷鸣：有可解释性。</p>

<p>李航：对，它需要是可解释的。如果长期记忆这种机制成功的话，神经网络能够不断去记住新的东西，那它就会越来越强大。</p>

<p>我再多说一句，好莱坞电影里面会有人和机器人谈恋爱。那太遥远了。第一步，机器首先需要有自己的意识。意识有很多定义，一个最基本的定义，是说如果一个系统能对外界的变化产生反应，那就是一种意识。最简单的意识，比如说温度计，或者向日葵，都可以看做是有最基本的意识的。</p>

<p>雷鸣：这么说人工智能已经有意识了？</p>

<p>李航：从这个意义上说，是的。但人工智能没有自我意识。它不知道自己是谁。第一，有了意识；第二，如果你能有记忆，你就又进一步了；第三，如果你有自我意识，你就有可能以此为基础生发出情感。这样才可能和人类恋爱。所以说，目前的人工智能可以说已经有了最基本的意识了；如果它现在又有了记忆，它就能够把它整个的历史串起来了，我想这是整个智能机器在朝着自我意识的方向上又进化了一步。这件事是很激动人心的。</p>

<p>雷鸣：也有一点吓人（笑）。好，感谢李航老师的分享！</p>

<p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrcceoer.html">http://www.iqiyi.com/l_19rrcceoer.html</a> 
课程介绍</p>

<p>“人工智能前沿与产业趋势” 课程由北京大学开设，并面向公众开放。课程由人工智能创新中心主任雷鸣老师主持，共 14 节，每节课邀请一位人工智能领域顶级专家和行业大咖作为主讲嘉宾，就人工智能和一个具体行业的结合深度探讨，分析相应技术的发展，如何影响产业，现状及未来趋势、对应挑战和与机遇。所有课程相关信息、通知都会在下方的公众号发布。</p>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/">全面解析Tensor2Tensor系统</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#tensorflow" title="Tag: tensorflow" rel="tag">tensorflow</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>本文由<a href="https://cloud.tencent.com/developer/user/2479598?fromSource=waitui">张金超博士</a>发表于<a href="https://cloud.tencent.com/developer/column/4248?fromSource=waitui">云 + 社区专栏</a></p>

<blockquote>
  <p>导语： Google Tensor2Tensor 系统是一套十分强大的深度学习系统，在多个任务上的表现非常抢眼。尤其在机器翻译问题上，单模型的表现就可以超过之前方法的集成模型。这一套系统的模型结构、训练和优化技巧等，可以被利用到公司的产品线上，直接转化成生产力。本文对 Tensor2Tensor 系统从模型到代码进行了全面的解析，期望能够给大家提供有用的信息。</p>
</blockquote>

<h1 id="第一章概述">第一章：概述</h1>

<p>Tensor2Tensor（T2T）是 Google Brain Team 在 Github 上开源出来的一套基于 TensorFlow 的深度学习系统。该系统最初是希望完全使用 Attention 方法来建模序列到序列（Sequence-to-Sequence，Seq2Seq）的问题，对应于《Attention Is All You Need》这篇论文。该项工作有一个有意思的名字叫 “Transformer”。随着系统的不断扩展，T2T 支持的功能变得越来越多，目前可以建模的问题包括：图像分类，语言模型、情感分析、语音识别、文本摘要，机器翻译。T2T 在很多任务上的表现很好，并且模型收敛比较快，在 TF 平台上的工程化代码实现的也非常好，是一个十分值得使用和学习的系统。</p>

<p>如果是从工程应用的角度出发，想快速的上手使用 T2T 系统，只需要对模型有一些初步的了解，阅读一下 workthrough 文档，很快就能做模型训练和数据解码了。这就是该系统想要达到的目的，即降低深度学习模型的使用门槛。系统对数据处理、模型、超参、计算设备都进行了较高的封装，在使用的时候只需要给到数据路径、指定要使用的模型和超参、说明计算设备就可以将系统运行起来了。</p>

<p>如果想深入了解系统的实现细节，在该系统上做二次开发或是实现一些研究性的想法，那就需要花费一定的时间和精力来对模型和代码进行研究。T2T 是一个较复杂的系统，笔者近期对模型和代码实现进行了全面的学习，同时对涉及到序列到序列功能的代码进行了剥离和重构，投入了较多的时间成本。因笔者是做自然语言处理研究的，这篇文章里主要关注的是 Transformer 模型。写这篇文章一方面是总结和记录一下这个过程中的一些收获，另一方面是把自己对 T2T 的理解分享出来，希望能够提供一些有用的信息给同学们。</p>

<h1 id="第二章序列到序列任务与-transformer-模型">第二章：序列到序列任务与 Transformer 模型</h1>

<h2 id="21-序列到序列任务与-encoder-decoder-框架">2.1 序列到序列任务与 Encoder-Decoder 框架</h2>

<p>序列到序列（Sequence-to-Sequence）是自然语言处理中的一个常见任务，主要用来做泛文本生成的任务，像机器翻译、文本摘要、歌词 / 故事生成、对话机器人等。最具有代表性的一个任务就是机器翻译（Machine Translation），将一种语言的序列映射到另一个语言的序列。例如，在汉 - 英机器翻译任务中，模型要将一个汉语句子（词序列）转化成一个英语句子（词序列）。</p>

<p>目前 Encoder-Decoder 框架是解决序列到序列问题的一个主流模型。模型使用 Encoder 对 source sequence 进行压缩表示，使用 Decoder 基于源端的压缩表示生成 target sequence。该结构的好处是可以实现两个 sequence 之间 end-to-end 方式的建模，模型中所有的参数变量统一到一个目标函数下进行训练，模型表现较好。图 1 展示了 Encoder-Decoder 模型的结构，从底向上是一个机器翻译的过程。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/9mnz6nbfqg.png?imageView2/2/w/1620" alt="" />图 1： 使用 Encoder-Decoder 模型建模序列到序列的问题</p>

<p>Encoder 和 Decoder 可以选用不同结构的 Neural Network，比如 RNN、CNN。RNN 的工作方式是对序列根据时间步，依次进行压缩表示。使用 RNN 的时候，一般会使用双向的 RNN 结构。具体方式是使用一个 RNN 对序列中的元素进行从左往右的压缩表示，另一个 RNN 对序列进行从右向左的压缩表示。两种表示被联合起来使用，作为最终序列的分布式表示。使用 CNN 结构的时候，一般使用多层的结构，来实现序列局部表示到全局表示的过程。使用 RNN 建模句子可以看做是一种时间序列的观点，使用 CNN 建模句子可以看做一种结构化的观点。使用 RNN 结构的序列到序列模型主要包括 RNNSearch、GNMT 等，使用 CNN 结构的序列到序列模型主要有 ConvS2S 等。</p>

<h2 id="22-神经网络模型与语言距离依赖现象">2.2 神经网络模型与语言距离依赖现象</h2>

<p>Transformer 是一种建模序列的新方法，序列到序列的模型依然是沿用了上述经典的 Encoder-Decoder 结构，不同的是不再使用 RNN 或是 CNN 作为序列建模机制了，而是使用了 self-attention 机制。这种机制理论上的优势就是更容易捕获 “长距离依赖信息（long distance dependency）”。所谓的“长距离依赖信息” 可以这么来理解：1）一个词其实是一个可以表达多样性语义信息的符号（歧义问题）。2）一个词的语义确定，要依赖其所在的上下文环境。（根据上下文消岐）3）有的词可能需要一个范围较小的上下文环境就能确定其语义（短距离依赖现象），有的词可能需要一个范围较大的上下文环境才能确定其语义（长距离依赖现象）。</p>

<p>举个例子，看下面两句话：“山上有很多杜鹃，春天到了的时候，会<a href="https://www.baidu.com/s?wd=%E6%BC%AB%E5%B1%B1%E9%81%8D%E9%87%8E&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd">漫山遍野</a>的开放，非常美丽。” “山上有很多杜鹃，春天到了的时候，会漫山遍野的啼鸣，非常婉转。”在这两句话中，“杜鹃”分别指花（azalea）和鸟（cuckoo）。在机器翻译问题中，如果不看距其比较远的距离的词，很难将 “杜鹃” 这个词翻译正确。该例子是比较明显的一个例子，可以明显的看到词之间的远距离依赖关系。当然，绝大多数的词义在一个较小范围的上下文语义环境中就可以确定，像上述的例子在语言中占的比例会相对较小。我们期望的是模型既能够很好的学习到短距离的依赖知识，也能够学习到长距离依赖的知识。</p>

<p>那么，为什么 Transformer 中的 self-attention 理论上能够更好的捕获这种长短距离的依赖知识呢？我们直观的来看一下，基于 RNN、CNN、self-attention 的三种序列建模方法，任意两个词之间的交互距离上的区别。图 2 是一个使用双向 RNN 来对序列进行建模的方法。由于是对序列中的元素按顺序处理的，两个词之间的交互距离可以认为是他们之间的相对距离。W1 和 Wn 之间的交互距离是 n-1。带有门控（Gate）机制的 RNN 模型理论上可以对历史信息进行有选择的存储和遗忘，具有比纯 RNN 结构更好的表现，但是门控参数量一定的情况下，这种能力是一定的。随着句子的增长，相对距离的增大，存在明显的理论上限。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/ua4lw0q4og.png?imageView2/2/w/1620" alt="" />图 2 使用双向 RNN 对序列进行建模</p>

<p>图 3 展示了使用多层 CNN 对序列进行建模的方法。第一层的 CNN 单元覆盖的语义环境范围较小，第二层覆盖的语义环境范围会变大，依次类推，越深层的 CNN 单元，覆盖的语义环境会越大。一个词首先会在底层 CNN 单元上与其近距离的词产生交互，然后在稍高层次的 CNN 单元上与其更远一些词产生交互。所以，多层的 CNN 结构体现的是一种从局部到全局的特征抽取过程。词之间的交互距离，与他们的相对距离成正比。距离较远的词只能在较高的 CNN 节点上相遇，才产生交互。这个过程可能会存在较多的信息丢失。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/oknaiat0yy.png?imageView2/2/w/1620" alt="" />图 3 使用多层 CNN 对序列进行建模</p>

<p>图 4 展示的是基于 self-attention 机制的序列建模方法。注意，为了使图展示的更清晰，少画了一些连接线，图中 “sentence” 层中的每个词和第一层 self-attention layer 中的节点都是全连接的关系，第一层 self-attention layer 和第二层 self-attention layer 之间的节点也都是全连接的关系。我们可以看到在这种建模方法中，任意两个词之间的交互距离都是 1，与词之间的相对距离不存在关系。这种方式下，每个词的语义的确定，都考虑了与整个句子中所有的词的关系。多层的 self-attention 机制，使得这种全局交互变的更加复杂，能够捕获到更多的信息。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/9ld5wwb8ob.png?imageView2/2/w/1620" alt="" />图 4 使用 self-attention 对序列进行建模</p>

<p>综上，self-attention 机制在建模序列问题时，能够捕获长距离依赖知识，具有更好的理论基础。</p>

<h2 id="23-self-attention-机制的形式化表达">2.3 self-attention 机制的形式化表达</h2>

<p>上面小节介绍了 self-attention 机制的好处，本小结来介绍一下 self-attention 机制的的数学形式化表达。首先，从 attention 机制讲起。可以将 attention 机制看做一种 query 机制，即用一个 query 来检索一个 memory 区域。我们将 query 表示为 key_q，memory 是一个键值对集合（a set of key-value pairs），共有 M 项，其中的第 i 项我们表示为</p>

<h2 id="24-attention-is-all-you-need">2.4 “Attention is All You Need”</h2>

<p>《Attention Is All You Need》这篇文章，描述了一个基于 self-attention 的序列到序列的模型，即 “Transformer”。该模型将 WMT2014 英 - 德翻译任务的 BLEU 值推到了新高，在英 - 法翻译任务上，接近于之前报出的最好成绩，而这仅仅是 Transformer 单模型的表现。之前报出的最好成绩都是基于集成方法的，需要训练多个模型，最后做集成。同时该模型也被用在英语的成分句法分析任务上，表现也基本接近于之前报出的最好模型成绩。该模型的收敛速度也非常的快，在英 - 法 3600 万句对的训练集上，只需要 8 卡并行 3.5 天就可以收敛。</p>

<p>该模型的表现的如此好的原因，其实不仅仅是一个 self-attention 机制导致的，实际上 Transformer 模型中使用了非常多有效的策略来使得模型对数据的拟合能力更强，收敛速度更快。整个 Transformer 的模型是一套解决方案，而不仅仅是对序列建模机制的改进。下面我们对其进行一一讲解。</p>

<h3 id="241-self-attention-机制的变种">2.4.1 Self-attention 机制的变种</h3>

<p>首先，还是来讲一下 Transformer 中的 self-attention 机制。上面讲到了 self-attention 的基本形式，但是 Transformer 里面的 self-attention 机制是一种新的变种，体现在两点，一方面是加了一个缩放因子（scaling factor），另一方面是引入了多头机制（multi-head attention）。</p>

<p>缩放因子体现在 Attention 的计算公式中多了一个向量的维度作为分母，目的是想避免维度过大导致的点乘结果过大，进入 softmax 函数的饱和域，引起梯度过小。Transformer 中的 self-attention 计算公式如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/67kp0nv56h.png?imageView2/2/w/1620" alt="" /></p>

<p>多头机制是指，引入多组的参数矩阵来分别对 Q、K、V 进行线性变换求 self-attention 的结果，然后将所有的结果拼接起来作为最后的 self-attention 输出。这样描述可能不太好理解，一看公式和示意图就会明白了，如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/iyy6tjm17t.png?imageView2/2/w/1620" alt="" /></p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/x9euximxkw.png?imageView2/2/w/1620" alt="" />图 5 单头和多头的 Attention 结构</p>

<p>这种方式使得模型具有多套比较独立的 attention 参数，理论上可以增强模型的能力。</p>

<h3 id="242-位置编码positional-encoding">2.4.2 位置编码（Positional Encoding）</h3>

<p>self-attention 机制建模序列的方式，既不是 RNN 的时序观点，也不是 CNN 的结构化观点，而是一种词袋（bag of words）的观点。进一步阐述的话，应该说该机制视一个序列为扁平的结构，因为不论看上去距离多远的词，在 self-attention 机制中都为 1。这样的建模方式，实际上会丢失词之间的相对距离关系。举个例子就是，“牛 吃了 草”、“草 吃了 牛”，“吃了 牛 草” 三个句子建模出来的每个词对应的表示，会是一致的。</p>

<p>为了缓解这个问题，Transformer 中将词在句子中所处的位置映射成 vector，补充到其 embedding 中去。该思路并不是第一次被提出，CNN 模型其实也存在同样的难以建模相对位置（时序信息）的缺陷，Facebook 提出了位置编码的方法。一种直接的方式是，直接对绝对位置信息建模到 embedding 里面，即将词 Wi 的 i 映射成一个向量，加到其 embedding 中去。这种方式的缺点是只能建模有限长度的序列。Transformer 文章中提出了一种非常新颖的时序信息建模方式，就是利用三角函数的周期性，来建模词之间的相对位置关系。具体的方式是将绝对位置作为三角函数中的变量做计算，具体公式如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/c5fz2lcx5o.png?imageView2/2/w/1620" alt="" /></p>

<p>该公式的设计非常先验，尤其是分母部分，不太好解释。从笔者个人的观点来看，一方面三角函数有很好的周期性，也就是隔一定的距离，因变量的值会重复出现，这种特性可以用来建模相对距离；另一方面，三角函数的值域是 [-1,1]，可以很好的提供 embedding 元素的值。</p>

<h3 id="243-多层结构">2.4.3 多层结构</h3>

<p>Transformer 中的多层结构非常强大，使用了之前已经被验证过的很多有效的方法，包括：residual connection、layer normalization，另外还有 self-attention 层与 Feed Forward 层的堆叠使用，也是非常值得参考的结构。图 6 展示了 Transformer 的 Encoder 和 Decoder 一层的结构。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/phxma860mx.png?imageView2/2/w/1620" alt="" />图 6 Transformer 模型结构</p>

<p>图 6 中，左侧的 Nx 代表一层的 Encoder，这一层中包含了两个子层（sub-layer），第一个子层是多头的 self-attention layer，第二个子层是一个 Feed Forward 层。每个子层的输入和输出都存在着 residual connection，这种方式理论上可以很好的回传梯度。Layer Normalization 的使用可以加快模型的收敛速度。self-attention 子层的计算，我们前面用了不少的篇幅讲过了，这里就不再赘述了。Feed Forward 子层实现中有两次线性变换，一次 Relu 非线性激活，具体计算公式如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/us7axqro09.png?imageView2/2/w/1620" alt="" /></p>

<p>文章的附页中将这种计算方式也看做是一种 attention 的变种形式。</p>

<p>图 6 中，右侧是 Decoder 中一层的结构，这一层中存在三个子层结构，第一层是 self-attention layer 用来建模已经生成的目标端句子。在训练的过程中，需要一个 mask 矩阵来控制每次 self-attention 计算的时候，只计算到前 t-1 个词，具体的实现方式，我们会在后面讲代码实现的时候进行说明。第二个子层是 Encoder 和 Decoder 之间的 attention 机制，也就是去源语言中找相关的语义信息，这部分的计算与其他序列到序列的注意力计算一致，在 Transformer 中使用了 dot-product 的方式。第三个子层是 Feed Forward 层，与 Encoder 中的子层完全一致。每个子层也都存在着 residual connection 和 layer normalization 操作，以加快模型收敛。</p>

<p>Transformer 中的这种多层 - 多子层的机制，可以使得模型的复杂度和可训练程度都变高，达到非常强的效果，值得我们借鉴。</p>

<h3 id="244-优化方法与正则策略">2.4.4 优化方法与正则策略</h3>

<p>模型的训练采用了 Adam 方法，文章提出了一种叫 warm up 的学习率调节方法，如公式所示：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/wksbljxz0i.png?imageView2/2/w/1620" alt="" /></p>

<p>公式比较先验，看上去比较复杂，其实逻辑表达起来比较清楚，需要预先设置一个 warmup_steps 超参。当训练步数 step_num 小于该值时，以括号中的第二项公式决定学习率，该公式实际是 step_num 变量的斜率为正的线性函数。当训练步数 step_num 大于 warm_steps 时，以括号中的第一项决定学习率，该公式就成了一个指数为负数的幂函数。所以整体来看，学习率呈先上升后下降的趋势，有利于模型的快速收敛。</p>

<p>模型中也采用了两项比较重要的正则化方法，一个就是常用的 dropout 方法，用在每个子层的后面和 attention 的计算中。另一个就是 label smoothing 方法，也就是训练的时候，计算交叉熵的时候，不再是 one-hot 的标准答案了，而是每个 0 值处也填充上一个非 0 的极小值。这样可以增强模型的鲁棒性，提升模型的 BLEU 值。这个思路其实也是一定程度在解决训练和解码过程中存在的 exposure bias 的问题。</p>

<h3 id="245-本章小结">2.4.5 本章小结</h3>

<p>Transformer 系统的强大表现，不仅仅是 self-attention 机制，还需要上述的一系列配合使用的策略。设计该系统的研究者对深度学习模型和优化算法有着非常深刻的认识和敏锐的感觉，很多地方值得我们借鉴学习。Transformer 的代码实现工程化比较好，但是也存在一些地方不方便阅读和理解，后面的章节中会对 Transformer 的代码实现进行详细讲解，将整体结构讲清楚，把其中的疑难模块点出来。</p>

<h1 id="第三章tensor2tensor-系统实现深度解析">第三章：Tensor2Tensor 系统实现深度解析</h1>

<p>Tensor2Tensor 的系统存在一些特点，导致使用和理解的时候可能会存在一些需要时间来思考和消化的地方，在此根据个人的理解，写出一些自己曾经花费时间的地方。</p>

<h2 id="31-使用篇">3.1 使用篇</h2>

<p>Tensor2Tensor 的使用是比较方便的，对于系统中可以支持的问题，直接给系统设置好下面的信息就可以运行了：数据，问题 (problem)，模型，超参集合，运行设备。这里的实现其实是采用了设计模型中的工厂模式，即给定一个问题名字，返回给相应的处理类；给定一个超参名，返回一套超参的对象。实现这种方式的一个重点文件是 utils/registry.py。在系统启动的时候，所有的问题和超参都会在 registry 中注册，保存到_MODELS，_HPAPAMS，_RANGED_HPARAMS 中等待调用。</p>

<p>在此主要以序列到序列的系统使用和实现为主线进行讲解。系统的运行分三个阶段：数据处理，训练，解码。对应着三个入口：t2t-datagen，t2t-trainer，t2t-decoder。</p>

<p><strong>数据处理</strong>的过程包括：</p>

<p>1.（下载）读取训练和开发数据。如果需要使用自己的数据的话，可以在问题中指定。</p>

<p>2.（读取）构造词汇表。可以使用自己预先构造好的词汇表。系统也提供构建 BPE 词汇表的方法。注意，这里有个实现细节是系统在抽取 BPE 词汇表的时候，有个参数，默认并非使用全量的数据。通过多次迭代尝试，得到最接近预设词汇表规模的一个词汇表。在大数据量的时候，这个迭代过程会非常慢。</p>

<p>3. 使用词汇表将单词映射成 id，每个句子后会加 EOS_ID，每个平行句对被构造成一个 dict 对象 ({‘inputs’:value，‘targets’：value})，将所有对象序列化，写入到文件中，供后面训练和评价使用。</p>

<p><strong>模型训练</strong>的过程的过程主要通过高级的 Tensorflow API 来管理，只是需要指定数据、问题名、模型名、超参名、设备信息就可以运行了。比较关键的一个文件是 utils/trainer_lib.py 文件，在这个文件中，构建 Experiment、Estimator、Monitor 等来控制训练流程。使用者主要需要设置的就是训练过程的一些参数，比如训练最大迭代次数，模型评估的频率，模型评估的指标等。超参可以直接使用系统已有的参数集，也可以通过字符串的形式向内传参。简单的任务不太需要动超参，因为系统中的超参集合基本上都是经过实验效果验证的。需要注意的就是 batch_size 过大的时候，可能会导致显存不足，导致程序错误。一般是使用 continuous_train_and_eval 模式，使模型的训练和评估间隔进行，随时可以监控模型的表现。</p>

<p><strong>解码</strong>的过程，可以提供整体文件、也可以是基于 Dataset 的，同时系统也提供 server 的方式，可以提供在线的服务，并没有什么特别好讲的。</p>

<h2 id="32-深度掌握篇">3.2 深度掌握篇</h2>

<h3 id="321-tensor2tensor-系统实现的特点">3.2.1 Tensor2Tensor 系统实现的特点</h3>

<p>下面列出了要深度掌握 Tensor2Tensor 系统时，可能因为其实现特点，会遇到的一些问题：</p>

<p>1. 系统支持多任务，任务混杂，导致代码结构比较复杂。在实现的时候，要考虑到整体的结构，所以会存在各种封装、继承、多态的实现。可能你只想用其中的一个功能，理解该功能对应的代码，但是却需要排除掉大量的不相关的代码。</p>

<p>2. 系统基于 Tensorflow 封装较高的 API。使用了 Tensorflow 中比较高的 API 来管理模型的训练和预测，Experiment，Monitor，Estimator，Dataset 对象的使用隐藏了比较多的控制流程，对于侧重应用的用户来说，可能是是好事情，设一设参数就能跑。但是对于想了解更多的开发人员来说，TF 该部分的文档实在很少，说的也不清楚，很多时候需要去阅读源代码才能知道实验到底是不是按照自己预期的进行的。这种方式也不太方便找 bug 和调试。</p>

<p>3. 某些方法调用比较深。原因应该还是出于整体结构和扩展性的考虑。这导致了实现一点很小功能的方法 A，需要再调一个其他方法 B，B 再去调用方法 C，实际上每个方法中就几行代码，甚至有的方法就是空操作。</p>

<p>4. 多层继承和多态也降低了代码的可读性。追溯一个类的某个方法的时候，需要看到其父类的父类的父类。。。这些父类和子类之间的方法又存在着调来调去的关系，同名方法又存在着覆盖的关系，所以要花一些时间来确定当前的方法名到底是调用的的哪个类中的方法。</p>

<p>5. 要求开发者有模型层面的理解和与代码实现的挂钩。肯定是要提高对模型逻辑的理解，但在读代码的过程中，会遇到两种问题：第一个，代码实现的是论文中的功能，但不是论文中的原始公式，可能要做变形以规避溢出的问题，或是实现更高的效率；第二个，某些代码实现与其论文中的表述存在不一致的情况。</p>

<h3 id="322-总体逻辑模块">3.2.2 总体逻辑模块</h3>

<p>总体来说，对 T2T 系统的代码逻辑划分如下，共包括三个大的模块：</p>

<ol>
  <li><strong>问题定义和数据管理的模块。</strong>该模块用来定义问题和处理数据，比如定义一个翻译的问题，里面定义抽词汇表和构造训练样本的方法。</li>
  <li><strong>模型定义和计算图构建的模块。</strong>该模块用来定义模型属性和计算图结构。</li>
  <li><strong>实验流程控制与并行化模块。</strong>该模块用于实验流程控制，设置可用计算设备，提供模型并行化运行方法。</li>
</ol>

<p><img src="https://ask.qcloudimg.com/draft/2479598/jq5voxlwmz.png?imageView2/2/w/1620" alt="" />图 7 Tensor2Tensor 主要逻辑模块</p>

<p>这里不会对代码做追踪式的分析，会分条的讲解一些阅读 Tensor2Tensor 系统代码时可能遇到的问题，点出一些重要的功能所在的位置和实现逻辑。</p>

<ol>
  <li><strong>工厂模式。</strong>系统使用工厂模式管理问题、模型、超参、模态等模块的方法。前面在使用篇讲到了 registry.py 这个比较关键的文件，是系统总体管理和调度模块的一个核心文件。如果要在系统中增加新的问题、模型、超参、模态等，也都需要通过在类前加装饰器的方式来注册到 registry 中，否则系统找不到新加的模块。</li>
  <li>** 问题类 (problem)。**data_generators/problem.py 中的 class Problem 是后面所有 problem 的基类。之前说到系统中的类之间的多层继承关系导致代码读起来比较麻烦，举个例子来说，一个翻译问题继承路线是这样的：Problem»Text2TextProblem»TranslateProblem»TranslateEndeWmtBpe32k» TranslateEndeWmt32k，中间各种的方法和变量覆盖，父类和子类之间方法的穿插调用，导致一些阅读困难。总的来说，一个序列到序列的问题应该包括以下信息和方法：数据文件信息，词汇表文件名、类型、大小，构造词汇表的方法，序列化训练数据和开发数据的方法，读取数据文件为 model（estimator）构造输入流 input_fn 的方法，设定问题评估 metric 的方法。可以总结来说，问题的属性定义、训练和评价样本的构造、数据的处理和读取，都由 problem 这个体系里面的类和方法来提供。</li>
  <li><strong>词汇表对象 (TextEncoder)。</strong>系统中有多种多样的词汇表（TextEncoder）对象，可以支持字母（character），子词（subword/bpe），词汇（token）等多重方式。TextEncoder 主要功能就是构建词汇表、实现符号到 id 的映射。T2T 里有构造 bpe 词汇表的方法，没有 word piece 词汇表的构造方法，也可以看出 T2T 研究团队和 GNMT 研究团队的区分。两个团队一直在交替的更新机器翻译任务的最高成绩。构建 BPE 词汇表的具体实现在 SubwordTextEncoder 中的 build_to_target_size（）方法，该方法不是之前 Sennrich 使用迭代次数来控制词汇表大小的方式，而是使用二分查找的方式，通过搜索最优的 minimum token count 值来逼近预先设置的词汇表的大小。</li>
  <li><strong>T2TModel 类。</strong>utils/t2t_model.py 中的 class T2TModel 是模型功能的基类，该类继承自 layer，Transformer 类便继承于此类。T2TModel 类中定义了模型的计算图结构，即给定 feature 后，模型是怎么根据 feature 进行图计算，得到 logit，loss，然后根据 loss 求梯度，调用 optimizer 进行梯度回传，进行参数更新的。构建计算图的目的是最终要构建 tf.estimator.EstimatorSpec（）对象。可以理解为，所有的模型图计算过程都在该对象中被表达了。T2TModel 可以返回三种 EstimatorSpec 对象，分别用于训练、评价和解码。训练的过程可以支持数据并行，具体的实现是同时在多个数据片上激活计算图，得到的 loss 做平均，这是一种同步并行训练的方式。T2TModel 中也提供了供解码的方法。</li>
  <li><strong>Transformer 类。</strong>models/transformer.py 中的 class Transformer 继承自 class T2TModel，为其父类构建图的时候，提供各种支持的方法，encode 方法可以使用 Encoder 结构对源端进行压缩表示，decode 方法使用 Decoder 结构对目标端进行生成。同时，transformer.py 中有多套参数供选择。模型中 feed-forward 子层的实现也在该文件中 (transformer_ffn_layer)。</li>
  <li><strong>数据并行类</strong>。devices.py 和 expert_utils.py 配合使用，主要功能是根据用户给定的并行设备参数，列出可以使用的设备名，然后给定一个能够调用这些设备，并行执行方法的方法。</li>
  <li><strong>实验流程控制。</strong>实验流程控制使用的是 Tensorflow 的高级 API 对象，主要包括 Experiment 对象、Estimator 对象、Dataset 对象。对这三个对象，我们可以这么理解：a) Experiment 是一次运行的实验，用来控制实验流程，输送数据到模型。b) Estimator 是具体的模型对象，可以包括训练、评估、解码三个功能。c) Dataset 为运行的实验过程读数据文件提供数据流。</li>
  <li><strong>Experiment 对象</strong>。我们来看下图中 Experiment 初始化所需的形参就能更好的理解 “实验” 这个概念了。Experiment 对象中需要迭代中的各种 step 参数，需要一个 Estimator 对象，两个输入流函数（input）。Experiment 对象在运行中，将数据给到 Estimator 对象，然后控制训练和迭代流程。</li>
</ol>

<p><img src="https://ask.qcloudimg.com/draft/2479598/h7es0f20gq.png?imageView2/2/w/1620" alt="" />图 8 Experiment 对象的部分形参</p>

<p>9.<strong>Estimator 对象</strong>。可以理解为模型对象，可以通过 Estimator 执行模型的训练、评估、解码。Estimator 对象最重要的一个形参是 model_fn，也就是具体执行训练、评估、解码的函数入口。三个入口分别对应着三个 EstimatorSpec 对象，如图 9，10 所示。</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/pbbu1q6r5c.png?imageView2/2/w/1620" alt="" />图 9 Estimator 中最重要的形参是 model_fn</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/np09pmg2ri.png?imageView2/2/w/1620" alt="" />图 10 Estimator 中的三种 model_fn，实现三种功能</p>

<p>从图 10 可以看出，用于训练的 EstimatorSpec 对象需要描述计算图中 feature 和（loss，train_op）之间的关系；用于评估的 EstimatorSpec 对象需要描述计算图中 feature 和（loss，eval_metrics_ops）之间的关系；用于评估的 EstimatorSpec 对象需要描述 features 和 predictions 之间的关系。</p>

<ol>
  <li><strong>Dataset 对象</strong>。该对象是读文件，构造训练和评估的数据流。训练和评估对应着两种不同的数据输入流，如图 11 所示。</li>
</ol>

<p><img src="https://ask.qcloudimg.com/draft/2479598/g0lek0c0om.png?imageView2/2/w/1620" alt="" />图 11 Dataset 对象提供数据流</p>

<p>\11. <strong>Positional encoding 的实现</strong>。论文中的实现和代码中的实现存在公式变形和不一致的情况，可能会导致困惑，故在此指出。论文中 Positional encoding <a href="https://www.baidu.com/s?wd=%E4%B8%AD%E4%B8%89%E8%A7%92&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd">中三角</a>函数的参数部分公式如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/8ozu6hqlk0.png?imageView2/2/w/1620" alt="" /></p>

<p>代码中的实现需要对该公式做变形，以规避数值溢出的风险，公式变形过程如下：</p>

<p><img src="https://ask.qcloudimg.com/draft/2479598/jwq86rrn9o.png?imageView2/2/w/1620" alt="" /></p>

<p>还需要指出的是，论文中根据维度下标的奇偶性来交替使用 sin 和 cos 函数的说法，在代码中并不是这样实现的，而是前一半的维度使用 sin 函数，后一半的维度使用 cos 函数，并没有考虑奇偶性</p>

<p>12. <strong>以 token 数量作为 batch size。</strong>这种方式比起以句子个数作为 batch size 的方式来，能到 batch 占显存的空间更加平均，不会导致因为训练数据导致的显存占用忽上忽下，造成显存空间不够用，导致程序崩溃。</p>

<p>\13. <strong>如何做 mask</strong>。由于模型是以 batch 为单位进行训练的，batch 的句长以其中最长的那个句子为准，其他句子要做 padding。padding 项在计算的过程中如果不处理的话，会引入噪音，所以就需要 mask，来使 padding 项不对计算起作用。mask 在 attention 机制中的实现非常简单，就是在 softmax 之前，把 padding 位置元素加一个极大的负数，强制其 softmax 后的概率结果为 0。举个例子，[1,1,1] 经过 softmax 计算后结果约为 [0.33,0.33,0.33]，[1,1,-1e9] softmax 的计算结果约为 [0.5, 0.5,0]。这样就相当于 mask 掉了数组中的第三项元素。在对 target sequence 进行建模的时候，需要保证每次只 attention 到前 t-1 个单词，这个地方也需要 mask，整体的 mask 是一个上三角矩阵，非 0 元素值为一个极大的负值。</p>

<p>\14. <strong>基于 batch 的解码</strong>。解码的时候，如果是基于文件的，那么就会将句子组成 batch 来并行解码。这里有个小 trick，就是先对句子进行排序，然后从长的句子开始组 batch，翻译，再把句子恢复成原先的顺序返回。这种方式可以很好的检测到显存不足的错误，因为解句子最长的一个 batch 的时候，显存都是够得，那其他的 batch 也不存在问题。</p>

<h1 id="总结">总结</h1>

<p>本文对 Google 的 Tensor2Tensor 系统进行了深度的解读，涉及到了比较多的方面，笔者也还需要对其进行更加深入的学习和研究，希望能够与对该模型以及 DL for NLP 技术感兴趣的同学们一起交流，共同进步！</p>

<blockquote>
  <h2 id="问答"><strong>问答</strong></h2>

  <p><a href="https://cloud.tencent.com/developer/ask/111361?fromSource=waitui">docker 和 docker-compose 有什么不同？</a></p>

  <h2 id="相关阅读"><strong>相关阅读</strong></h2>

  <p><a href="https://cloud.tencent.com/developer/article/1157502?fromSource=waitui">深度学习之神经网络核心原理与算法 - 归一化与参数初始化</a></p>

  <p><a href="https://cloud.tencent.com/developer/article/1157398?fromSource=waitui">启发式寻路算法</a></p>

  <p><a href="https://cloud.tencent.com/developer/article/1157495?fromSource=waitui">深度学习（5）——RBF 算法简介</a></p>
</blockquote>

<p><strong>此文已由作者授权腾讯云 + 社区发布，原文链接：<a href="https://cloud.tencent.com/developer/article/1116709?fromSource=waitui">https://cloud.tencent.com/developer/article/1116709?fromSource=waitui</a></strong></p>

<p><strong>欢迎大家前往腾讯云 + 社区或关注云加社区微信公众号（QcloudCommunity），第一时间获取更多海量技术实践干货哦~</strong></p>

<p>海量技术实践经验，尽在云加社区！ <a href="https://cloud.tencent.com/developer?fromSource=waitui">https://cloud.tencent.com/developer?fromSource=waitui</a></p>

<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet" />


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://github.com/zergtant/pytorch-handbook/blob/master/chapter3/3.2-mnist.ipynb</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>&nbsp;
    
        <a href="/tag/#cnn" title="Tag: cnn" rel="tag">cnn</a>&nbsp;
    
        <a href="/tag/#cv" title="Tag: cv" rel="tag">cv</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.4-cnn.ipynb</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Word2Vec_Tutorial_part2/">Word2Vec Tutorial part2</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</p>
</blockquote>

<h1 id="word2vec-tutorial-part-2---negative-sampling">Word2Vec Tutorial Part 2 - Negative Sampling</h1>

<p>11 Jan 2017</p>

<p>In part 2 of the word2vec tutorial (here’s <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">part 1</a>), I’ll cover a few additional modifications to the basic skip-gram model which are important for actually making it feasible to train.</p>

<p>When you read the tutorial on the skip-gram model for Word2Vec, you may have noticed something–it’s a huge neural network!</p>

<p>In the example I gave, we had word vectors with 300 components, and a vocabulary of 10,000 words. Recall that the neural network had two weight matrices–a hidden layer and output layer. Both of these layers would have a weight matrix with 300 x 10,000 = 3 million weights each!</p>

<p>Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast.</p>

<p>The authors of Word2Vec addressed these issues in their second <a href="http://arxiv.org/pdf/1310.4546.pdf">paper</a>.</p>

<p>There are three innovations in this second paper:</p>

<ol>
  <li>Treating common word pairs or phrases as single “words” in their model.</li>
  <li>Subsampling frequent words to decrease the number of training examples.</li>
  <li>Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.</li>
</ol>

<p>It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.</p>

<h1 id="word-pairs-and-phrases">Word Pairs and “Phrases”</h1>

<p>The authors pointed out that a word pair like “Boston Globe” (a newspaper) has a much different meaning than the individual words “Boston” and “Globe”. So it makes sense to treat “Boston Globe”, wherever it occurs in the text, as a single word with its own word vector representation.</p>

<p>You can see the results in their published model, which was trained on 100 billion words from a Google News dataset. The addition of phrases to the model swelled the vocabulary size to 3 million words!</p>

<p>If you’re interested in their resulting vocabulary, I poked around it a bit and published a post on it <a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/">here</a>. You can also just browse their vocabulary <a href="https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary">here</a>.</p>

<p>Phrase detection is covered in the “Learning Phrases” section of their <a href="http://arxiv.org/pdf/1310.4546.pdf">paper</a>. They shared their implementation in word2phrase.c–I’ve shared a commented (but otherwise unaltered) copy of this code <a href="https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2phrase.c">here</a>.</p>

<p>I don’t think their phrase detection approach is a key contribution of their paper, but I’ll share a little about it anyway since it’s pretty straightforward.</p>

<p>Each pass of their tool only looks at combinations of 2 words, but you can run it multiple times to get longer phrases. So, the first pass will pick up the phrase “New_York”, and then running it again will pick up “New_York_City” as a combination of “New_York” and “City”.</p>

<p>The tool counts the number of times each combination of two words appears in the training text, and then these counts are used in an equation to determine which word combinations to turn into phrases. The equation is designed to make phrases out of words which occur together often relative to the number of individual occurrences. It also favors phrases made of infrequent words in order to avoid making phrases out of common words like “and the” or “this is”.</p>

<p>You can see more details about their equation in my code comments <a href="https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2phrase.c#L389">here</a>.</p>

<p>One thought I had for an alternate phrase recognition strategy would be to use the titles of all Wikipedia articles as your vocabulary.</p>

<h1 id="subsampling-frequent-words">Subsampling Frequent Words</h1>

<p>In part 1 of this tutorial, I showed how training samples were created from the source text, but I’ll repeat it here. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p>

<p><a href="http://mccormickml.com/assets/word2vec/training_data.png"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data" /></a></p>

<p>There are two “problems” with common words like “the”:</p>

<ol>
  <li>When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.</li>
  <li>We will have many more samples of (“the”, …) than we need to learn a good vector for “the”.</li>
</ol>

<p>Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency.</p>

<p>If we have a window size of 10, and we remove a specific instance of “the” from our text:</p>

<ol>
  <li>As we train on the remaining words, “the” will not appear in any of their context windows.</li>
  <li>We’ll have 10 fewer training samples where “the” is the input word.</li>
</ol>

<p>Note how these two effects help address the two problems stated above.</p>

<h3 id="sampling-rate">Sampling rate</h3>

<p>The word2vec C code implements an equation for calculating a probability with which to keep a given word in the vocabulary.</p>

<p>wiwi is the word, z(wi)z(wi) is the fraction of the total words in the corpus that are that word. For example, if the word “peanut” occurs 1,000 times in a 1 billion word corpus, then z(‘peanut’) = 1E-6.</p>

<p>There is also a parameter in the code named ‘sample’ which controls how much subsampling occurs, and the default value is 0.001. Smaller values of ‘sample’ mean words are less likely to be kept.</p>

<p>P(wi)P(wi) is the probability of <em>keeping</em> the word:</p>

<p>P(wi)=(z(wi)0.001−−−−−√+1)⋅0.001z(wi)P(wi)=(z(wi)0.001+1)⋅0.001z(wi)</p>

<p>You can plot this quickly in Google to see the shape.</p>

<p><a href="http://mccormickml.com/assets/word2vec/subsample_func_plot.png"><img src="http://mccormickml.com/assets/word2vec/subsample_func_plot.png" alt="Plot of subsampling function" /></a></p>

<p>No single word should be a very large percentage of the corpus, so we want to look at pretty small values on the x-axis.</p>

<p>Here are some interesting points in this function (again this is using the default sample value of 0.001).</p>

<ul>
  <li>
    <p>P(wi)=1.0P(wi)=1.0</p>

    <p>(100% chance of being kept) when</p>

    <p>z(wi)&lt;=0.0026z(wi)&lt;=0.0026</p>

    <p>.</p>

    <ul>
      <li>This means that only words which represent more than 0.26% of the total words will be subsampled.</li>
    </ul>
  </li>
  <li>
    <p>P(wi)=0.5P(wi)=0.5 (50% chance of being kept) when z(wi)=0.00746z(wi)=0.00746.</p>
  </li>
  <li>
    <p>P(wi)=0.033P(wi)=0.033</p>

    <p>(3.3% chance of being kept) when</p>

    <p>z(wi)=1.0z(wi)=1.0</p>

    <p>.</p>

    <ul>
      <li>That is, if the corpus consisted entirely of word wiwi, which of course is ridiculous.</li>
    </ul>
  </li>
</ul>

<p>You may notice that the paper defines this function a little differently than what’s implemented in the C code, but I figure the C implementation is the more authoritative version.</p>

<h1 id="negative-sampling">Negative Sampling</h1>

<p>Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak <em>all</em> of the weights in the neural network.</p>

<p>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!</p>

<p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.</p>

<p>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for <em>all</em> of the other thousands of output neurons to output a 0.</p>

<p>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).</p>

<p>The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.</p>

<p>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!</p>

<p>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).</p>

<h3 id="selecting-negative-samples">Selecting Negative Samples</h3>

<p>The “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.</p>

<p>For instance, suppose you had your entire training corpus as a list of words, and you chose your 5 negative samples by picking randomly from the list. In this case, the probability for picking the word “couch” would be equal to the number of times “couch” appears in the corpus, divided the total number of word occus in the corpus. This is expressed by the following equation:</p>

<p>P(wi)=f(wi)∑nj=0(f(wj))P(wi)=f(wi)∑j=0n(f(wj))</p>

<p>The authors state in their paper that they tried a number of variations on this equation, and the one which performed best was to raise the word counts to the 3/4 power:</p>

<p>P(wi)=f(wi)3/4∑nj=0(f(wj)3/4)P(wi)=f(wi)3/4∑j=0n(f(wj)3/4)</p>

<p>If you play with some sample values, you’ll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.</p>

<p>The way this selection is implemented in the C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by P(wi)P(wi) * table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those.</p>

<h2 id="other-resources">Other Resources</h2>

<p>For the most detailed and accurate explanation of word2vec, you should check out the C code. I’ve published an extensively commented (but otherwise unaltered) version of the code <a href="https://github.com/chrisjmccormick/word2vec_commented">here</a>.</p>

<p>Also, did you know that the word2vec model can also be applied to non-text data for recommender systems and ad targeting? Instead of learning vectors from a sequence of words, you can learn vectors from a sequence of user actions. Read more about this in my new post <a href="http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/">here</a>.</p>

<p>Finally, I’ve also created a <a href="http://mccormickml.com/2016/04/27/word2vec-resources/">post</a> with links to and descriptions of other word2vec tutorials, papers, and implementations.</p>

<h3 id="cite">Cite</h3>

<p>McCormick, C. (2017, January 11). <em>Word2Vec Tutorial Part 2 - Negative Sampling</em>. Retrieved from http://www.mccormickml.com</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Word2Vec_Tutorial_part2/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Word2Vec-Tutorial/">Word2Vec Tutorial</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</p>
</blockquote>

<h1 id="word2vec-tutorial---the-skip-gram-model">Word2Vec Tutorial - The Skip-Gram Model</h1>

<p>19 Apr 2016</p>

<p><strong>UPDATE:</strong> I’m proud to announce that I’ve published my first eBook, <em>The Inner Workings of word2vec</em>. It includes all of the material in this post series, but goes deeper with additional topics like CBOW and Hierarchical Softmax, as well as example code that demonstrates the algorithm details in action. I’m continuing to add more topics and code to the book–picking it up now entitles you to receive all future revisions. Thanks for your support!</p>

<p>This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.</p>

<h1 id="the-model">The Model</h1>

<p>The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s all of the little tweaks and enhancements that start to clutter the explanation.</p>

<p>Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.</p>

<p>Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.</p>

<h1 id="the-fake-task">The Fake Task</h1>

<p>So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.</p>

<p>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p>

<p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).</p>

<p>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.</p>

<p>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p>

<p><a href="http://mccormickml.com/assets/word2vec/training_data.png"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data" /></a></p>

<p>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.</p>

<h1 id="model-details">Model Details</h1>

<p>So how is this all represented?</p>

<p>First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.</p>

<p>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.</p>

<p>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.</p>

<p>Here’s the architecture of our neural network.</p>

<p><a href="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png"><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="Skip-gram Neural Network Architecture" /></a></p>

<p>There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.</p>

<p>When <em>training</em> this network on word pairs, the input is a one-hot vector representing the input word and the training output <em>is also a one-hot vector</em>representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, <em>not</em> a one-hot vector).</p>

<h1 id="the-hidden-layer">The Hidden Layer</h1>

<p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p>

<p>300 features is what Google used in their published model trained on the Google news dataset (you can download it from <a href="https://code.google.com/archive/p/word2vec/">here</a>). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).</p>

<p>If you look at the <em>rows</em> of this weight matrix, these are actually what will be our word vectors!</p>

<p><a href="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png"><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="Hidden Layer Weight Matrix" /></a></p>

<p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p>

<p>Let’s get back, though, to working through the definition of this model that we’re going to train.</p>

<p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p>

<p><a href="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png"><img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" alt="Effect of matrix multiplication with a one-hot vector" /></a></p>

<p>This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.</p>

<h1 id="the-output-layer">The Output Layer</h1>

<p>The <code class="highlighter-rouge">1 x 300</code> word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">here</a>, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.</p>

<p>Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function <code class="highlighter-rouge">exp(x)</code> to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from <em>all</em> 10,000 output nodes.</p>

<p>Here’s an illustration of calculating the output of the output neuron for the word “car”.</p>

<p><a href="http://mccormickml.com/assets/word2vec/output_weights_function.png"><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="Behavior of the output neuron" /></a></p>

<p>Note that neural network does not know anything about the offset of the output word relative to the input word. It <em>does not</em> learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, <em>every single occurrence</em> of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ <em>is not</em> 100%; you may have picked one of the other words in the vicinity.</p>

<h1 id="intuition">Intuition</h1>

<p>Ok, are you ready for an exciting bit of insight into this network?</p>

<p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p>

<p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p>

<p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p>

<h1 id="next-up">Next Up</h1>

<p>You may have noticed that the skip-gram neural network contains a huge number of weights… For our example with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive, so the word2vec authors introduced a number of tweaks to make training feasible. These are covered in <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">part 2 of this tutorial</a>.</p>

<p>Did you know that the word2vec model can also be applied to non-text data for recommender systems and ad targeting? Instead of learning vectors from a sequence of words, you can learn vectors from a sequence of user actions. Read more about this in my new post <a href="http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/">here</a>.</p>

<h1 id="other-resources">Other Resources</h1>

<p>I’ve also created a <a href="http://mccormickml.com/2016/04/27/word2vec-resources/">post</a> with links to and descriptions of other word2vec tutorials, papers, and implementations.</p>

<h3 id="cite">Cite</h3>

<p>McCormick, C. (2016, April 19). <em>Word2Vec Tutorial - The Skip-Gram Model</em>. Retrieved from http://www.mccormickml.com</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Word2Vec-Tutorial/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <a href="/index.html" class="previous"><i class="fa fa-angle-double-left"></i></a>
            <a href="/" class="previous"><i class="fa fa-angle-left"></i></a>
          
          <span class="page_number ">2/6</span>
          
            <a href="/page3" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page6" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/04/04/Get_To_The_Point_Summarization_with_Pointer-Generator_Networks%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">Get To The Point: Summarization with Pointer-Generator Networks论文笔记</a></li>
                    
                        <li><a href="/2019/03/28/The_Evolved_Transformer_%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">《The Evolved Transformer》论文总结</a></li>
                    
                        <li><a href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/">《Pretraining-Based Natural Language Generation for Text Summarization》论文解读</a></li>
                    
                        <li><a href="/2019/03/20/%E6%96%87%E6%91%98/">文摘</a></li>
                    
                        <li><a href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/">chat-bot</a></li>
                    
                        <li><a href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/">自然语言处理的是个发展趋势</a></li>
                    
                        <li><a href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/">华为李航-NLP有个基本问题，深度学习4个做的很好</a></li>
                    
                        <li><a href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/">全面解析Tensor2Tensor系统</a></li>
                    
                        <li><a href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a></li>
                    
                        <li><a href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">17</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#reproduction" class="categories-list-item" cate="reproduction">
                            <span class="name">
                                reproduction
                            </span>
                            <span class="badge">11</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#digest" class="categories-list-item" cate="digest">
                            <span class="name">
                                digest
                            </span>
                            <span class="badge">1</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#NLP" class="categories-list-item" cate="NLP">
                            <span class="name">
                                NLP
                            </span>
                            <span class="badge">2</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 11pt; color: #777;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 11pt; color: #777;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 10pt; color: #888;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 14pt; color: #444;">pytorch</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 9pt; color: #999;">markdown</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cnn" style="font-size: 9pt; color: #999;">cnn</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cv" style="font-size: 9pt; color: #999;">cv</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#tensorflow" style="font-size: 9pt; color: #999;">tensorflow</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#life" style="font-size: 9pt; color: #999;">life</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#Feature_Extractor" style="font-size: 9pt; color: #999;">Feature_Extractor</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#summarization" style="font-size: 9pt; color: #999;">summarization</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
