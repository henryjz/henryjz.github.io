<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/page2/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/">自然语言与多模态交互前沿技术</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-06
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h3 id="北大ai第三讲何晓东-京东ai研究院常务副院长ieee-fellow自然语言与多模态交互前言技术">北大AI第三讲：何晓东-京东AI研究院常务副院长、IEEE Fellow：自然语言与多模态交互前言技术</h3>

<h4 id="nlp进展">NLP进展：</h4>

<ul>
  <li>语言理解/语义槽值提取</li>
  <li>语言理解/意图分类，2016年提出层次化注意力模型（HAN），以更好的在词、句子、段落、等多个层面来理解语言，判断意图，并通过对神经元激活的的可视化给出一定程度的可解释性。</li>
  <li>语言理解/语义的表征：从自然语言中提取出语义并将其投影到语义空间以帮助搜索、推荐、分类、问答等应用；自然语言的描述，通过深度神经网络逐步抽取语义上的不变形（invariance），生成抽象的语义表征。</li>
  <li>机器阅读理解（MRC）：机器阅读文本，回答问题；BERT模型在SQuAD封闭数据集上的成绩已经超过人类</li>
</ul>

<h4 id="未来">未来</h4>

<ul>
  <li>多模态智能：综合文字、语音、图像、知识图谱等信息来获取信息
    <ul>
      <li>建立多模态语义空间：联结图像和文字
        <ul>
          <li>通过深度结构语义模型（DSSM）把图像和文字表征成语义空间内的向量</li>
          <li>在此空间中进行语义相似度计算，生成最匹配图像内容的文字表述。</li>
        </ul>
      </li>
      <li>理解场景和知识，用语言表达（image caption）
        <ul>
          <li>一个棒球</li>
          <li>一个棒球运动员</li>
          <li>一个棒球运动员在扔</li>
          <li>一个棒球运动员在扔一个球</li>
        </ul>
      </li>
      <li>图像描述机器人：CaptionBot</li>
      <li>智能绘画机器人：AI根据语言描述创作绘画</li>
      <li>AI+Art：更多（艺术化的）创作</li>
      <li>综合图像和语言推理，回答问题：eg.那两把蓝色椅子之间是什么？</li>
      <li>视觉-语言多模态导航：结合语言理解和对环境的视觉信息建模，机器人按指令从一个地方走到另一个地方。</li>
    </ul>
  </li>
  <li>复杂内容创作：比如人工智能写作（长文章）
    <ul>
      <li>创作长文的技术挑战：
        <ul>
          <li>从简单输入到创作长文需要大量内容的扩充</li>
          <li>长文的生成要可控，能满足组合爆炸式需求。模型需要时组合性的，能与训练的。</li>
          <li>现有的端到端的模型不适合长文创作：为短文本生成（如机器翻译）而设计，不能抓住长文的高层语义。度量优化，信用分配，维持一致性，目标函数平衡等要重新设计。</li>
        </ul>
      </li>
      <li>长文创作的前沿探索：比如顶层设计和规划
        <ul>
          <li>现有的文本生成模型缺乏“规划”，应先产生粗略的高层主题规划，然后再对主题和子主题展开长文</li>
          <li>最近的一些工作：多层增强学习模型及其在主题设计和长文生成中的应用。</li>
        </ul>
      </li>
      <li>创作诗歌（控制）</li>
    </ul>
  </li>
  <li>情感智能：不只识别人的情感，还能像人一样表达情感和风格
    <ul>
      <li>生成带情感的语言：让AI在语言表达中加入情感，提升用户体验</li>
      <li>表达情感和风格：让AI用语言表达浪漫或者幽默的风格——StyleNet</li>
    </ul>
  </li>
  <li>多轮人机对话：理解语境、常识、语言，生成逻辑严谨的有情感的对话，服务于人
    <ul>
      <li>图灵测试：通过人类和机器之间的自然语言对话来判断机器是否具有智能。</li>
      <li>主要的人机对话系统框架：任务型对话系统、问答型对话系统、聊天型对话系统、检索性对话系统</li>
      <li>我们将成为有史以来第一代与AI共生的人类，《从Eliza到小冰：社交对话机器人的机遇与挑战》</li>
    </ul>
  </li>
</ul>

<h4 id="ai产业化的下一个方向是什么">AI产业化的下一个方向是什么</h4>

<ul>
  <li>
    <p>智能服务产业是新蓝海：传统人类密集型产业，有广阔自动化、智能化空间；随着AI技术、IOT技术等的创新，市场在快速成长。</p>
  </li>
  <li>
    <p>服务型对话</p>

    <ul>
      <li>服务：生活、娱乐、消费、客服等为人提供的的服务</li>
      <li>对话：多模态、大规模开放领域，具有常识和情感、能完成复杂任务的智能交互技术。</li>
    </ul>
  </li>
  <li>
    <p>产业界应用：</p>

    <ul>
      <li>京东客服机器人：首个大规模商用情感客服机器人：能够检测用户的情感类型，做出道歉、安抚、祝福的动作，提升用户体验。</li>
      <li>京东智能服务产品矩阵：JIMI和AlphaSales</li>
      <li>京东智能IoT</li>
      <li>京东智能市政服务</li>
    </ul>
  </li>
  <li>
    <p>人机融合、多模态智能服务的产业时代</p>

    <ul>
      <li>
        <p>服务即对话：多模态、大规模开放领域、具有常识和情感、能完成复杂任务的对话系统是推动下一代智能产业的核心技术。</p>

        <table>
          <thead>
            <tr>
              <th>分级</th>
              <th>目标</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>低级智能对话</td>
              <td>对用户简单意图进行识别并给出预设答案</td>
            </tr>
            <tr>
              <td>初级智能对话</td>
              <td>能识别复杂意图，联系上下文给出回答</td>
            </tr>
            <tr>
              <td>中级智能对话</td>
              <td>根据用户问题及情绪完成个性化多轮对话，协助用户完成目标</td>
            </tr>
            <tr>
              <td>高级智能对话</td>
              <td>能对多模态信息进行推理，自主判断，并组织语言与用户沟通，具备自我学习能力</td>
            </tr>
            <tr>
              <td>通用智能对话</td>
              <td>能基于一切信息开展自我学习，自我适应，及自我创新。在复杂问题领域达到人类水平。</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<h4 id="提问环节">提问环节</h4>

<ul>
  <li>关于OpenAI的GPT模型暴力解决问题的看法
    <ul>
      <li>算法是解决问题的核心，但是光有算法是不够的。GPT2是算法和产业界结合的一个例子，有大量高质量的数据和算力。对科研机构不算一个坏事。算法是灵魂，数据和算力是物质基础</li>
    </ul>
  </li>
  <li>NLP领域的问题是否可以认为比图像领域的问题要难解决，所有在进度上有所落后？
    <ul>
      <li>NLP领域的问题是一种认知领域的问题，比图像领域的感知问题要复杂一些，所以图像领域的问题能够比较清晰的被定义，相对而言也会推进的更加深入。</li>
    </ul>
  </li>
  <li>如何判断机器是否真正理解了人类的问题？
    <ul>
      <li>这是一个哲学问题，科学家的作用就是将哲学问题转化成科学问题，可以通过定义一些测试任务来进行一定程度上的判断。比如小冰提出了聊天轮数的metric，作为判断聊天机器人是否能像人类一样进行聊天。</li>
    </ul>
  </li>
  <li>智能音响等IoT产品的发展趋势？
    <ul>
      <li>每一次交互的革命都能带来一个万亿级别的产业，智能IoT就具有这样的潜力</li>
    </ul>
  </li>
  <li>如何看待当前CV行业如火如荼，NLP行业相对比较平静的现象？
    <ul>
      <li>当问题已经定义的很清楚的时候，机会相对来说就小了很多。智能服务产业的未来可能要比CV的智能安防产业还要大。</li>
    </ul>
  </li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/">最近阅读文章小抄</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-06
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h4 id="2019121nlp-领域的-imagenet-时代到来词嵌入已死语言模型当立">2019.1.21–《NLP 领域的 ImageNet 时代到来：词嵌入「已死」，语言模型当立》</h4>

<p>计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/">Transformer在NLP词向量预训练中的应用</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-02-28
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h4 id="bert发展沿革">BERT发展沿革</h4>

<h5 id="四个部分">四个部分</h5>

<ul>
  <li>回顾Transformer的网络结构、和RNN、RNN的对比</li>
  <li>介绍ELMo模型语境化的词嵌入，重点介绍其双向语言模型和根据具体语境生成词嵌入的原理</li>
  <li>介绍三种使用Transformer作为特征提取器的网络：GPT、BERT。分别讨论其思路、原理、输入输出方式、下游任务的匹配方式，介绍他们的联系和区别</li>
</ul>

<h5 id="一回顾transformer">一：回顾Transformer</h5>

<p>​	RNN的两个缺点：1.并行性能力差；2.捕获长期依赖的能力差</p>

<p>​	第一点：RNN之所以是RNN，能将其和其它模型区分开的最典型标志是：T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt，另外一个输入，T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)，这种序列依赖关系使得RNN无法并行计算，只能按着时间步一个单词一个单词往后走。</p>

<p>​	第二点：RNN长期依赖的根本问题是，经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。梯度爆炸可以使用梯度修剪的方式解决。梯度消失的问题在引进LSTM和GRU之后也得到了解决，然而LSTM或者GRU都没有解决RNN无法并行计算的局限性。新的特征提取器Transformer能够同时解决这两个问题。</p>

<p>​	Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，简单来说就是每个单词都会产生三个向量——query、key、value，当前单词的query向量和其他单词的key向量进行内积操作，并且进行softmax归一化之后会得到当前单词和其他单词的注意力打分，然后使用注意力打分对所有的单词的值向量做加权求和。Transformer是用位置函数来进行位置编码的。Self attention层的输出会传递到前馈（feed-forward）神经网络中，每个位置的单词对应的前馈神经网络都完全一样，不是共享参数的而是各自独立的。前馈神经网络的输出就是对于特定单词想要得到的最终的词嵌入。</p>

<h5 id="二语境化的词嵌入elmo">二：语境化的词嵌入ELMo</h5>

<p><strong>《ELMO：Deep contextualized word representations》</strong>是NAACL 2018的最佳论文，全称为Embedding from Language Models，它解决了以往使用RNN做为特征提取器的Word Embedding网络的一个没有解决的问题：语义多样性的问题，比如一个单词“bank”“有多种含义，取决于它的上下文是什么。ELMO的<strong>本质思想</strong>是：先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p>

<p><img src="https://pic4.zhimg.com/80/v2-fe335ea9fdcd6e0e5ec4a9ac0e2290db_hd.jpg" alt="" /></p>

<p>具体的<strong>实现原理</strong>：它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 的上下文去正确预测单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> ， <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。</p>

<p><strong>总结</strong>：ELMo使用了双向语言建模的方式使得词嵌入能够获取上下文的信息；ELMo根据具体语境下将单词的三个Embedding融合的方式解决多语义的问题。<strong>不足之处</strong>：BiLSTM的特征提取能力要显著低于Transformer。</p>

<h5 id="三gpt">三：GPT</h5>

<p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，ELMO在做语言模型预训练的时候，预测单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。</p>

<p><img src="https://pic1.zhimg.com/80/v2-5028b1de8fb50e6630cc9839f0b16568_hd.jpg" alt="" /></p>

<p>总结：GPT在BERT之前使用Transformer+两阶段训练的方式获取word Embedding，这种做法已经成为了NLP预训练模型的标准方式，所以GPT的贡献是具有开创性的。但是因为没有使用双向语言模型使得其效果很快被BERT超越，从事后看，BERT本质上也就是比GPT多使用了双向语言模型。</p>

<h5 id="四bert">四：BERT</h5>

<p>BERT=Transformer+双向语言建模预训练+下游任务Fine-tunning</p>

<p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。</p>

<p><img src="https://pic3.zhimg.com/v2-330788d33e39396db17655e42c7f6afa_r.jpg" alt="" /></p>

<p><strong>第一阶段的预训练</strong></p>

<p>BERT 的创新点在于它将双向 Transformer 用于语言模型，没有使用传统的从左到右或从右到左的语言模型来预训练 BERT，而是使用两个新型无监督预测任务。</p>

<p>任务 #1：Masked LM</p>

<p>在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。</p>

<p>这样就需要：</p>

<ol>
  <li>在 encoder 的输出上添加一个分类层</li>
  <li>用嵌入矩阵乘以输出向量，将其转换为词汇的维度</li>
  <li>用 softmax 计算词汇表中每个单词的概率</li>
</ol>

<p>BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/1667471-29bc20334044e169.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/777/format/webp" alt="" /></p>

<p>任务 #2：下一句预测</p>

<p>在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
 在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。</p>

<p>为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：</p>

<ol>
  <li>在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。</li>
  <li>将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。</li>
  <li>给每个 token 添加一个位置 embedding，来表示它在序列中的位置。</li>
</ol>

<p>为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：</p>

<ol>
  <li>整个输入序列输入给 Transformer 模型</li>
  <li>用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量</li>
  <li>用 softmax 计算 IsNextSequence 的概率</li>
</ol>

<p>在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。</p>

<p><strong>第二阶段微调Fine-tuning</strong></p>

<p>BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：</p>

<ol>
  <li>在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层</li>
  <li>在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。</li>
  <li>在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。</li>
</ol>

<p><img src="https://upload-images.jianshu.io/upload_images/1667471-aa82f64085510604.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/522/format/webp" alt="" /></p>

<ul>
  <li>句子关系类任务，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。</li>
  <li>对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；</li>
  <li>对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。</li>
  <li>生成类任务,尽管Bert论文没有提，最简单的是直接在单个Transformer结构上加装隐层产生输出，更复杂一点就是使用encoder-decoder结构，编码器和解码器都是用预训练的词嵌入进行初始化。</li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">流畅的python笔记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2018-07-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#python" title="Tag: python" rel="tag">python</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>[TOC]</p>

<h3 id="1鸭子模型">1.鸭子模型</h3>

<p>在程序设计中，鸭子类型（duck typing）是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由”当前方法和属性的集合”决定。“鸭子测试”可以这样表述:</p>

<blockquote>
  <p>一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟可以被称为鸭子“</p>
</blockquote>

<p>在鸭子类型中，关注点在于对象的行为，能作什么；而不是关注对象所属的类型。例如，在不使用鸭子类型的语言中，我们可以编写一个函数，它接受一个类型为”鸭子”的对象，并调用它的”走”和”叫”方法。在使用鸭子类型的语言中，这样的一个函数可以接受一个任意类型的对象，并调用它的”走”和”叫”方法。如果这些需要被调用的方法不存在，那么将引发一个运行时错误。任何拥有这样的正确的”走”和”叫”方法的对象都可被函数接受的这种行为引出了以上表述，这种决定类型的方式因此得名。</p>

<p>鸭子类型通常得益于”不”测试方法和函数中参数的类型，而是依赖文档、清晰的代码和测试来确保正确使用。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#使用鸭子类型处理单个字符串或由字符串组成的可迭代对象
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">field_names</span> <span class="o">=</span> <span class="n">field_names</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">','</span><span class="p">,</span><span class="s">' '</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">AttributeError</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="n">field_names</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">field_names</span><span class="p">)</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/">深度学习小记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2018-07-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#deep_learning" title="Tag: deep_learning" rel="tag">deep_learning</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul>
  <li>人工智能早期，计算机能够迅速解决那些对人类智力来说非常困难的问题。而人工智能的真正挑战在于解决那些对人来说很容易执行，但很难形式化描述的任务，如识别人们所说的话或图像中的脸。</li>
  <li>深度学习讨论的是一种让计算机从经验中学习，并根据层次化的概念来__理解世界__的解决方案，而每个概念则通过某些相对简单的概念之间的关系来定义</li>
  <li>人类擅长对事物抽象因而能够认识世界，计算机则只能做一些形式化的数据处理，人工智能要做的就是通过形式化的数据处理__从另一条路径__达到认识世界的目的</li>
  <li>深度学习
    <ul>
      <li>计算机难以理解原始感官输入数据的含义，如表示为像素值集合的图像。将一组像素映射到对象标识的函数非常复杂。</li>
      <li>深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个模型的不同层描述）
        <ul>
          <li>可见层：包含我们能观察到的变量</li>
          <li>隐藏层：它们的值不再数据中给出，所以称为隐藏层。模型必须确定那些概念有利于解释观察数据中关系</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>课程组织
    <ul>
      <li>介绍基本的数学工具和机器学习的概念</li>
      <li>介绍最成熟的深度学习算法</li>
      <li>讨论某些具有展望性的想法，他们被广泛的认为是深度学习未来的研究重点</li>
    </ul>
  </li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2018/07/16/%E7%A1%95%E5%A3%AB%E7%8F%AD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E6%89%8B%E5%86%8C-%E8%A7%82%E7%82%B9%E6%94%B6%E8%8E%B7/">如何阅读论文</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2018-07-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#paper" title="Tag: paper" rel="tag">paper</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h1 id="硕士班研究所新生手册观点收获">硕士班研究所新生手册(观点收获)</h1>

<p>Issued by and valid in the PPSC Lab.Directed by Prof.MH.Perng</p>

<h3 id="论文的要求">论文的要求</h3>

<ul>
  <li>论文的主要内容，是叙述一套方法在一个特定场合中的应用</li>
  <li>这套方法必须要有所创新或突破，并因而对学术界有所贡献。因此，他或者是解决既有问题的新方法，或者是既有方法的新应用，或者是以一个新的方法开启一整片新的应用领域。</li>
  <li>提出足够的证据来让读者信服说：针对这个应用场合，你所提出的方法确实比文献中一切方法有更优越之处</li>
  <li>清楚的指出这个方法在应用上的限制，并且提出充分证据来说服读者，任何应用场合，只要能满足你所提出来的假设（前提）条件，你的方法一定适用，而且你所描述的有点就一定存在</li>
  <li>清楚指出这个方法的限制和可能的缺点</li>
  <li>清楚地交代这个方法的应用程序以及所有仿真或实验结果的过程，使得这个专业领域内的任何读者，都有办法根据你的描述，在他的实验室中复制出你的研究成果。</li>
  <li>你对这个方法中每一个步骤都必须要提供充分的理由说明“为什么非如此不可”。</li>
  <li>论文必须在适当的位置清楚注明所有和你研究之题目相关的文献，而且是和你所研究的问题相关的学术文献（尤其是学术期刊论文），你都有必要全部找出来（如果漏掉就是你的过失），仔细读过。假如在学位论文口试时，有口试委员指出有一篇既有文献，在你所讨论的问题中处理的比你的方法还好，这就构成你论文无法及格的充分理由</li>
  <li>所谓对学术界的贡献指的是：把你的所有研究成果扣除学术界已经发表的所有成果（不管你实际有没有参考过，没有参考过也算是你的重大过失），剩下的就是你的贡献。假如这个贡献太少，也构成你的论文无法及格的充分理由。</li>
</ul>

<h3 id="完成硕士论文所需要的能力">完成硕士论文所需要的能力</h3>

<ul>
  <li>数据检索的能力
    <ul>
      <li>有能力利用数据检索系统（教育部[博硕士论文检索系统]、Compendex和SCI这三套论文数据索引系统），查处所有相关的论文，而无任何遗漏</li>
      <li>关键词很重要，假如你用的关键词太一般化，通常找到的集合会太大，除了所有相关文献之外还加上好几十倍的不相关的文献</li>
    </ul>
  </li>
  <li>资料筛选能力：在你找到的百来篇和你的研究子题直接且密切相关的论文中，你如何只读论文的题目、摘要、简介和结论，而还没有完全看懂内文，就准确的判断这篇论文中是否值得你进一步参考的内容，以便快速的把需要仔细读完的论文从数百篇为二三十篇。</li>
  <li>期刊论文的阅读能力：硕士毕业生和大学毕业生的最大区别就是：学士只需要吸收系统的能力（也就是读别人整理、组织好的知识，典型的就是课本），但硕士则学习过自己从无组织的知识中检索、筛选、组织知识的能力</li>
  <li>期刊论文的分析能力
    <ul>
      <li>一个严格训练过的合格硕士，他做事的时候应该是不需要有人在背后替他做检证，他自己就应该要有能力分析自己的优缺点，主动向上级或者平行单位要求支持。其实，至少要能够完成这个能力，才勉强可以说你是有“独立自主的判断能力。</li>
    </ul>
  </li>
  <li>创新的能力
    <ul>
      <li>大学毕业生的主要能力是吸收既有知识，但硕士毕业生却应该要有能力创造知识</li>
    </ul>
  </li>
</ul>

<h3 id="为什么要坚持培养阅读与分析期刊论文的能力">为什么要坚持培养阅读与分析期刊论文的能力</h3>

<ul>
  <li>对于那些之想学现成技术而不想研究方法的学生，十年后可能会因为不会读期刊论文而面临提前退休</li>
  <li>技术的创新不是完全靠天才，只要学会分析期刊论文的优缺点和一套技术创新的方法，几乎就可以轻易的组合出你所需要的绝大部分创意。</li>
</ul>

<h3 id="期刊论文的分析技巧和程序">期刊论文的分析技巧和程序</h3>

<ul>
  <li>Abstract
    <ul>
      <li>说明这篇论文的主要贡献、方法特色与主要内容，需要培养只看Abstract和Inroduction便可以判断出这篇论文和你研究的有没有直接关系的能力，从而决定要不要把它读完。</li>
    </ul>
  </li>
  <li>Introduction
    <ul>
      <li>功能是介绍问题的背景和起源，交代前人在这个题目上已经有过的主要贡献和遗留的问题</li>
    </ul>
  </li>
  <li>main body
    <ul>
      <li>这篇论文的假设及其成立的难度以判断其参考价值</li>
      <li>在这些假设下，这篇论文的主要好处</li>
      <li>这些好处主要表现在那些公式的那些项目的简化上，从中可以评估出这个方法使用上的方便程度或者计算效率</li>
      <li>不需要完全弄懂一片论文所有的恒等式推导过程或者把整篇论文细细读完，只需要把确定会用到的部分完全搞懂就好，不确定或者不会用到的地方，只需要了解他的主要点子就够了。</li>
    </ul>
  </li>
</ul>

<h3 id="论文阅读的补充说明">论文阅读的补充说明</h3>

<ul>
  <li>硕士生开始读论文的时候容易犯的毛病：
    <ul>
      <li>老是想逐行读懂，有一行读不懂就受不了</li>
      <li>不敢发挥自己的想象，读论文像在读教科书，论文没写的就不会，自己去猜测或想象的时候，老怕弄错作者的意思</li>
    </ul>
  </li>
  <li>每次读论文要呆着问题去阅读，只图你要回答的问题。因此要有选择的阅读，由粗而细，一定是一整批一起读懂到某个层次，而不是逐篇逐篇的一次读懂</li>
</ul>

<h3 id="论文报告的要求与技巧">论文报告的要求与技巧</h3>

<ul>
  <li>第一页列出论文的题目、作者、论文出处与年份</li>
  <li>每一页幻灯片只能讲一个观念</li>
  <li>说明这篇论文所研究的问题的重点，以及这个问题可能和工业界的哪些应用有关</li>
  <li>清楚交代这篇论文的主要假设、主要公式与主要应用方式（以及应用上可能的解题流程）</li>
  <li>说明这篇论文的范例（simulation examples and/or experiments)，预测这个方法在不同场合可能会有的准确度或者好用的程度</li>
  <li>你个人的分析、评价与批评，包括：
    <ul>
      <li>这篇论文的最主要创意是什么？</li>
      <li>这些创意在应用上有什么好处？</li>
      <li>这些创意和应用上的好处是哪些条件下才能成立？</li>
      <li>这篇论文最主要的缺点或局限是什么？</li>
      <li>这些缺点或局限在应用上有什么坏处？</li>
      <li>这些缺点和应用上的坏处是因为哪些因素而引入的？</li>
      <li>你建议学长学弟的时候参考这篇论文的哪些部分（点子）</li>
    </ul>
  </li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2018/07/16/%E7%A1%95%E5%A3%AB%E7%8F%AD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E6%89%8B%E5%86%8C-%E8%A7%82%E7%82%B9%E6%94%B6%E8%8E%B7/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <a href="/index.html" class="previous"><i class="fa fa-angle-double-left"></i></a>
            <a href="/" class="previous"><i class="fa fa-angle-left"></i></a>
          
          <span class="page_number ">2/3</span>
          
            <a href="/page3" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page3" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/">深入解析torch.nn</a></li>
                    
                        <li><a href="/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/">爬虫环境安装</a></li>
                    
                        <li><a href="/2019/03/11/markdown_Guide/">Markdown_Guide</a></li>
                    
                        <li><a href="/2019/03/11/Pytorch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Pytorch环境搭建</a></li>
                    
                        <li><a href="/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/">GPT2.0笔记以及对NLP领域趋势的思考</a></li>
                    
                        <li><a href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文《MT-DNN》笔记</a></li>
                    
                        <li><a href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/">自然语言与多模态交互前沿技术</a></li>
                    
                        <li><a href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/">最近阅读文章小抄</a></li>
                    
                        <li><a href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/">Transformer在NLP词向量预训练中的应用</a></li>
                    
                        <li><a href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">流畅的python笔记</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">17</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 12pt; color: #666;">markdown</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 15pt; color: #333;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 12pt; color: #666;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 12pt; color: #666;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 12pt; color: #666;">pytorch</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
