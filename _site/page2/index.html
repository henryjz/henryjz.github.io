<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/page2/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://github.com/zergtant/pytorch-handbook/blob/master/chapter3/3.2-mnist.ipynb</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>&nbsp;
    
        <a href="/tag/#cnn" title="Tag: cnn" rel="tag">cnn</a>&nbsp;
    
        <a href="/tag/#cv" title="Tag: cv" rel="tag">cv</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.4-cnn.ipynb</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Word2Vec_Tutorial_part2/">Word2Vec Tutorial part2</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</p>
</blockquote>

<h1 id="word2vec-tutorial-part-2---negative-sampling">Word2Vec Tutorial Part 2 - Negative Sampling</h1>

<p>11 Jan 2017</p>

<p>In part 2 of the word2vec tutorial (here’s <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">part 1</a>), I’ll cover a few additional modifications to the basic skip-gram model which are important for actually making it feasible to train.</p>

<p>When you read the tutorial on the skip-gram model for Word2Vec, you may have noticed something–it’s a huge neural network!</p>

<p>In the example I gave, we had word vectors with 300 components, and a vocabulary of 10,000 words. Recall that the neural network had two weight matrices–a hidden layer and output layer. Both of these layers would have a weight matrix with 300 x 10,000 = 3 million weights each!</p>

<p>Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast.</p>

<p>The authors of Word2Vec addressed these issues in their second <a href="http://arxiv.org/pdf/1310.4546.pdf">paper</a>.</p>

<p>There are three innovations in this second paper:</p>

<ol>
  <li>Treating common word pairs or phrases as single “words” in their model.</li>
  <li>Subsampling frequent words to decrease the number of training examples.</li>
  <li>Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.</li>
</ol>

<p>It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.</p>

<h1 id="word-pairs-and-phrases">Word Pairs and “Phrases”</h1>

<p>The authors pointed out that a word pair like “Boston Globe” (a newspaper) has a much different meaning than the individual words “Boston” and “Globe”. So it makes sense to treat “Boston Globe”, wherever it occurs in the text, as a single word with its own word vector representation.</p>

<p>You can see the results in their published model, which was trained on 100 billion words from a Google News dataset. The addition of phrases to the model swelled the vocabulary size to 3 million words!</p>

<p>If you’re interested in their resulting vocabulary, I poked around it a bit and published a post on it <a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/">here</a>. You can also just browse their vocabulary <a href="https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary">here</a>.</p>

<p>Phrase detection is covered in the “Learning Phrases” section of their <a href="http://arxiv.org/pdf/1310.4546.pdf">paper</a>. They shared their implementation in word2phrase.c–I’ve shared a commented (but otherwise unaltered) copy of this code <a href="https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2phrase.c">here</a>.</p>

<p>I don’t think their phrase detection approach is a key contribution of their paper, but I’ll share a little about it anyway since it’s pretty straightforward.</p>

<p>Each pass of their tool only looks at combinations of 2 words, but you can run it multiple times to get longer phrases. So, the first pass will pick up the phrase “New_York”, and then running it again will pick up “New_York_City” as a combination of “New_York” and “City”.</p>

<p>The tool counts the number of times each combination of two words appears in the training text, and then these counts are used in an equation to determine which word combinations to turn into phrases. The equation is designed to make phrases out of words which occur together often relative to the number of individual occurrences. It also favors phrases made of infrequent words in order to avoid making phrases out of common words like “and the” or “this is”.</p>

<p>You can see more details about their equation in my code comments <a href="https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2phrase.c#L389">here</a>.</p>

<p>One thought I had for an alternate phrase recognition strategy would be to use the titles of all Wikipedia articles as your vocabulary.</p>

<h1 id="subsampling-frequent-words">Subsampling Frequent Words</h1>

<p>In part 1 of this tutorial, I showed how training samples were created from the source text, but I’ll repeat it here. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p>

<p><a href="http://mccormickml.com/assets/word2vec/training_data.png"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data" /></a></p>

<p>There are two “problems” with common words like “the”:</p>

<ol>
  <li>When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.</li>
  <li>We will have many more samples of (“the”, …) than we need to learn a good vector for “the”.</li>
</ol>

<p>Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency.</p>

<p>If we have a window size of 10, and we remove a specific instance of “the” from our text:</p>

<ol>
  <li>As we train on the remaining words, “the” will not appear in any of their context windows.</li>
  <li>We’ll have 10 fewer training samples where “the” is the input word.</li>
</ol>

<p>Note how these two effects help address the two problems stated above.</p>

<h3 id="sampling-rate">Sampling rate</h3>

<p>The word2vec C code implements an equation for calculating a probability with which to keep a given word in the vocabulary.</p>

<p>wiwi is the word, z(wi)z(wi) is the fraction of the total words in the corpus that are that word. For example, if the word “peanut” occurs 1,000 times in a 1 billion word corpus, then z(‘peanut’) = 1E-6.</p>

<p>There is also a parameter in the code named ‘sample’ which controls how much subsampling occurs, and the default value is 0.001. Smaller values of ‘sample’ mean words are less likely to be kept.</p>

<p>P(wi)P(wi) is the probability of <em>keeping</em> the word:</p>

<p>P(wi)=(z(wi)0.001−−−−−√+1)⋅0.001z(wi)P(wi)=(z(wi)0.001+1)⋅0.001z(wi)</p>

<p>You can plot this quickly in Google to see the shape.</p>

<p><a href="http://mccormickml.com/assets/word2vec/subsample_func_plot.png"><img src="http://mccormickml.com/assets/word2vec/subsample_func_plot.png" alt="Plot of subsampling function" /></a></p>

<p>No single word should be a very large percentage of the corpus, so we want to look at pretty small values on the x-axis.</p>

<p>Here are some interesting points in this function (again this is using the default sample value of 0.001).</p>

<ul>
  <li>
    <p>P(wi)=1.0P(wi)=1.0</p>

    <p>(100% chance of being kept) when</p>

    <p>z(wi)&lt;=0.0026z(wi)&lt;=0.0026</p>

    <p>.</p>

    <ul>
      <li>This means that only words which represent more than 0.26% of the total words will be subsampled.</li>
    </ul>
  </li>
  <li>
    <p>P(wi)=0.5P(wi)=0.5 (50% chance of being kept) when z(wi)=0.00746z(wi)=0.00746.</p>
  </li>
  <li>
    <p>P(wi)=0.033P(wi)=0.033</p>

    <p>(3.3% chance of being kept) when</p>

    <p>z(wi)=1.0z(wi)=1.0</p>

    <p>.</p>

    <ul>
      <li>That is, if the corpus consisted entirely of word wiwi, which of course is ridiculous.</li>
    </ul>
  </li>
</ul>

<p>You may notice that the paper defines this function a little differently than what’s implemented in the C code, but I figure the C implementation is the more authoritative version.</p>

<h1 id="negative-sampling">Negative Sampling</h1>

<p>Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak <em>all</em> of the weights in the neural network.</p>

<p>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!</p>

<p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.</p>

<p>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for <em>all</em> of the other thousands of output neurons to output a 0.</p>

<p>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).</p>

<p>The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.</p>

<p>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!</p>

<p>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).</p>

<h3 id="selecting-negative-samples">Selecting Negative Samples</h3>

<p>The “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.</p>

<p>For instance, suppose you had your entire training corpus as a list of words, and you chose your 5 negative samples by picking randomly from the list. In this case, the probability for picking the word “couch” would be equal to the number of times “couch” appears in the corpus, divided the total number of word occus in the corpus. This is expressed by the following equation:</p>

<p>P(wi)=f(wi)∑nj=0(f(wj))P(wi)=f(wi)∑j=0n(f(wj))</p>

<p>The authors state in their paper that they tried a number of variations on this equation, and the one which performed best was to raise the word counts to the 3/4 power:</p>

<p>P(wi)=f(wi)3/4∑nj=0(f(wj)3/4)P(wi)=f(wi)3/4∑j=0n(f(wj)3/4)</p>

<p>If you play with some sample values, you’ll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.</p>

<p>The way this selection is implemented in the C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by P(wi)P(wi) * table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those.</p>

<h2 id="other-resources">Other Resources</h2>

<p>For the most detailed and accurate explanation of word2vec, you should check out the C code. I’ve published an extensively commented (but otherwise unaltered) version of the code <a href="https://github.com/chrisjmccormick/word2vec_commented">here</a>.</p>

<p>Also, did you know that the word2vec model can also be applied to non-text data for recommender systems and ad targeting? Instead of learning vectors from a sequence of words, you can learn vectors from a sequence of user actions. Read more about this in my new post <a href="http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/">here</a>.</p>

<p>Finally, I’ve also created a <a href="http://mccormickml.com/2016/04/27/word2vec-resources/">post</a> with links to and descriptions of other word2vec tutorials, papers, and implementations.</p>

<h3 id="cite">Cite</h3>

<p>McCormick, C. (2017, January 11). <em>Word2Vec Tutorial Part 2 - Negative Sampling</em>. Retrieved from http://www.mccormickml.com</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Word2Vec_Tutorial_part2/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Word2Vec-Tutorial/">Word2Vec Tutorial</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</p>
</blockquote>

<h1 id="word2vec-tutorial---the-skip-gram-model">Word2Vec Tutorial - The Skip-Gram Model</h1>

<p>19 Apr 2016</p>

<p><strong>UPDATE:</strong> I’m proud to announce that I’ve published my first eBook, <em>The Inner Workings of word2vec</em>. It includes all of the material in this post series, but goes deeper with additional topics like CBOW and Hierarchical Softmax, as well as example code that demonstrates the algorithm details in action. I’m continuing to add more topics and code to the book–picking it up now entitles you to receive all future revisions. Thanks for your support!</p>

<p>This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.</p>

<h1 id="the-model">The Model</h1>

<p>The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s all of the little tweaks and enhancements that start to clutter the explanation.</p>

<p>Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.</p>

<p>Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.</p>

<h1 id="the-fake-task">The Fake Task</h1>

<p>So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.</p>

<p>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p>

<p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).</p>

<p>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.</p>

<p>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p>

<p><a href="http://mccormickml.com/assets/word2vec/training_data.png"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data" /></a></p>

<p>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.</p>

<h1 id="model-details">Model Details</h1>

<p>So how is this all represented?</p>

<p>First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.</p>

<p>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.</p>

<p>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.</p>

<p>Here’s the architecture of our neural network.</p>

<p><a href="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png"><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="Skip-gram Neural Network Architecture" /></a></p>

<p>There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.</p>

<p>When <em>training</em> this network on word pairs, the input is a one-hot vector representing the input word and the training output <em>is also a one-hot vector</em>representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, <em>not</em> a one-hot vector).</p>

<h1 id="the-hidden-layer">The Hidden Layer</h1>

<p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p>

<p>300 features is what Google used in their published model trained on the Google news dataset (you can download it from <a href="https://code.google.com/archive/p/word2vec/">here</a>). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).</p>

<p>If you look at the <em>rows</em> of this weight matrix, these are actually what will be our word vectors!</p>

<p><a href="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png"><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="Hidden Layer Weight Matrix" /></a></p>

<p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p>

<p>Let’s get back, though, to working through the definition of this model that we’re going to train.</p>

<p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p>

<p><a href="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png"><img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" alt="Effect of matrix multiplication with a one-hot vector" /></a></p>

<p>This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.</p>

<h1 id="the-output-layer">The Output Layer</h1>

<p>The <code class="highlighter-rouge">1 x 300</code> word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">here</a>, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.</p>

<p>Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function <code class="highlighter-rouge">exp(x)</code> to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from <em>all</em> 10,000 output nodes.</p>

<p>Here’s an illustration of calculating the output of the output neuron for the word “car”.</p>

<p><a href="http://mccormickml.com/assets/word2vec/output_weights_function.png"><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="Behavior of the output neuron" /></a></p>

<p>Note that neural network does not know anything about the offset of the output word relative to the input word. It <em>does not</em> learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, <em>every single occurrence</em> of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ <em>is not</em> 100%; you may have picked one of the other words in the vicinity.</p>

<h1 id="intuition">Intuition</h1>

<p>Ok, are you ready for an exciting bit of insight into this network?</p>

<p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p>

<p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p>

<p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p>

<h1 id="next-up">Next Up</h1>

<p>You may have noticed that the skip-gram neural network contains a huge number of weights… For our example with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive, so the word2vec authors introduced a number of tweaks to make training feasible. These are covered in <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">part 2 of this tutorial</a>.</p>

<p>Did you know that the word2vec model can also be applied to non-text data for recommender systems and ad targeting? Instead of learning vectors from a sequence of words, you can learn vectors from a sequence of user actions. Read more about this in my new post <a href="http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/">here</a>.</p>

<h1 id="other-resources">Other Resources</h1>

<p>I’ve also created a <a href="http://mccormickml.com/2016/04/27/word2vec-resources/">post</a> with links to and descriptions of other word2vec tutorials, papers, and implementations.</p>

<h3 id="cite">Cite</h3>

<p>McCormick, C. (2016, April 19). <em>Word2Vec Tutorial - The Skip-Gram Model</em>. Retrieved from http://www.mccormickml.com</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Word2Vec-Tutorial/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Understanding_LSTM_Networks/">Understanding LSTM Networks</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p>
</blockquote>

<h1 id="understanding-lstm-networks">Understanding LSTM Networks</h1>

<p>Posted on August 27, 2015</p>

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p>

<p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p>

<p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</p>

<p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img" /></p>

<p><strong>Recurrent Neural Networks have loops.</strong></p>

<p>In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.</p>

<p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p>

<p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="An unrolled recurrent neural network." /></p>

<p><strong>An unrolled recurrent neural network.</strong></p>

<p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.</p>

<p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.</p>

<p>Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.</p>

<h2 id="the-problem-of-long-term-dependencies">The Problem of Long-Term Dependencies</h2>

<p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.</p>

<p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the <em>sky</em>,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.</p>

<p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img" /></p>

<p>But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent <em>French</em>.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.</p>

<p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.</p>

<p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies." /></p>

<p>In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by [Hochreiter (1991) <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p>

<p>Thankfully, LSTMs don’t have this problem!</p>

<h2 id="lstm-networks">LSTM Networks</h2>

<p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1">1</a> They work tremendously well on a large variety of problems, and are now widely used.</p>

<p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p>

<p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Understanding_LSTM_Networks/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/Pytorch%E4%B8%AD%E7%9A%84rnn/">pytorch中rnn的使用</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.5-rnn.ipynb</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/Pytorch%E4%B8%AD%E7%9A%84rnn/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <a href="/index.html" class="previous"><i class="fa fa-angle-double-left"></i></a>
            <a href="/" class="previous"><i class="fa fa-angle-left"></i></a>
          
          <span class="page_number ">2/5</span>
          
            <a href="/page3" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page5" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/">《Pretraining-Based Natural Language Generation for Text Summarization》论文解读</a></li>
                    
                        <li><a href="/2019/03/20/%E6%96%87%E6%91%98/">文摘</a></li>
                    
                        <li><a href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/">chat-bot</a></li>
                    
                        <li><a href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/">自然语言处理的是个发展趋势</a></li>
                    
                        <li><a href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/">华为李航-NLP有个基本问题，深度学习4个做的很好</a></li>
                    
                        <li><a href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/">全面解析Tensor2Tensor系统</a></li>
                    
                        <li><a href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a></li>
                    
                        <li><a href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a></li>
                    
                        <li><a href="/2019/03/13/Word2Vec_Tutorial_part2/">Word2Vec Tutorial part2</a></li>
                    
                        <li><a href="/2019/03/13/Word2Vec-Tutorial/">Word2Vec Tutorial</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">17</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#reproduction" class="categories-list-item" cate="reproduction">
                            <span class="name">
                                reproduction
                            </span>
                            <span class="badge">11</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#digest" class="categories-list-item" cate="digest">
                            <span class="name">
                                digest
                            </span>
                            <span class="badge">1</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 11pt; color: #777;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 11pt; color: #777;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 10pt; color: #888;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 14pt; color: #444;">pytorch</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 9pt; color: #999;">markdown</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cnn" style="font-size: 9pt; color: #999;">cnn</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cv" style="font-size: 9pt; color: #999;">cv</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#tensorflow" style="font-size: 9pt; color: #999;">tensorflow</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#life" style="font-size: 9pt; color: #999;">life</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
