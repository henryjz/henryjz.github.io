<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>henryzhou</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 11 Mar 2019 12:33:11 +0800</pubDate>
    <lastBuildDate>Mon, 11 Mar 2019 12:33:11 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>爬虫环境安装</title>
        <description>&lt;h4 id=&quot;spider-note&quot;&gt;spider Note&lt;/h4&gt;

&lt;h5 id=&quot;python环境安装&quot;&gt;python环境安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;python3.7&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;请求库的安装&quot;&gt;请求库的安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    请求库requests的安装：&lt;code class=&quot;highlighter-rouge&quot;&gt;conda install requests -n spider&lt;/code&gt;
  &lt;/li&gt;
  &lt;li&gt;
    自动化测试工具Senlenium的安装：&lt;code class=&quot;highlighter-rouge&quot;&gt;conda install selenium -n spider&lt;/code&gt;
  &lt;/li&gt;
  &lt;li&gt;
    senlenium驱动chrome浏览器工具chromedriver添加到PATH中：

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;在&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bashrc&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;最后添加：&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;henry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bashrc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    使用phantomJS实现后台的浏览器控制

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;下载&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phantomJS&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，并且将其路径添加到系统变量中&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    安装异步web请求库aiohttp

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aiohttp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;解析库的安装&quot;&gt;解析库的安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    lxml的安装，支持HTML、XML的解析，支持XPath解析方式

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lxml&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    beautifulsoup4的安装，其依赖lxml

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beautifulsoup4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    pyquery的安装

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyquery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    安装验证码识别工具tesseract

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tesseract&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ocr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;libtesseract&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;libleptonica&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tesserocr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pillow&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;数据库的安装&quot;&gt;数据库的安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    mysql的安装

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#mysql启动、关闭、重启命令
#sudo service mysql start
#sudo service mysql stop
#sudo service mysql restart
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mysql5&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.7&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;以上不会有密码设置过程，需要手动配置&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;debian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#密码在/etc/mysql/debian.cnf文件中可查看(V3XKquzHqnW18GWc)，在mysql命令行中执行下列语句
&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;databases&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;；&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;authentication_string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PASSWORD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;zhou&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'root'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plugin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mysql_native_password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flush&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;privileges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#完全卸载mysql的方法
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;purge&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-*&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;etc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autoremove&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    安装redis

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;redis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;存储库的安装&quot;&gt;存储库的安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    安装pymysql存储库以使用python和mysql交互

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pymysql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    安装redis_py

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;redis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;web库的安装&quot;&gt;Web库的安装&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    安装Flask web库来搭建一些API接口，供爬虫使用，后面会利用Flask+Redis维护动态代理池和Cookies池

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tornado&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    安装tornado，后面会使用tornado+redis来搭建一个ADSL拨号代理池
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 11 Mar 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</guid>
        
        <category>spider</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Markdown_Guide</title>
        <description>&lt;h1 id=&quot;欢迎使用markdown编辑器写博客&quot;&gt;欢迎使用Markdown编辑器写博客&lt;/h1&gt;

本Markdown编辑器使用&lt;a href=&quot;https://github.com/benweet/stackedit&quot;&gt;StackEdit&lt;/a&gt;修改而来，用它写博客，将会带来全新的体验哦：

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown和扩展Markdown简洁的语法&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;代码块高亮&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;图片链接和图片上传&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;LaTex&lt;/em&gt;数学公式&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;UML序列图和流程图&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;离线写博客&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;导入导出Markdown文件&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;丰富的快捷键&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;快捷键&quot;&gt;快捷键&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;加粗    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + B&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;斜体    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + I&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;引用    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + Q&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;插入链接    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + L&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;插入代码    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + K&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;插入图片    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + G&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;提升标题    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + H&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;有序列表    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + O&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;无序列表    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + U&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;横线    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + R&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;撤销    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + Z&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;重做    &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + Y&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;markdown及扩展&quot;&gt;Markdown及扩展&lt;/h2&gt;

&lt;blockquote&gt;
  Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。    —— &lt;a href=&quot;https://zh.wikipedia.org/wiki/Markdown&quot; target=&quot;_blank&quot;&gt; [ 维基百科 ]&lt;/a&gt;
&lt;/blockquote&gt;

使用简单的符号标识不同的标题，将某些文字标记为&lt;strong&gt;粗体&lt;/strong&gt;或者&lt;em&gt;斜体&lt;/em&gt;，创建一个&lt;a href=&quot;http://www.csdn.net&quot;&gt;链接&lt;/a&gt;等，详细语法参考帮助？。

本编辑器支持 &lt;strong&gt;Markdown Extra&lt;/strong&gt; , 　扩展了很多好用的功能。具体请参考&lt;a href=&quot;https://github.com/jmcmanus/pagedown-extra&quot; title=&quot;Pagedown Extra&quot;&gt;Github&lt;/a&gt;.

&lt;h3 id=&quot;表格&quot;&gt;表格&lt;/h3&gt;

&lt;strong&gt;Markdown　Extra&lt;/strong&gt;　表格语法：

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;项目&lt;/th&gt;
      &lt;th&gt;价格&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Computer&lt;/td&gt;
      &lt;td&gt;$1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Phone&lt;/td&gt;
      &lt;td&gt;$12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pipe&lt;/td&gt;
      &lt;td&gt;$1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

可以使用冒号来定义对齐方式：

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;项目&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;价格&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;数量&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Computer&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1600 元&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Phone&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12 元&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Pipe&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1 元&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;234&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

###定义列表

&lt;dl&gt;
  &lt;dt&gt;&lt;strong&gt;Markdown　Extra&lt;/strong&gt;　定义列表语法：&lt;/dt&gt;
  &lt;dt&gt;项目１&lt;/dt&gt;
  &lt;dt&gt;项目２&lt;/dt&gt;
  &lt;dd&gt;定义 A&lt;/dd&gt;
  &lt;dd&gt;定义 B&lt;/dd&gt;
  &lt;dt&gt;项目３&lt;/dt&gt;
  &lt;dd&gt;定义 C&lt;/dd&gt;
  &lt;dd&gt;
    定义 D

    &lt;blockquote&gt;
      定义D内容
    &lt;/blockquote&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h3 id=&quot;代码块&quot;&gt;代码块&lt;/h3&gt;
代码块语法遵循标准markdown代码，例如：
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_authorization&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;somefunc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''A docstring'''&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# interesting
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Greater'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SomeClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'''interpreter
... prompt'''&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

###脚注
生成一个脚注[^footnote].
[^footnote]: 这里是 &lt;strong&gt;脚注&lt;/strong&gt; 的 &lt;em&gt;内容&lt;/em&gt;.

&lt;h3 id=&quot;目录&quot;&gt;目录&lt;/h3&gt;
用 &lt;code class=&quot;highlighter-rouge&quot;&gt;[TOC]&lt;/code&gt;来生成目录：

[TOC]

&lt;h3 id=&quot;数学公式&quot;&gt;数学公式&lt;/h3&gt;
使用MathJax渲染&lt;em&gt;LaTex&lt;/em&gt; 数学公式，详见&lt;a href=&quot;http://math.stackexchange.com/&quot;&gt;math.stackexchange.com&lt;/a&gt;.

&lt;ul&gt;
  &lt;li&gt;行内公式，数学公式为：$\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$。&lt;/li&gt;
  &lt;li&gt;块级公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a}&lt;/script&gt;

更多LaTex语法请参考 &lt;a href=&quot;http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;这儿&lt;/a&gt;.

&lt;h3 id=&quot;uml-图&quot;&gt;UML 图:&lt;/h3&gt;

可以渲染序列图：

&lt;pre&gt;&lt;code class=&quot;language-sequence&quot;&gt;张三-&amp;gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&amp;gt;张三: 忙得吐血，哪有时间写。
&lt;/code&gt;&lt;/pre&gt;

或者流程图：

&lt;pre&gt;&lt;code class=&quot;language-flow&quot;&gt;st=&amp;gt;start: 开始
e=&amp;gt;end: 结束
op=&amp;gt;operation: 我的操作
cond=&amp;gt;condition: 确认？

st-&amp;gt;op-&amp;gt;cond
cond(yes)-&amp;gt;e
cond(no)-&amp;gt;op
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;关于 &lt;strong&gt;序列图&lt;/strong&gt; 语法，参考 &lt;a href=&quot;http://bramp.github.io/js-sequence-diagrams/&quot;&gt;这儿&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;关于 &lt;strong&gt;流程图&lt;/strong&gt; 语法，参考 &lt;a href=&quot;http://adrai.github.io/flowchart.js/&quot;&gt;这儿&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;离线写博客&quot;&gt;离线写博客&lt;/h2&gt;

即使用户在没有网络的情况下，也可以通过本编辑器离线写博客（直接在曾经使用过的浏览器中输入&lt;a href=&quot;http://write.blog.csdn.net/mdeditor&quot;&gt;write.blog.csdn.net/mdeditor&lt;/a&gt;即可。&lt;strong&gt;Markdown编辑器&lt;/strong&gt;使用浏览器离线存储将内容保存在本地。

用户写博客的过程中，内容实时保存在浏览器缓存中，在用户关闭浏览器或者其它异常情况下，内容不会丢失。用户再次打开浏览器时，会显示上次用户正在编辑的没有发表的内容。

博客发表后，本地缓存将被删除。　

用户可以选择 &lt;i class=&quot;icon-disk&quot;&gt;&lt;/i&gt; 把正在写的博客保存到服务器草稿箱，即使换浏览器或者清除缓存，内容也不会丢失。

&lt;blockquote&gt;
  &lt;strong&gt;注意：&lt;/strong&gt;虽然浏览器存储大部分时候都比较可靠，但为了您的数据安全，在联网后，&lt;strong&gt;请务必及时发表或者保存到服务器草稿箱&lt;/strong&gt;。
&lt;/blockquote&gt;

&lt;h2 id=&quot;浏览器兼容&quot;&gt;浏览器兼容&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;目前，本编辑器对Chrome浏览器支持最为完整。建议大家使用较新版本的Chrome。&lt;/li&gt;
  &lt;li&gt;IE９以下不支持&lt;/li&gt;
  &lt;li&gt;IE９，１０，１１存在以下问题
    &lt;ol&gt;
      &lt;li&gt;不支持离线功能&lt;/li&gt;
      &lt;li&gt;IE9不支持文件导入导出&lt;/li&gt;
      &lt;li&gt;IE10不支持拖拽文件导入&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

</description>
        <pubDate>Mon, 11 Mar 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/03/11/markdown_Guide/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/11/markdown_Guide/</guid>
        
        <category>markdown</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>GPT2.0笔记以及对NLP领域趋势的思考</title>
        <description>&lt;h5 id=&quot;gpt10&quot;&gt;GPT1.0&lt;/h5&gt;

简述如下：GPT 1.0采取预训练+FineTuning两个阶段，它采取Transformer作为特征抽取器。预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。

从大框架上来说，Bert基本就是GPT 1.0的结构，除了预训练阶段采取的是“双向语言模型”之外，它们并没什么本质差异，其它的技术差异都是细枝末节，不影响大局，基本可忽略。

&lt;h5 id=&quot;gpt20&quot;&gt;GPT2.0&lt;/h5&gt;

GPT2.0大框架其实还是GPT 1.0的框架，但是把第二阶段的Finetuning做有监督地下游NLP任务，换成了无监督地做下游任务。本质上，GPT2.0选择了这么一条路来强化Bert或者是强化GPT 1.0的第一个预训练阶段：就是说首先把Transformer模型参数扩容，常规的Transformer Big包含24个叠加的Block，GPT2.0将Transformer层数增加到48层，参数规模15亿。真正的目的是：GPT 2.0准备用更多的训练数据来做预训练，更大的模型，更多的参数，意味着更高的模型容量，所以先扩容，免得Transformer楼层不够多的房间（模型容量）容纳不下过多的住户（就是NLP知识）。

GPT2.0的语料：GPT2.0找了800万互联网网页作为语言模型的训练数据，它们被称为WebText，互联网网页的优点是覆盖的主题范围非常广，这样训练出来的语言模型，通用性好，覆盖几乎任何领域的内容，这意味着它可以用于任意领域的下游任务，有点像图像领域的Imagenet的意思。GPT 2.0论文其实更强调训练数据的通用性强这点。当然，除了量大通用性强外，数据质量也很重要，高质量的数据必然包含更好的语言及人类知识，所以GPT 2.0还做了数据质量筛选，过滤出高质量的网页内容来。

GPT 2.0用这些网页做“单向语言模型”，GPT 2.0没有像Bert或者1.0版本一样，拿这个第一阶段的预训练模型有监督地去做第二阶段的Finetuning任务，而是选择了无监督地去做下游任务。另外论文中提到的对Transformer结构的微调，以及BPE输入方式，我相信都是不太关键的改动，应该不影响大局。

&lt;strong&gt;问题一：为什么GPT 2.0第二阶段不通过Finetuning去有监督地做下游任务呢？&lt;/strong&gt;无监督地去做很多第二阶段的任务，只是GPT作者想说明在第一阶段Transformer学到了很多通用的包含各个领域的知识，第二部分各种实验是对这点的例证，如此而已。这是为何说第二阶段其实不重要，因为它不是论文的中心思想，而是说明中心思想的例子。

&lt;strong&gt;问题二：在预训练阶段，为什么GPT 2.0仍然固执地用单向语言模型，而不是双向语言模型呢？&lt;/strong&gt;Bert在论文的实验部分已经证明了：Bert的效果比GPT好主要归因于这个双向语言模型。也许GPT 作者只想强调他们想做语言模型这个事情，毕竟生成内容后续单词这种模式，单向语言模型更方便，这估计是真正原因。

&lt;strong&gt;问题三：GPT 2.0 既然第二阶段是无监督的任务，而它不做Finetuning，那么你训练好一个语言模型，它当然会根据输入的一句话，给你蹦出后面可能紧跟那个单词，这是标准的语言模型过程，这个正常。但是如果这时候让它去做一个文本摘要任务，它怎么知道它现在在做什么事情呢，根据输入，应该输出什么东西呢？&lt;/strong&gt;其实GPT 2.0在做下游无监督任务的时候，给定输入（对于不同类型的输入，加入一些引导字符，引导GPT正确地预测目标，比如如果做摘要，在输入时候加入“TL：DR”引导字符串），它的输出跟语言模型的输出是一样的，就是蹦出一个单词。那么问题来了：对于比如摘要任务，我们期待的输出结果是一句话或者几句话，你给我一个单词，有点太小气，那该怎么办？很简单，继续一个字一个字往出蹦，按照这些字从系统里蹦出来的时间顺序连起来，就是你想要的摘要结果，这种所有任务采取相同的往出蹦字的输出模式也是有点意思的。就是说，GPT2.0给出了一种新颖的生成式任务的做法，就是一个字一个字往出蹦，然后拼接出输出内容作为翻译结果或者摘要结果。传统的NLP网络的输出模式一般需要有个序列的产生结构的，而GPT 2.0完全是语言模型的产生结果方式：一个字一个字往出蹦，没有输出的序列结构。

&lt;h5 id=&quot;归纳&quot;&gt;归纳&lt;/h5&gt;

我们可以从两个不同的角度来理解GPT 2.0。

&lt;strong&gt;一个角度是把它看作采取类似Elmo/GPT/Bert的两阶段模型解决NLP任务的一种后续改进策略&lt;/strong&gt;，这种策略可以用来持续优化第一阶段的预训练过程。通过现在的Transformer架构，采用更高质量的数据，采用更宽泛的数据（Web数据量大了估计包含任何你能想到的领域），采用更大量的数据（WebText，800万网页），Transformer采用更复杂的模型（最大的GPT2.0模型是Transformer的两倍层深），那么在Transformer里能学会更多更好的NLP的通用知识。如果我们第二阶段仍然采取Finetuning，对下游任务的提升效果是可以很乐观地期待的。

&lt;strong&gt;另外一个角度也可以把GPT 2.0看成一个效果特别好的语言模型&lt;/strong&gt;，可以用它来做语言生成类任务，比如摘要，QA这种，再比如给个故事的开头，让它给你写完后面的情节，目前看它的效果出奇的好。

&lt;h5 id=&quot;bert的另外一种改进模式&quot;&gt;Bert的另外一种改进模式&lt;/h5&gt;

GPT2.0给出的思路是优化Bert的第一个预训练阶段，方向是扩充数据数量，提升数据质量，增强通用性，追求的是通过做大来做强。

另一个思路：机器学习里面还有有监督学习，NLP任务里也有不少有监督任务是有训练数据的，这些数据能用来改善Bert第二阶段学习各种知识的Transformer。这种做法一个典型的模型是最近微软推出的MT-DNN，核心思想：结构上底层就是标准的Bert Transformer，第一阶段采用Bert的预训练模型不动，在Finetuning阶段，在上层针对不同任务构造不同优化目标，所有不同上层任务共享底层Transformer参数，这样就强迫Transformer通过预训练做很多NLP任务，来学会新的知识，并编码到Transformer的参数中。

&lt;h5 id=&quot;nlp主流模型进化&quot;&gt;NLP主流模型进化&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;采取Bert的两阶段模式&lt;/li&gt;
  &lt;li&gt;特征抽取器采用Transformer&lt;/li&gt;
  &lt;li&gt;Bert两阶段模式中，第一个预训练阶段的两种改进方向：
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;一种是强调通用性好以及规模大&lt;/strong&gt;。加入越来越多高质量的各种类型的无监督数据，GPT 2.0指出了个明路，就是净化的高质量网页&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;第二种是通过多任务训练&lt;/strong&gt;，加入各种新型的NLP任务数据，它的好处是有监督，能够有针对性的把任务相关的知识编码到网络参数里，所以明显的好处是学习目标明确，学习效率高；而对应的缺点是NLP的具体有监督任务，往往训练数据量少，于是包含的知识点少；而且有点偏科，学到的知识通用性不强。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GPT2.0&lt;strong&gt;采取超深层Transformer+更大量的网页数据去做更好的语言模型&lt;/strong&gt;，并进而做各种生成式任务是很有研究应用前景。&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 11 Mar 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/</guid>
        
        <category>spider</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>论文《MT-DNN》笔记</title>
        <description>&lt;h2 id=&quot;论文multi-task-deep-neural-networks-for-natural-language-understanding笔记&quot;&gt;论文《Multi-Task Deep Neural Networks for Natural Language Understanding》笔记&lt;/h2&gt;

&lt;blockquote&gt;
  论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1901.11504.pdf&quot;&gt;微软MT-DNN论文《Multi-Task Deep Neural Networks for Natural Language Understanding》&lt;/a&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;mt-dnn简单介绍&quot;&gt;MT-DNN简单介绍&lt;/h4&gt;

​	谷歌的&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt;在各个NLP任务（GLUE、SQuAD、命名实体识别、SWAG)上的表现都很好，但是BERT在词向量的预训练的第二阶段只使用了单个任务进行模型fine-tune，我们自然而然地会问：Fine-tune阶段使用多任务同时对网络参数进行微调效果会不会更好？。

​	微软研究院在2019年发布的论文《Multi-Task Deep Neural Networks for Natural Language Understanding》就做了这方面的实验。论文提出了一个假设：在单一领域的数据集上使用单一的任务训练模型限制了模型的泛化。MT-DNN提供的思路是：利用多任务之间的约束来避免单一任务上的过拟合问题，从而提高模型的泛化能力。文章中使用的多任务是相似的，作者任务机器能够像人一样在相似的任务中获取到相关的经验，比如会滑雪的人就能比较容易的学会滑冰，对机器来说也就是能够使用更少的训练数据是模型获得相同的效果。

&lt;h4 id=&quot;实验结果&quot;&gt;实验结果&lt;/h4&gt;

​	&lt;strong&gt;(1)&lt;/strong&gt;MT-DNN在8/9的GLUE&lt;a href=&quot;Gerneral Lanuage Understanding Evaluation，是评估模型自然语言理解能力的最权威的指标&quot;&gt;1&lt;/a&gt;任务中取得了SOAT成绩，其中未达到SOAT成绩的原因是数据集存在问题。这８个数据集（任务）可以归纳分为以下四种类别：

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;任务&lt;/th&gt;
      &lt;th&gt;数据集&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Single sentence classification&lt;/td&gt;
      &lt;td&gt;CoLA：情感分类&lt;br /&gt;SST-2：判断句子是否符合语法要求&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Text similarity score&lt;/td&gt;
      &lt;td&gt;STS-B：两句话的相似性&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pairwise Text classification&lt;/td&gt;
      &lt;td&gt;RET、MNLI：判断两句话的关系(emtaiment, controdictional, neutral)&lt;br /&gt;QQP, MRPC：判断那两句话是否具有相同的语义&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Relevence ranking&lt;/td&gt;
      &lt;td&gt;QNLI：判断问答句子对的相关性&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

​	&lt;strong&gt;(2)&lt;/strong&gt;通过这种多任务训练得到的模型能够很好的适用于其他未见过的相似任务，即使只有很少的带标注的数据。因为MT-DNN底层使用的是BERT(Base)的网络，所以这种相似任务之间的适用性的提高可以确定由多任务的fine-tune带来的。实验表明即使只使用原始数据集的0.1%、1%样本，同样能够获得不错的准确率。下面是MT-DNN模型和BERT两个模型在SNLI数据集上的表现：

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;模型&lt;/th&gt;
      &lt;th&gt;0.1%&lt;/th&gt;
      &lt;th&gt;1％&lt;/th&gt;
      &lt;th&gt;10%&lt;/th&gt;
      &lt;th&gt;100%&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;BERT&lt;/td&gt;
      &lt;td&gt;51%&lt;/td&gt;
      &lt;td&gt;82%&lt;/td&gt;
      &lt;td&gt;90%&lt;/td&gt;
      &lt;td&gt;94%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MT-DNN&lt;/td&gt;
      &lt;td&gt;82%&lt;/td&gt;
      &lt;td&gt;88%&lt;/td&gt;
      &lt;td&gt;91%&lt;/td&gt;
      &lt;td&gt;96%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

</description>
        <pubDate>Sun, 10 Mar 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</guid>
        
        <category>paper</category>
        
        <category>nlp</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>自然语言与多模态交互前沿技术</title>
        <description>&lt;h3 id=&quot;北大ai第三讲何晓东-京东ai研究院常务副院长ieee-fellow自然语言与多模态交互前言技术&quot;&gt;北大AI第三讲：何晓东-京东AI研究院常务副院长、IEEE Fellow：自然语言与多模态交互前言技术&lt;/h3&gt;

&lt;h4 id=&quot;nlp进展&quot;&gt;NLP进展：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;语言理解/语义槽值提取&lt;/li&gt;
  &lt;li&gt;语言理解/意图分类，2016年提出层次化注意力模型（HAN），以更好的在词、句子、段落、等多个层面来理解语言，判断意图，并通过对神经元激活的的可视化给出一定程度的可解释性。&lt;/li&gt;
  &lt;li&gt;语言理解/语义的表征：从自然语言中提取出语义并将其投影到语义空间以帮助搜索、推荐、分类、问答等应用；自然语言的描述，通过深度神经网络逐步抽取语义上的不变形（invariance），生成抽象的语义表征。&lt;/li&gt;
  &lt;li&gt;机器阅读理解（MRC）：机器阅读文本，回答问题；BERT模型在SQuAD封闭数据集上的成绩已经超过人类&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;未来&quot;&gt;未来&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;多模态智能：综合文字、语音、图像、知识图谱等信息来获取信息
    &lt;ul&gt;
      &lt;li&gt;建立多模态语义空间：联结图像和文字
        &lt;ul&gt;
          &lt;li&gt;通过深度结构语义模型（DSSM）把图像和文字表征成语义空间内的向量&lt;/li&gt;
          &lt;li&gt;在此空间中进行语义相似度计算，生成最匹配图像内容的文字表述。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;理解场景和知识，用语言表达（image caption）
        &lt;ul&gt;
          &lt;li&gt;一个棒球&lt;/li&gt;
          &lt;li&gt;一个棒球运动员&lt;/li&gt;
          &lt;li&gt;一个棒球运动员在扔&lt;/li&gt;
          &lt;li&gt;一个棒球运动员在扔一个球&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;图像描述机器人：CaptionBot&lt;/li&gt;
      &lt;li&gt;智能绘画机器人：AI根据语言描述创作绘画&lt;/li&gt;
      &lt;li&gt;AI+Art：更多（艺术化的）创作&lt;/li&gt;
      &lt;li&gt;综合图像和语言推理，回答问题：eg.那两把蓝色椅子之间是什么？&lt;/li&gt;
      &lt;li&gt;视觉-语言多模态导航：结合语言理解和对环境的视觉信息建模，机器人按指令从一个地方走到另一个地方。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;复杂内容创作：比如人工智能写作（长文章）
    &lt;ul&gt;
      &lt;li&gt;创作长文的技术挑战：
        &lt;ul&gt;
          &lt;li&gt;从简单输入到创作长文需要大量内容的扩充&lt;/li&gt;
          &lt;li&gt;长文的生成要可控，能满足组合爆炸式需求。模型需要时组合性的，能与训练的。&lt;/li&gt;
          &lt;li&gt;现有的端到端的模型不适合长文创作：为短文本生成（如机器翻译）而设计，不能抓住长文的高层语义。度量优化，信用分配，维持一致性，目标函数平衡等要重新设计。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;长文创作的前沿探索：比如顶层设计和规划
        &lt;ul&gt;
          &lt;li&gt;现有的文本生成模型缺乏“规划”，应先产生粗略的高层主题规划，然后再对主题和子主题展开长文&lt;/li&gt;
          &lt;li&gt;最近的一些工作：多层增强学习模型及其在主题设计和长文生成中的应用。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;创作诗歌（控制）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;情感智能：不只识别人的情感，还能像人一样表达情感和风格
    &lt;ul&gt;
      &lt;li&gt;生成带情感的语言：让AI在语言表达中加入情感，提升用户体验&lt;/li&gt;
      &lt;li&gt;表达情感和风格：让AI用语言表达浪漫或者幽默的风格——StyleNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;多轮人机对话：理解语境、常识、语言，生成逻辑严谨的有情感的对话，服务于人
    &lt;ul&gt;
      &lt;li&gt;图灵测试：通过人类和机器之间的自然语言对话来判断机器是否具有智能。&lt;/li&gt;
      &lt;li&gt;主要的人机对话系统框架：任务型对话系统、问答型对话系统、聊天型对话系统、检索性对话系统&lt;/li&gt;
      &lt;li&gt;我们将成为有史以来第一代与AI共生的人类，《从Eliza到小冰：社交对话机器人的机遇与挑战》&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ai产业化的下一个方向是什么&quot;&gt;AI产业化的下一个方向是什么&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    智能服务产业是新蓝海：传统人类密集型产业，有广阔自动化、智能化空间；随着AI技术、IOT技术等的创新，市场在快速成长。
  &lt;/li&gt;
  &lt;li&gt;
    服务型对话

    &lt;ul&gt;
      &lt;li&gt;服务：生活、娱乐、消费、客服等为人提供的的服务&lt;/li&gt;
      &lt;li&gt;对话：多模态、大规模开放领域，具有常识和情感、能完成复杂任务的智能交互技术。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    产业界应用：

    &lt;ul&gt;
      &lt;li&gt;京东客服机器人：首个大规模商用情感客服机器人：能够检测用户的情感类型，做出道歉、安抚、祝福的动作，提升用户体验。&lt;/li&gt;
      &lt;li&gt;京东智能服务产品矩阵：JIMI和AlphaSales&lt;/li&gt;
      &lt;li&gt;京东智能IoT&lt;/li&gt;
      &lt;li&gt;京东智能市政服务&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    人机融合、多模态智能服务的产业时代

    &lt;ul&gt;
      &lt;li&gt;
        服务即对话：多模态、大规模开放领域、具有常识和情感、能完成复杂任务的对话系统是推动下一代智能产业的核心技术。

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th&gt;分级&lt;/th&gt;
              &lt;th&gt;目标&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;低级智能对话&lt;/td&gt;
              &lt;td&gt;对用户简单意图进行识别并给出预设答案&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td&gt;初级智能对话&lt;/td&gt;
              &lt;td&gt;能识别复杂意图，联系上下文给出回答&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td&gt;中级智能对话&lt;/td&gt;
              &lt;td&gt;根据用户问题及情绪完成个性化多轮对话，协助用户完成目标&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td&gt;高级智能对话&lt;/td&gt;
              &lt;td&gt;能对多模态信息进行推理，自主判断，并组织语言与用户沟通，具备自我学习能力&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td&gt;通用智能对话&lt;/td&gt;
              &lt;td&gt;能基于一切信息开展自我学习，自我适应，及自我创新。在复杂问题领域达到人类水平。&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;提问环节&quot;&gt;提问环节&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;关于OpenAI的GPT模型暴力解决问题的看法
    &lt;ul&gt;
      &lt;li&gt;算法是解决问题的核心，但是光有算法是不够的。GPT2是算法和产业界结合的一个例子，有大量高质量的数据和算力。对科研机构不算一个坏事。算法是灵魂，数据和算力是物质基础&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NLP领域的问题是否可以认为比图像领域的问题要难解决，所有在进度上有所落后？
    &lt;ul&gt;
      &lt;li&gt;NLP领域的问题是一种认知领域的问题，比图像领域的感知问题要复杂一些，所以图像领域的问题能够比较清晰的被定义，相对而言也会推进的更加深入。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如何判断机器是否真正理解了人类的问题？
    &lt;ul&gt;
      &lt;li&gt;这是一个哲学问题，科学家的作用就是将哲学问题转化成科学问题，可以通过定义一些测试任务来进行一定程度上的判断。比如小冰提出了聊天轮数的metric，作为判断聊天机器人是否能像人类一样进行聊天。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;智能音响等IoT产品的发展趋势？
    &lt;ul&gt;
      &lt;li&gt;每一次交互的革命都能带来一个万亿级别的产业，智能IoT就具有这样的潜力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如何看待当前CV行业如火如荼，NLP行业相对比较平静的现象？
    &lt;ul&gt;
      &lt;li&gt;当问题已经定义的很清楚的时候，机会相对来说就小了很多。智能服务产业的未来可能要比CV的智能安防产业还要大。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 06 Mar 2019 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/</guid>
        
        <category>nlp</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>最近阅读文章小抄</title>
        <description>&lt;h4 id=&quot;2019121nlp-领域的-imagenet-时代到来词嵌入已死语言模型当立&quot;&gt;2019.1.21–《NLP 领域的 ImageNet 时代到来：词嵌入「已死」，语言模型当立》&lt;/h4&gt;

计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。

</description>
        <pubDate>Wed, 06 Mar 2019 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/</guid>
        
        <category>nlp</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Transformer在NLP词向量预训练中的应用</title>
        <description>&lt;h4 id=&quot;bert发展沿革&quot;&gt;BERT发展沿革&lt;/h4&gt;

&lt;h5 id=&quot;四个部分&quot;&gt;四个部分&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;回顾Transformer的网络结构、和RNN、RNN的对比&lt;/li&gt;
  &lt;li&gt;介绍ELMo模型语境化的词嵌入，重点介绍其双向语言模型和根据具体语境生成词嵌入的原理&lt;/li&gt;
  &lt;li&gt;介绍三种使用Transformer作为特征提取器的网络：GPT、BERT。分别讨论其思路、原理、输入输出方式、下游任务的匹配方式，介绍他们的联系和区别&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;一回顾transformer&quot;&gt;一：回顾Transformer&lt;/h5&gt;

​	RNN的两个缺点：1.并行性能力差；2.捕获长期依赖的能力差

​	第一点：RNN之所以是RNN，能将其和其它模型区分开的最典型标志是：T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt，另外一个输入，T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)，这种序列依赖关系使得RNN无法并行计算，只能按着时间步一个单词一个单词往后走。

​	第二点：RNN长期依赖的根本问题是，经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。梯度爆炸可以使用梯度修剪的方式解决。梯度消失的问题在引进LSTM和GRU之后也得到了解决，然而LSTM或者GRU都没有解决RNN无法并行计算的局限性。新的特征提取器Transformer能够同时解决这两个问题。

​	Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，简单来说就是每个单词都会产生三个向量——query、key、value，当前单词的query向量和其他单词的key向量进行内积操作，并且进行softmax归一化之后会得到当前单词和其他单词的注意力打分，然后使用注意力打分对所有的单词的值向量做加权求和。Transformer是用位置函数来进行位置编码的。Self attention层的输出会传递到前馈（feed-forward）神经网络中，每个位置的单词对应的前馈神经网络都完全一样，不是共享参数的而是各自独立的。前馈神经网络的输出就是对于特定单词想要得到的最终的词嵌入。

&lt;h5 id=&quot;二语境化的词嵌入elmo&quot;&gt;二：语境化的词嵌入ELMo&lt;/h5&gt;

&lt;strong&gt;《ELMO：Deep contextualized word representations》&lt;/strong&gt;是NAACL 2018的最佳论文，全称为Embedding from Language Models，它解决了以往使用RNN做为特征提取器的Word Embedding网络的一个没有解决的问题：语义多样性的问题，比如一个单词“bank”“有多种含义，取决于它的上下文是什么。ELMO的&lt;strong&gt;本质思想&lt;/strong&gt;是：先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。

&lt;img src=&quot;https://pic4.zhimg.com/80/v2-fe335ea9fdcd6e0e5ec4a9ac0e2290db_hd.jpg&quot; alt=&quot;&quot; /&gt;

具体的&lt;strong&gt;实现原理&lt;/strong&gt;：它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 &lt;img src=&quot;https://www.zhihu.com/equation?tex=W_i&quot; alt=&quot;W_i&quot; /&gt; 的上下文去正确预测单词 &lt;img src=&quot;https://www.zhihu.com/equation?tex=W_i&quot; alt=&quot;W_i&quot; /&gt; ， &lt;img src=&quot;https://www.zhihu.com/equation?tex=W_i&quot; alt=&quot;W_i&quot; /&gt; 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 &lt;img src=&quot;https://www.zhihu.com/equation?tex=W_i&quot; alt=&quot;W_i&quot; /&gt; 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。

&lt;strong&gt;总结&lt;/strong&gt;：ELMo使用了双向语言建模的方式使得词嵌入能够获取上下文的信息；ELMo根据具体语境下将单词的三个Embedding融合的方式解决多语义的问题。&lt;strong&gt;不足之处&lt;/strong&gt;：BiLSTM的特征提取能力要显著低于Transformer。

&lt;h5 id=&quot;三gpt&quot;&gt;三：GPT&lt;/h5&gt;

GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，ELMO在做语言模型预训练的时候，预测单词 &lt;img src=&quot;https://www.zhihu.com/equation?tex=W_i&quot; alt=&quot;W_i&quot; /&gt; 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。

&lt;img src=&quot;https://pic1.zhimg.com/80/v2-5028b1de8fb50e6630cc9839f0b16568_hd.jpg&quot; alt=&quot;&quot; /&gt;

总结：GPT在BERT之前使用Transformer+两阶段训练的方式获取word Embedding，这种做法已经成为了NLP预训练模型的标准方式，所以GPT的贡献是具有开创性的。但是因为没有使用双向语言模型使得其效果很快被BERT超越，从事后看，BERT本质上也就是比GPT多使用了双向语言模型。

&lt;h5 id=&quot;四bert&quot;&gt;四：BERT&lt;/h5&gt;

BERT=Transformer+双向语言建模预训练+下游任务Fine-tunning

Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。

&lt;img src=&quot;https://pic3.zhimg.com/v2-330788d33e39396db17655e42c7f6afa_r.jpg&quot; alt=&quot;&quot; /&gt;

&lt;strong&gt;第一阶段的预训练&lt;/strong&gt;

BERT 的创新点在于它将双向 Transformer 用于语言模型，没有使用传统的从左到右或从右到左的语言模型来预训练 BERT，而是使用两个新型无监督预测任务。

任务 #1：Masked LM

在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。

这样就需要：

&lt;ol&gt;
  &lt;li&gt;在 encoder 的输出上添加一个分类层&lt;/li&gt;
  &lt;li&gt;用嵌入矩阵乘以输出向量，将其转换为词汇的维度&lt;/li&gt;
  &lt;li&gt;用 softmax 计算词汇表中每个单词的概率&lt;/li&gt;
&lt;/ol&gt;

BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。

&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/1667471-29bc20334044e169.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/777/format/webp&quot; alt=&quot;&quot; /&gt;

任务 #2：下一句预测

在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
 在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。

为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：

&lt;ol&gt;
  &lt;li&gt;在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。&lt;/li&gt;
  &lt;li&gt;将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。&lt;/li&gt;
  &lt;li&gt;给每个 token 添加一个位置 embedding，来表示它在序列中的位置。&lt;/li&gt;
&lt;/ol&gt;

为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：

&lt;ol&gt;
  &lt;li&gt;整个输入序列输入给 Transformer 模型&lt;/li&gt;
  &lt;li&gt;用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量&lt;/li&gt;
  &lt;li&gt;用 softmax 计算 IsNextSequence 的概率&lt;/li&gt;
&lt;/ol&gt;

在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。

&lt;strong&gt;第二阶段微调Fine-tuning&lt;/strong&gt;

BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：

&lt;ol&gt;
  &lt;li&gt;在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层&lt;/li&gt;
  &lt;li&gt;在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。&lt;/li&gt;
  &lt;li&gt;在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。&lt;/li&gt;
&lt;/ol&gt;

&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/1667471-aa82f64085510604.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/522/format/webp&quot; alt=&quot;&quot; /&gt;

&lt;ul&gt;
  &lt;li&gt;句子关系类任务，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。&lt;/li&gt;
  &lt;li&gt;对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；&lt;/li&gt;
  &lt;li&gt;对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。&lt;/li&gt;
  &lt;li&gt;生成类任务,尽管Bert论文没有提，最简单的是直接在单个Transformer结构上加装隐层产生输出，更复杂一点就是使用encoder-decoder结构，编码器和解码器都是用预训练的词嵌入进行初始化。&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 28 Feb 2019 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/</guid>
        
        <category>nlp</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>流畅的python笔记</title>
        <description>[TOC]

&lt;h3 id=&quot;1鸭子模型&quot;&gt;1.鸭子模型&lt;/h3&gt;

在程序设计中，鸭子类型（duck typing）是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由”当前方法和属性的集合”决定。“鸭子测试”可以这样表述:

&lt;blockquote&gt;
  一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟可以被称为鸭子“
&lt;/blockquote&gt;

在鸭子类型中，关注点在于对象的行为，能作什么；而不是关注对象所属的类型。例如，在不使用鸭子类型的语言中，我们可以编写一个函数，它接受一个类型为”鸭子”的对象，并调用它的”走”和”叫”方法。在使用鸭子类型的语言中，这样的一个函数可以接受一个任意类型的对象，并调用它的”走”和”叫”方法。如果这些需要被调用的方法不存在，那么将引发一个运行时错误。任何拥有这样的正确的”走”和”叫”方法的对象都可被函数接受的这种行为引出了以上表述，这种决定类型的方式因此得名。

鸭子类型通常得益于”不”测试方法和函数中参数的类型，而是依赖文档、清晰的代码和测试来确保正确使用。

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#使用鸭子类型处理单个字符串或由字符串组成的可迭代对象
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;field_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;field_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;field_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;field_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 16 Jul 2018 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</guid>
        
        <category>python</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>深度学习小记</title>
        <description>&lt;ul&gt;
  &lt;li&gt;人工智能早期，计算机能够迅速解决那些对人类智力来说非常困难的问题。而人工智能的真正挑战在于解决那些对人来说很容易执行，但很难形式化描述的任务，如识别人们所说的话或图像中的脸。&lt;/li&gt;
  &lt;li&gt;深度学习讨论的是一种让计算机从经验中学习，并根据层次化的概念来__理解世界__的解决方案，而每个概念则通过某些相对简单的概念之间的关系来定义&lt;/li&gt;
  &lt;li&gt;人类擅长对事物抽象因而能够认识世界，计算机则只能做一些形式化的数据处理，人工智能要做的就是通过形式化的数据处理__从另一条路径__达到认识世界的目的&lt;/li&gt;
  &lt;li&gt;深度学习
    &lt;ul&gt;
      &lt;li&gt;计算机难以理解原始感官输入数据的含义，如表示为像素值集合的图像。将一组像素映射到对象标识的函数非常复杂。&lt;/li&gt;
      &lt;li&gt;深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个模型的不同层描述）
        &lt;ul&gt;
          &lt;li&gt;可见层：包含我们能观察到的变量&lt;/li&gt;
          &lt;li&gt;隐藏层：它们的值不再数据中给出，所以称为隐藏层。模型必须确定那些概念有利于解释观察数据中关系&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;课程组织
    &lt;ul&gt;
      &lt;li&gt;介绍基本的数学工具和机器学习的概念&lt;/li&gt;
      &lt;li&gt;介绍最成熟的深度学习算法&lt;/li&gt;
      &lt;li&gt;讨论某些具有展望性的想法，他们被广泛的认为是深度学习未来的研究重点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 16 Jul 2018 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/</guid>
        
        <category>deep_learning</category>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>如何阅读论文</title>
        <description>&lt;h1 id=&quot;硕士班研究所新生手册观点收获&quot;&gt;硕士班研究所新生手册(观点收获)&lt;/h1&gt;

Issued by and valid in the PPSC Lab.Directed by Prof.MH.Perng

&lt;h3 id=&quot;论文的要求&quot;&gt;论文的要求&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;论文的主要内容，是叙述一套方法在一个特定场合中的应用&lt;/li&gt;
  &lt;li&gt;这套方法必须要有所创新或突破，并因而对学术界有所贡献。因此，他或者是解决既有问题的新方法，或者是既有方法的新应用，或者是以一个新的方法开启一整片新的应用领域。&lt;/li&gt;
  &lt;li&gt;提出足够的证据来让读者信服说：针对这个应用场合，你所提出的方法确实比文献中一切方法有更优越之处&lt;/li&gt;
  &lt;li&gt;清楚的指出这个方法在应用上的限制，并且提出充分证据来说服读者，任何应用场合，只要能满足你所提出来的假设（前提）条件，你的方法一定适用，而且你所描述的有点就一定存在&lt;/li&gt;
  &lt;li&gt;清楚指出这个方法的限制和可能的缺点&lt;/li&gt;
  &lt;li&gt;清楚地交代这个方法的应用程序以及所有仿真或实验结果的过程，使得这个专业领域内的任何读者，都有办法根据你的描述，在他的实验室中复制出你的研究成果。&lt;/li&gt;
  &lt;li&gt;你对这个方法中每一个步骤都必须要提供充分的理由说明“为什么非如此不可”。&lt;/li&gt;
  &lt;li&gt;论文必须在适当的位置清楚注明所有和你研究之题目相关的文献，而且是和你所研究的问题相关的学术文献（尤其是学术期刊论文），你都有必要全部找出来（如果漏掉就是你的过失），仔细读过。假如在学位论文口试时，有口试委员指出有一篇既有文献，在你所讨论的问题中处理的比你的方法还好，这就构成你论文无法及格的充分理由&lt;/li&gt;
  &lt;li&gt;所谓对学术界的贡献指的是：把你的所有研究成果扣除学术界已经发表的所有成果（不管你实际有没有参考过，没有参考过也算是你的重大过失），剩下的就是你的贡献。假如这个贡献太少，也构成你的论文无法及格的充分理由。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;完成硕士论文所需要的能力&quot;&gt;完成硕士论文所需要的能力&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;数据检索的能力
    &lt;ul&gt;
      &lt;li&gt;有能力利用数据检索系统（教育部[博硕士论文检索系统]、Compendex和SCI这三套论文数据索引系统），查处所有相关的论文，而无任何遗漏&lt;/li&gt;
      &lt;li&gt;关键词很重要，假如你用的关键词太一般化，通常找到的集合会太大，除了所有相关文献之外还加上好几十倍的不相关的文献&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;资料筛选能力：在你找到的百来篇和你的研究子题直接且密切相关的论文中，你如何只读论文的题目、摘要、简介和结论，而还没有完全看懂内文，就准确的判断这篇论文中是否值得你进一步参考的内容，以便快速的把需要仔细读完的论文从数百篇为二三十篇。&lt;/li&gt;
  &lt;li&gt;期刊论文的阅读能力：硕士毕业生和大学毕业生的最大区别就是：学士只需要吸收系统的能力（也就是读别人整理、组织好的知识，典型的就是课本），但硕士则学习过自己从无组织的知识中检索、筛选、组织知识的能力&lt;/li&gt;
  &lt;li&gt;期刊论文的分析能力
    &lt;ul&gt;
      &lt;li&gt;一个严格训练过的合格硕士，他做事的时候应该是不需要有人在背后替他做检证，他自己就应该要有能力分析自己的优缺点，主动向上级或者平行单位要求支持。其实，至少要能够完成这个能力，才勉强可以说你是有“独立自主的判断能力。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;创新的能力
    &lt;ul&gt;
      &lt;li&gt;大学毕业生的主要能力是吸收既有知识，但硕士毕业生却应该要有能力创造知识&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;为什么要坚持培养阅读与分析期刊论文的能力&quot;&gt;为什么要坚持培养阅读与分析期刊论文的能力&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;对于那些之想学现成技术而不想研究方法的学生，十年后可能会因为不会读期刊论文而面临提前退休&lt;/li&gt;
  &lt;li&gt;技术的创新不是完全靠天才，只要学会分析期刊论文的优缺点和一套技术创新的方法，几乎就可以轻易的组合出你所需要的绝大部分创意。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;期刊论文的分析技巧和程序&quot;&gt;期刊论文的分析技巧和程序&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Abstract
    &lt;ul&gt;
      &lt;li&gt;说明这篇论文的主要贡献、方法特色与主要内容，需要培养只看Abstract和Inroduction便可以判断出这篇论文和你研究的有没有直接关系的能力，从而决定要不要把它读完。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Introduction
    &lt;ul&gt;
      &lt;li&gt;功能是介绍问题的背景和起源，交代前人在这个题目上已经有过的主要贡献和遗留的问题&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;main body
    &lt;ul&gt;
      &lt;li&gt;这篇论文的假设及其成立的难度以判断其参考价值&lt;/li&gt;
      &lt;li&gt;在这些假设下，这篇论文的主要好处&lt;/li&gt;
      &lt;li&gt;这些好处主要表现在那些公式的那些项目的简化上，从中可以评估出这个方法使用上的方便程度或者计算效率&lt;/li&gt;
      &lt;li&gt;不需要完全弄懂一片论文所有的恒等式推导过程或者把整篇论文细细读完，只需要把确定会用到的部分完全搞懂就好，不确定或者不会用到的地方，只需要了解他的主要点子就够了。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;论文阅读的补充说明&quot;&gt;论文阅读的补充说明&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;硕士生开始读论文的时候容易犯的毛病：
    &lt;ul&gt;
      &lt;li&gt;老是想逐行读懂，有一行读不懂就受不了&lt;/li&gt;
      &lt;li&gt;不敢发挥自己的想象，读论文像在读教科书，论文没写的就不会，自己去猜测或想象的时候，老怕弄错作者的意思&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每次读论文要呆着问题去阅读，只图你要回答的问题。因此要有选择的阅读，由粗而细，一定是一整批一起读懂到某个层次，而不是逐篇逐篇的一次读懂&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;论文报告的要求与技巧&quot;&gt;论文报告的要求与技巧&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;第一页列出论文的题目、作者、论文出处与年份&lt;/li&gt;
  &lt;li&gt;每一页幻灯片只能讲一个观念&lt;/li&gt;
  &lt;li&gt;说明这篇论文所研究的问题的重点，以及这个问题可能和工业界的哪些应用有关&lt;/li&gt;
  &lt;li&gt;清楚交代这篇论文的主要假设、主要公式与主要应用方式（以及应用上可能的解题流程）&lt;/li&gt;
  &lt;li&gt;说明这篇论文的范例（simulation examples and/or experiments)，预测这个方法在不同场合可能会有的准确度或者好用的程度&lt;/li&gt;
  &lt;li&gt;你个人的分析、评价与批评，包括：
    &lt;ul&gt;
      &lt;li&gt;这篇论文的最主要创意是什么？&lt;/li&gt;
      &lt;li&gt;这些创意在应用上有什么好处？&lt;/li&gt;
      &lt;li&gt;这些创意和应用上的好处是哪些条件下才能成立？&lt;/li&gt;
      &lt;li&gt;这篇论文最主要的缺点或局限是什么？&lt;/li&gt;
      &lt;li&gt;这些缺点或局限在应用上有什么坏处？&lt;/li&gt;
      &lt;li&gt;这些缺点和应用上的坏处是因为哪些因素而引入的？&lt;/li&gt;
      &lt;li&gt;你建议学长学弟的时候参考这篇论文的哪些部分（点子）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 16 Jul 2018 21:35:10 +0800</pubDate>
        <link>http://localhost:4000/2018/07/16/%E7%A1%95%E5%A3%AB%E7%8F%AD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E6%89%8B%E5%86%8C-%E8%A7%82%E7%82%B9%E6%94%B6%E8%8E%B7/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/16/%E7%A1%95%E5%A3%AB%E7%8F%AD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%96%B0%E7%94%9F%E6%89%8B%E5%86%8C-%E8%A7%82%E7%82%B9%E6%94%B6%E8%8E%B7/</guid>
        
        <category>paper</category>
        
        
        <category>notes</category>
        
      </item>
    
  </channel>
</rss>
