<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/page4/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文《MT-DNN》笔记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-10
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#paper" title="Tag: paper" rel="tag">paper</a>&nbsp;
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h2 id="论文multi-task-deep-neural-networks-for-natural-language-understanding笔记">论文《Multi-Task Deep Neural Networks for Natural Language Understanding》笔记</h2>

<blockquote>
  <p>论文地址：<a href="https://arxiv.org/pdf/1901.11504.pdf">微软MT-DNN论文《Multi-Task Deep Neural Networks for Natural Language Understanding》</a></p>
</blockquote>

<h4 id="mt-dnn简单介绍">MT-DNN简单介绍</h4>

<p>​	谷歌的<a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>在各个NLP任务（GLUE、SQuAD、命名实体识别、SWAG)上的表现都很好，但是BERT在词向量的预训练的第二阶段只使用了单个任务进行模型fine-tune，我们自然而然地会问：Fine-tune阶段使用多任务同时对网络参数进行微调效果会不会更好？。</p>

<p>​	微软研究院在2019年发布的论文《Multi-Task Deep Neural Networks for Natural Language Understanding》就做了这方面的实验。论文提出了一个假设：在单一领域的数据集上使用单一的任务训练模型限制了模型的泛化。MT-DNN提供的思路是：利用多任务之间的约束来避免单一任务上的过拟合问题，从而提高模型的泛化能力。文章中使用的多任务是相似的，作者任务机器能够像人一样在相似的任务中获取到相关的经验，比如会滑雪的人就能比较容易的学会滑冰，对机器来说也就是能够使用更少的训练数据是模型获得相同的效果。</p>

<h4 id="实验结果">实验结果</h4>

<p>​	<strong>(1)</strong>MT-DNN在8/9的GLUE<a href="Gerneral Lanuage Understanding Evaluation，是评估模型自然语言理解能力的最权威的指标">1</a>任务中取得了SOAT成绩，其中未达到SOAT成绩的原因是数据集存在问题。这８个数据集（任务）可以归纳分为以下四种类别：</p>

<table>
  <thead>
    <tr>
      <th>任务</th>
      <th>数据集</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Single sentence classification</td>
      <td>CoLA：情感分类<br />SST-2：判断句子是否符合语法要求</td>
    </tr>
    <tr>
      <td>Text similarity score</td>
      <td>STS-B：两句话的相似性</td>
    </tr>
    <tr>
      <td>Pairwise Text classification</td>
      <td>RET、MNLI：判断两句话的关系(emtaiment, controdictional, neutral)<br />QQP, MRPC：判断那两句话是否具有相同的语义</td>
    </tr>
    <tr>
      <td>Relevence ranking</td>
      <td>QNLI：判断问答句子对的相关性</td>
    </tr>
  </tbody>
</table>

<p>​	<strong>(2)</strong>通过这种多任务训练得到的模型能够很好的适用于其他未见过的相似任务，即使只有很少的带标注的数据。因为MT-DNN底层使用的是BERT(Base)的网络，所以这种相似任务之间的适用性的提高可以确定由多任务的fine-tune带来的。实验表明即使只使用原始数据集的0.1%、1%样本，同样能够获得不错的准确率。下面是MT-DNN模型和BERT两个模型在SNLI数据集上的表现：</p>

<table>
  <thead>
    <tr>
      <th>模型</th>
      <th>0.1%</th>
      <th>1％</th>
      <th>10%</th>
      <th>100%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BERT</td>
      <td>51%</td>
      <td>82%</td>
      <td>90%</td>
      <td>94%</td>
    </tr>
    <tr>
      <td>MT-DNN</td>
      <td>82%</td>
      <td>88%</td>
      <td>91%</td>
      <td>96%</td>
    </tr>
  </tbody>
</table>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/">自然语言与多模态交互前沿技术</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-06
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h3 id="北大ai第三讲何晓东-京东ai研究院常务副院长ieee-fellow自然语言与多模态交互前言技术">北大AI第三讲：何晓东-京东AI研究院常务副院长、IEEE Fellow：自然语言与多模态交互前言技术</h3>

<h4 id="nlp进展">NLP进展：</h4>

<ul>
  <li>语言理解/语义槽值提取</li>
  <li>语言理解/意图分类，2016年提出层次化注意力模型（HAN），以更好的在词、句子、段落、等多个层面来理解语言，判断意图，并通过对神经元激活的的可视化给出一定程度的可解释性。</li>
  <li>语言理解/语义的表征：从自然语言中提取出语义并将其投影到语义空间以帮助搜索、推荐、分类、问答等应用；自然语言的描述，通过深度神经网络逐步抽取语义上的不变形（invariance），生成抽象的语义表征。</li>
  <li>机器阅读理解（MRC）：机器阅读文本，回答问题；BERT模型在SQuAD封闭数据集上的成绩已经超过人类</li>
</ul>

<h4 id="未来">未来</h4>

<ul>
  <li>多模态智能：综合文字、语音、图像、知识图谱等信息来获取信息
    <ul>
      <li>建立多模态语义空间：联结图像和文字
        <ul>
          <li>通过深度结构语义模型（DSSM）把图像和文字表征成语义空间内的向量</li>
          <li>在此空间中进行语义相似度计算，生成最匹配图像内容的文字表述。</li>
        </ul>
      </li>
      <li>理解场景和知识，用语言表达（image caption）
        <ul>
          <li>一个棒球</li>
          <li>一个棒球运动员</li>
          <li>一个棒球运动员在扔</li>
          <li>一个棒球运动员在扔一个球</li>
        </ul>
      </li>
      <li>图像描述机器人：CaptionBot</li>
      <li>智能绘画机器人：AI根据语言描述创作绘画</li>
      <li>AI+Art：更多（艺术化的）创作</li>
      <li>综合图像和语言推理，回答问题：eg.那两把蓝色椅子之间是什么？</li>
      <li>视觉-语言多模态导航：结合语言理解和对环境的视觉信息建模，机器人按指令从一个地方走到另一个地方。</li>
    </ul>
  </li>
  <li>复杂内容创作：比如人工智能写作（长文章）
    <ul>
      <li>创作长文的技术挑战：
        <ul>
          <li>从简单输入到创作长文需要大量内容的扩充</li>
          <li>长文的生成要可控，能满足组合爆炸式需求。模型需要时组合性的，能与训练的。</li>
          <li>现有的端到端的模型不适合长文创作：为短文本生成（如机器翻译）而设计，不能抓住长文的高层语义。度量优化，信用分配，维持一致性，目标函数平衡等要重新设计。</li>
        </ul>
      </li>
      <li>长文创作的前沿探索：比如顶层设计和规划
        <ul>
          <li>现有的文本生成模型缺乏“规划”，应先产生粗略的高层主题规划，然后再对主题和子主题展开长文</li>
          <li>最近的一些工作：多层增强学习模型及其在主题设计和长文生成中的应用。</li>
        </ul>
      </li>
      <li>创作诗歌（控制）</li>
    </ul>
  </li>
  <li>情感智能：不只识别人的情感，还能像人一样表达情感和风格
    <ul>
      <li>生成带情感的语言：让AI在语言表达中加入情感，提升用户体验</li>
      <li>表达情感和风格：让AI用语言表达浪漫或者幽默的风格——StyleNet</li>
    </ul>
  </li>
  <li>多轮人机对话：理解语境、常识、语言，生成逻辑严谨的有情感的对话，服务于人
    <ul>
      <li>图灵测试：通过人类和机器之间的自然语言对话来判断机器是否具有智能。</li>
      <li>主要的人机对话系统框架：任务型对话系统、问答型对话系统、聊天型对话系统、检索性对话系统</li>
      <li>我们将成为有史以来第一代与AI共生的人类，《从Eliza到小冰：社交对话机器人的机遇与挑战》</li>
    </ul>
  </li>
</ul>

<h4 id="ai产业化的下一个方向是什么">AI产业化的下一个方向是什么</h4>

<ul>
  <li>
    <p>智能服务产业是新蓝海：传统人类密集型产业，有广阔自动化、智能化空间；随着AI技术、IOT技术等的创新，市场在快速成长。</p>
  </li>
  <li>
    <p>服务型对话</p>

    <ul>
      <li>服务：生活、娱乐、消费、客服等为人提供的的服务</li>
      <li>对话：多模态、大规模开放领域，具有常识和情感、能完成复杂任务的智能交互技术。</li>
    </ul>
  </li>
  <li>
    <p>产业界应用：</p>

    <ul>
      <li>京东客服机器人：首个大规模商用情感客服机器人：能够检测用户的情感类型，做出道歉、安抚、祝福的动作，提升用户体验。</li>
      <li>京东智能服务产品矩阵：JIMI和AlphaSales</li>
      <li>京东智能IoT</li>
      <li>京东智能市政服务</li>
    </ul>
  </li>
  <li>
    <p>人机融合、多模态智能服务的产业时代</p>

    <ul>
      <li>
        <p>服务即对话：多模态、大规模开放领域、具有常识和情感、能完成复杂任务的对话系统是推动下一代智能产业的核心技术。</p>

        <table>
          <thead>
            <tr>
              <th>分级</th>
              <th>目标</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>低级智能对话</td>
              <td>对用户简单意图进行识别并给出预设答案</td>
            </tr>
            <tr>
              <td>初级智能对话</td>
              <td>能识别复杂意图，联系上下文给出回答</td>
            </tr>
            <tr>
              <td>中级智能对话</td>
              <td>根据用户问题及情绪完成个性化多轮对话，协助用户完成目标</td>
            </tr>
            <tr>
              <td>高级智能对话</td>
              <td>能对多模态信息进行推理，自主判断，并组织语言与用户沟通，具备自我学习能力</td>
            </tr>
            <tr>
              <td>通用智能对话</td>
              <td>能基于一切信息开展自我学习，自我适应，及自我创新。在复杂问题领域达到人类水平。</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<h4 id="提问环节">提问环节</h4>

<ul>
  <li>关于OpenAI的GPT模型暴力解决问题的看法
    <ul>
      <li>算法是解决问题的核心，但是光有算法是不够的。GPT2是算法和产业界结合的一个例子，有大量高质量的数据和算力。对科研机构不算一个坏事。算法是灵魂，数据和算力是物质基础</li>
    </ul>
  </li>
  <li>NLP领域的问题是否可以认为比图像领域的问题要难解决，所有在进度上有所落后？
    <ul>
      <li>NLP领域的问题是一种认知领域的问题，比图像领域的感知问题要复杂一些，所以图像领域的问题能够比较清晰的被定义，相对而言也会推进的更加深入。</li>
    </ul>
  </li>
  <li>如何判断机器是否真正理解了人类的问题？
    <ul>
      <li>这是一个哲学问题，科学家的作用就是将哲学问题转化成科学问题，可以通过定义一些测试任务来进行一定程度上的判断。比如小冰提出了聊天轮数的metric，作为判断聊天机器人是否能像人类一样进行聊天。</li>
    </ul>
  </li>
  <li>智能音响等IoT产品的发展趋势？
    <ul>
      <li>每一次交互的革命都能带来一个万亿级别的产业，智能IoT就具有这样的潜力</li>
    </ul>
  </li>
  <li>如何看待当前CV行业如火如荼，NLP行业相对比较平静的现象？
    <ul>
      <li>当问题已经定义的很清楚的时候，机会相对来说就小了很多。智能服务产业的未来可能要比CV的智能安防产业还要大。</li>
    </ul>
  </li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/">最近阅读文章小抄</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-06
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h4 id="2019121nlp-领域的-imagenet-时代到来词嵌入已死语言模型当立">2019.1.21–《NLP 领域的 ImageNet 时代到来：词嵌入「已死」，语言模型当立》</h4>

<p>计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/">Transformer在NLP词向量预训练中的应用</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-02-28
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h4 id="bert发展沿革">BERT发展沿革</h4>

<h5 id="四个部分">四个部分</h5>

<ul>
  <li>回顾Transformer的网络结构、和RNN、RNN的对比</li>
  <li>介绍ELMo模型语境化的词嵌入，重点介绍其双向语言模型和根据具体语境生成词嵌入的原理</li>
  <li>介绍三种使用Transformer作为特征提取器的网络：GPT、BERT。分别讨论其思路、原理、输入输出方式、下游任务的匹配方式，介绍他们的联系和区别</li>
</ul>

<h5 id="一回顾transformer">一：回顾Transformer</h5>

<p>​	RNN的两个缺点：1.并行性能力差；2.捕获长期依赖的能力差</p>

<p>​	第一点：RNN之所以是RNN，能将其和其它模型区分开的最典型标志是：T时刻隐层状态的计算，依赖两个输入，一个是T时刻的句子输入单词Xt，另外一个输入，T时刻的隐层状态St还依赖T-1时刻的隐层状态S(t-1)，这种序列依赖关系使得RNN无法并行计算，只能按着时间步一个单词一个单词往后走。</p>

<p>​	第二点：RNN长期依赖的根本问题是，经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。梯度爆炸可以使用梯度修剪的方式解决。梯度消失的问题在引进LSTM和GRU之后也得到了解决，然而LSTM或者GRU都没有解决RNN无法并行计算的局限性。新的特征提取器Transformer能够同时解决这两个问题。</p>

<p>​	Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，简单来说就是每个单词都会产生三个向量——query、key、value，当前单词的query向量和其他单词的key向量进行内积操作，并且进行softmax归一化之后会得到当前单词和其他单词的注意力打分，然后使用注意力打分对所有的单词的值向量做加权求和。Transformer是用位置函数来进行位置编码的。Self attention层的输出会传递到前馈（feed-forward）神经网络中，每个位置的单词对应的前馈神经网络都完全一样，不是共享参数的而是各自独立的。前馈神经网络的输出就是对于特定单词想要得到的最终的词嵌入。</p>

<h5 id="二语境化的词嵌入elmo">二：语境化的词嵌入ELMo</h5>

<p><strong>《ELMO：Deep contextualized word representations》</strong>是NAACL 2018的最佳论文，全称为Embedding from Language Models，它解决了以往使用RNN做为特征提取器的Word Embedding网络的一个没有解决的问题：语义多样性的问题，比如一个单词“bank”“有多种含义，取决于它的上下文是什么。ELMO的<strong>本质思想</strong>是：先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p>

<p><img src="https://pic4.zhimg.com/80/v2-fe335ea9fdcd6e0e5ec4a9ac0e2290db_hd.jpg" alt="" /></p>

<p>具体的<strong>实现原理</strong>：它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 的上下文去正确预测单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> ， <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。</p>

<p><strong>总结</strong>：ELMo使用了双向语言建模的方式使得词嵌入能够获取上下文的信息；ELMo根据具体语境下将单词的三个Embedding融合的方式解决多语义的问题。<strong>不足之处</strong>：BiLSTM的特征提取能力要显著低于Transformer。</p>

<h5 id="三gpt">三：GPT</h5>

<p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，ELMO在做语言模型预训练的时候，预测单词 <img src="https://www.zhihu.com/equation?tex=W_i" alt="W_i" /> 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。</p>

<p><img src="https://pic1.zhimg.com/80/v2-5028b1de8fb50e6630cc9839f0b16568_hd.jpg" alt="" /></p>

<p>总结：GPT在BERT之前使用Transformer+两阶段训练的方式获取word Embedding，这种做法已经成为了NLP预训练模型的标准方式，所以GPT的贡献是具有开创性的。但是因为没有使用双向语言模型使得其效果很快被BERT超越，从事后看，BERT本质上也就是比GPT多使用了双向语言模型。</p>

<h5 id="四bert">四：BERT</h5>

<p>BERT=Transformer+双向语言建模预训练+下游任务Fine-tunning</p>

<p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。</p>

<p><img src="https://pic3.zhimg.com/v2-330788d33e39396db17655e42c7f6afa_r.jpg" alt="" /></p>

<p><strong>第一阶段的预训练</strong></p>

<p>BERT 的创新点在于它将双向 Transformer 用于语言模型，没有使用传统的从左到右或从右到左的语言模型来预训练 BERT，而是使用两个新型无监督预测任务。</p>

<p>任务 #1：Masked LM</p>

<p>在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。</p>

<p>这样就需要：</p>

<ol>
  <li>在 encoder 的输出上添加一个分类层</li>
  <li>用嵌入矩阵乘以输出向量，将其转换为词汇的维度</li>
  <li>用 softmax 计算词汇表中每个单词的概率</li>
</ol>

<p>BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。</p>

<p><img src="https://upload-images.jianshu.io/upload_images/1667471-29bc20334044e169.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/777/format/webp" alt="" /></p>

<p>任务 #2：下一句预测</p>

<p>在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
 在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。</p>

<p>为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：</p>

<ol>
  <li>在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。</li>
  <li>将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。</li>
  <li>给每个 token 添加一个位置 embedding，来表示它在序列中的位置。</li>
</ol>

<p>为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：</p>

<ol>
  <li>整个输入序列输入给 Transformer 模型</li>
  <li>用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量</li>
  <li>用 softmax 计算 IsNextSequence 的概率</li>
</ol>

<p>在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。</p>

<p><strong>第二阶段微调Fine-tuning</strong></p>

<p>BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：</p>

<ol>
  <li>在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层</li>
  <li>在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。</li>
  <li>在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。</li>
</ol>

<p><img src="https://upload-images.jianshu.io/upload_images/1667471-aa82f64085510604.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/522/format/webp" alt="" /></p>

<ul>
  <li>句子关系类任务，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。</li>
  <li>对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；</li>
  <li>对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。</li>
  <li>生成类任务,尽管Bert论文没有提，最简单的是直接在单个Transformer结构上加装隐层产生输出，更复杂一点就是使用encoder-decoder结构，编码器和解码器都是用预训练的词嵌入进行初始化。</li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">流畅的python笔记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2018-07-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#python" title="Tag: python" rel="tag">python</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>[TOC]</p>

<h3 id="1鸭子模型">1.鸭子模型</h3>

<p>在程序设计中，鸭子类型（duck typing）是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由”当前方法和属性的集合”决定。“鸭子测试”可以这样表述:</p>

<blockquote>
  <p>一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟可以被称为鸭子“</p>
</blockquote>

<p>在鸭子类型中，关注点在于对象的行为，能作什么；而不是关注对象所属的类型。例如，在不使用鸭子类型的语言中，我们可以编写一个函数，它接受一个类型为”鸭子”的对象，并调用它的”走”和”叫”方法。在使用鸭子类型的语言中，这样的一个函数可以接受一个任意类型的对象，并调用它的”走”和”叫”方法。如果这些需要被调用的方法不存在，那么将引发一个运行时错误。任何拥有这样的正确的”走”和”叫”方法的对象都可被函数接受的这种行为引出了以上表述，这种决定类型的方式因此得名。</p>

<p>鸭子类型通常得益于”不”测试方法和函数中参数的类型，而是依赖文档、清晰的代码和测试来确保正确使用。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#使用鸭子类型处理单个字符串或由字符串组成的可迭代对象
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">field_names</span> <span class="o">=</span> <span class="n">field_names</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">','</span><span class="p">,</span><span class="s">' '</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">AttributeError</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="n">field_names</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">field_names</span><span class="p">)</span>
</code></pre></div></div>


                </div>
                <div class="read-all">
                    <a  href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/">深度学习小记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2018-07-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#deep_learning" title="Tag: deep_learning" rel="tag">deep_learning</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <ul>
  <li>人工智能早期，计算机能够迅速解决那些对人类智力来说非常困难的问题。而人工智能的真正挑战在于解决那些对人来说很容易执行，但很难形式化描述的任务，如识别人们所说的话或图像中的脸。</li>
  <li>深度学习讨论的是一种让计算机从经验中学习，并根据层次化的概念来__理解世界__的解决方案，而每个概念则通过某些相对简单的概念之间的关系来定义</li>
  <li>人类擅长对事物抽象因而能够认识世界，计算机则只能做一些形式化的数据处理，人工智能要做的就是通过形式化的数据处理__从另一条路径__达到认识世界的目的</li>
  <li>深度学习
    <ul>
      <li>计算机难以理解原始感官输入数据的含义，如表示为像素值集合的图像。将一组像素映射到对象标识的函数非常复杂。</li>
      <li>深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个模型的不同层描述）
        <ul>
          <li>可见层：包含我们能观察到的变量</li>
          <li>隐藏层：它们的值不再数据中给出，所以称为隐藏层。模型必须确定那些概念有利于解释观察数据中关系</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>课程组织
    <ul>
      <li>介绍基本的数学工具和机器学习的概念</li>
      <li>介绍最成熟的深度学习算法</li>
      <li>讨论某些具有展望性的想法，他们被广泛的认为是深度学习未来的研究重点</li>
    </ul>
  </li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2018/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%8D%E8%A8%80/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <a href="/index.html" class="previous"><i class="fa fa-angle-double-left"></i></a>
            <a href="/page3" class="previous"><i class="fa fa-angle-left"></i></a>
          
          <span class="page_number ">4/5</span>
          
            <a href="/page5" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page5" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/">《Pretraining-Based Natural Language Generation for Text Summarization》论文解读</a></li>
                    
                        <li><a href="/2019/03/20/%E6%96%87%E6%91%98/">文摘</a></li>
                    
                        <li><a href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/">chat-bot</a></li>
                    
                        <li><a href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/">自然语言处理的是个发展趋势</a></li>
                    
                        <li><a href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/">华为李航-NLP有个基本问题，深度学习4个做的很好</a></li>
                    
                        <li><a href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/">全面解析Tensor2Tensor系统</a></li>
                    
                        <li><a href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a></li>
                    
                        <li><a href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a></li>
                    
                        <li><a href="/2019/03/13/Word2Vec_Tutorial_part2/">Word2Vec Tutorial part2</a></li>
                    
                        <li><a href="/2019/03/13/Word2Vec-Tutorial/">Word2Vec Tutorial</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">17</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#reproduction" class="categories-list-item" cate="reproduction">
                            <span class="name">
                                reproduction
                            </span>
                            <span class="badge">11</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#digest" class="categories-list-item" cate="digest">
                            <span class="name">
                                digest
                            </span>
                            <span class="badge">1</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 11pt; color: #777;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 11pt; color: #777;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 10pt; color: #888;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 14pt; color: #444;">pytorch</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 9pt; color: #999;">markdown</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cnn" style="font-size: 9pt; color: #999;">cnn</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cv" style="font-size: 9pt; color: #999;">cv</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#tensorflow" style="font-size: 9pt; color: #999;">tensorflow</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#life" style="font-size: 9pt; color: #999;">life</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
