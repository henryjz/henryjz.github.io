<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a class="active" href="/">
                        
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/">深入解析torch.nn</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-12
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://blog.csdn.net/weixin_36811328/article/details/87905208</p>
</blockquote>

<p>原文地址：<a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html#sphx-glr-download-beginner-nn-tutorial-py">WHAT IS TORCH.NN REALLY?</a>
本人英语学渣，如有错误请及时指出以便更正，使用的源码可点击原文地址进行下载。</p>

<hr />

<p>pytorch提供了许多优雅的类和模块帮助我们构建与训练网络，比如 <code class="highlighter-rouge">torch.nn</code>, <code class="highlighter-rouge">torch.optim</code>,<code class="highlighter-rouge">Dataset</code>等。为了充分利用这些模块的功能，灵活操作它们解决各种不同的问题，我们需要更好地理解当我们调用这些模块时它们到底干了些什么，为此，我们首先不调用这些模块实现<strong>MNIST</strong>手写字识别，仅使用最基本的 pytorch 张量函数。然后，我们逐渐增加 <code class="highlighter-rouge">torch.nn</code>, <code class="highlighter-rouge">torch.optim</code>, <code class="highlighter-rouge">Dataset</code>, or <code class="highlighter-rouge">DataLoader</code>,具体地展示每个模块具体干了些什么，展示这些模块是怎样使代码变得更加优雅灵活。
<strong>此教程适用范围：熟悉pytorch的张量操作</strong></p>

<h1 id="加载-mnist-数据集">加载 MNIST 数据集</h1>

<p>我们使用经典的 <code class="highlighter-rouge">MNIST</code> 数据集，一个包含了0-9数字的二值图像库。</p>

<p>还会用到 <code class="highlighter-rouge">pathlib</code> 库用于目录操作，一个python3自带的标准库。使用 <code class="highlighter-rouge">requests</code> 下载数据集。当用到一个模块时才会进行导入，而不会一开始全部导入，以便更好地理解每个步骤。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'data'</span><span class="p">)</span>
<span class="n">PATH</span> <span class="o">=</span> <span class="n">DATA_PATH</span> <span class="o">/</span> <span class="s">"mnist"</span>

<span class="n">PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">exit_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">URL</span> <span class="o">=</span> <span class="s">"http://deeplearning.net/data/mnist/"</span>
<span class="n">FILENAME</span> <span class="o">=</span> <span class="s">"mnist.pkl.gz"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
	<span class="n">content</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span> <span class="o">+</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
	<span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"wb"</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
<span class="mi">1234567891011121314</span>
</code></pre></div></div>

<p>该数据集采用numpy数组格式，并使用pickle存储，pickle是一种特定于python的格式，用于序列化数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">gzip</span>

<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="nb">open</span><span class="p">((</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">(),</span><span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
	<span class="p">((</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),(</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span><span class="p">),</span><span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="s">"latin-1"</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>每张训练图片分辨率为 28x28， 被存储为 784(=28x28) 的一行。我们输出看一下数据，首先需要转换回 28x28的图像。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">form</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span><span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p><img src="https://img-blog.csdnimg.cn/20190224165051244.png" alt="img" /></p>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50000,784)
1
</code></pre></div></div>

<p>PyTorch使用 torch.tensor ，所以我们需要对numpy类型数据进行转换</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
	<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">x_valid</span><span class="p">,</span><span class="n">y_valid</span><span class="p">)</span>
	<span class="p">)</span>
<span class="n">n</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y_train</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span><span class="n">y_train</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span><span class="n">y_train</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="mi">123456789</span>
</code></pre></div></div>

<h1 id="从头创建神经网络不使用torchnn">从头创建神经网络（不使用torch.nn）</h1>

<p>让我们仅仅使用 pytorch 中的张量操作来创建模型，假设你已经熟悉神经网络的基础知识（不熟悉请参考<a href="https://course.fast.ai/">corse.fast.ai</a> ）</p>

<p>pytorch提供了很多创建张量的操作，我们将用这些方法来初始化权值weights和偏置 bais来创建一个线性模型。这些只是常规张量，有一个非常特别的补充：我们告诉PyTorch这些张量需要支持求导(requires_grad=True)。这样PyTorch将记录在张量上完成的所有操作，以便它可以在反向传播过程中自动计算梯度！</p>

<p>对于权值weights，我们再初始化<strong>之后</strong>再设置 <code class="highlighter-rouge">requires_grad</code>,因为我们不想这一步包含在梯度的计算中（注：pytorch中以 <code class="highlighter-rouge">_</code> 结尾的操作都是在原变量中(in-place)执行的）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">780</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>多亏了pytorch的自动求导功能，我们可以使用python的所有标准函数来构建模型。 我们这儿利用矩阵乘法，加法来构建线性模型。我们编写 <code class="highlighter-rouge">log_softmax</code>函数作为激活函数。 虽然pytorch提供了大量写好的损失函数，激活函数，你依然可以自由地编写自己的函数替代它们。 pytorch 甚至支持创建自己的 GPU函数或者CPU矢量函数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>           <span class="c1"># python的广播机制
</span><span class="mi">12345</span>
</code></pre></div></div>

<p>上面的 <code class="highlighter-rouge">@</code> 符号表示向量的点乘，接下来我们会调用一批数据（batch，64张图片）输入此模型。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span> 					<span class="c1"># batch size
</span><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>    		<span class="c1"># a mini-batch from x
</span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>   		<span class="c1"># predictions
</span><span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-2.4513, -2.5024, -2.0599, -3.1052, -3.2918, -2.2665, -1.9007, -2.2588,
        -2.0149, -2.0287], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])
12
</code></pre></div></div>

<p>正如我们看到的，<code class="highlighter-rouge">preds</code> 张量不仅包含了一组张量，还包含了求导函数。反向传播的时候会用到此函数。让我们使用标准的python语句接着来实现 negative log likelihood loss 损失函数（译者加：也被称为交叉熵损失函数）：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">target</span><span class="p">):</span>
	<span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nll</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>现在用我们的损失函数来检查我们随机初始化的模型，待会就能看到再反向传播之后是否会改善模型性能。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.3620, grad_fn=&lt;NegBackward&gt;)
1
</code></pre></div></div>

<p>接下来定义一个计算准确度的函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span><span class="err">：</span>
	<span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># 得到最大值的索引
</span>	<span class="k">return</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">yb</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>检查模型的准确度：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0938)
1
</code></pre></div></div>

<p>现在我们开始循环训练模型，每一步我们执行以下操作：</p>

<ul>
  <li>选择一批数据（a batch）</li>
  <li>使用模型进行预测</li>
  <li>计算损失函数</li>
  <li>反向传播更新参数 weights 和 bias</li>
</ul>

<p>我们现在使用 <code class="highlighter-rouge">torch.no_grad()</code> 更新参数，以避免参数更新过程被记录入求导函数中。</p>

<p>然后我们清零导数，以便开始下一轮循环，否则导数会在原来的基础上累加，而非替代原来的数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.core.debugger</span> <span class="kn">import</span> <span class="n">set_trace</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># learning rate
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># how many epochs to train for
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#         set_trace()
</span>        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="mi">123456789101112131415161718192021</span>
</code></pre></div></div>

<p>目前为止，我们从头创建一个迷你版的神经网络</p>

<p>让我们来检查一下损失和准确率，并于迭代更新参数之前进行比较，我们期望得到更小的损失于更高的准确率。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0822, grad_fn=&lt;NegBackward&gt;) tensor(1.)
1
</code></pre></div></div>

<h1 id="使用-torchnnfunctional-简化代码">使用 torch.nn.functional 简化代码</h1>

<p>现在我们使用<code class="highlighter-rouge">torch.nn.functional</code>重构之前的代码，这样会使代码变得更加简洁与灵活，更易理解。</p>

<p>首先最简单的一步是，用 <code class="highlighter-rouge">torch.nn.functional</code>( 为了方便后面统一称作F) 中带有的损失函数来代替我们自己编写的函数，使得代码变得更简短。这些函数都包包含于模块 <code class="highlighter-rouge">torch.nn</code>里面，除了大量的损失函数与激活函数，里面还包含了大量用于构建网络的函数。</p>

<p>如果我们的网络中使用 negative log likelihood loss 作为损失函数， log softmax activation 作为激活函数 （即我们上面实现的损失函数与激活函数）。在pytorch中我们直接使用函数 <code class="highlighter-rouge">F.cross_entropy</code> 便可实现上面两个函数的功能。所以我们可以用此函数代替上面实现的激活函数与损失函数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
<span class="mi">123456</span>
</code></pre></div></div>

<p>让我测试一下是否和上面自己实现的函数效果一致：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)
1
</code></pre></div></div>

<h1 id="引入-nnmodule-重构代码">引入 nn.Module 重构代码</h1>

<p>接下来我们引入 <code class="highlighter-rouge">nn.Module</code>和<code class="highlighter-rouge">nn.Parameter</code> 改进代码。我们创建 <code class="highlighter-rouge">nn.Module</code>的子类。这个例子中我们创建一个包含权重，偏置，以及包含前向传播的类。<code class="highlighter-rouge">nn.Module</code>含有许多的属性与方法可供调用 （比如： <code class="highlighter-rouge">.parameters</code> <code class="highlighter-rouge">.zero_grad()</code>）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="n">sefl</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">xb</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
<span class="mi">12345678910</span>
</code></pre></div></div>

<p>接下来实例化我们的模型：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>现在我们可以和之前一样使用损失函数了。注意：<code class="highlighter-rouge">nn.Module</code> 对象可以像函数一样调用，但实际上是自动调用了对象内部的函数 <code class="highlighter-rouge">forward</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.2082, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>在之前，我们必须进行如下得操作对权重，偏置进行更新，梯度清零：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
	<span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>现在我们可以充分利用 <code class="highlighter-rouge">nn.Module</code> 的方法属性更简单地完成这些操作，如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>现在我们将整个训练过程写进函数 <code class="highlighter-rouge">fit</code>中。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
			<span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
			<span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
			<span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
			<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
			<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span>
		
			<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
			<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
				<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
				<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">fit</span><span class="p">()</span>
<span class="mi">123456789101112131415</span>
</code></pre></div></div>

<p>让我们再一次确认损失情况：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0812, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入-nnlinear-重构代码">引入 nn.Linear 重构代码</h1>

<p>比起手动定义 权重 与 偏置，并且使用 <code class="highlighter-rouge">self.weights</code>和 <code class="highlighter-rouge">self.bias</code> 来计算 <code class="highlighter-rouge">xb @ self.weights + self.bias</code>的方式，我们可以使用pytorch中的 <code class="highlighter-rouge">nn.Linear</code>来定义线性层，他自动为我们实现以上权重参数的定义以及计算的过程。除了线性模型之外，pytorch还有一系列的其它网络层供我们使用，大大简化了我们的编程过程。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">xb</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">1234567</span>
</code></pre></div></div>

<p>同上面一样实例化模型，计算损失</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.2731, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>训练，并查看训练之后的损失</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0820, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入-optim-重构代码">引入 optim 重构代码</h1>

<p>接下来使用<code class="highlighter-rouge">torch.optim</code>改进训练过程，而不用手动更新参数</p>

<p>之前的手动优化过程如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
	<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>使用如下代码替代手动的参数更新：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="c1"># optim.zero_grad() resets the gradient to 0 and we need to call it 
# before computing the gradient for the next minibatch.
</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
<span class="mi">1234</span>
</code></pre></div></div>

<p>结合之前的完整跟新代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">get_model</span><span class="p">():</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
	<span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
		<span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span><span class="n">bs</span>
		<span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
		<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
		<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span>

		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">))</span>
<span class="mi">1234567891011121314151617181920212223</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.3785, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0802, grad_fn=&lt;NllLossBackward&gt;)
12
</code></pre></div></div>

<h1 id="引入-dataset-处理数据">引入 Dataset 处理数据</h1>

<p>pytorch定义了 Dataset 类，其中主要包含了 <code class="highlighter-rouge">__len__</code> 函数与 <code class="highlighter-rouge">__getitem__</code>函数。<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">此教程</a>以创建 <code class="highlighter-rouge">FacialLandmarkDataset</code> 为例详细地介绍了Dataset类的使用。</p>

<p>pytorch的 <code class="highlighter-rouge">TensorDataset</code> 是一个包含张量的数据集。通过定义长度索引等方式，使我们更好地利用索引，切片等方法迭代数据。这会让我们很容易地在一行代码中获取我们地数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">form</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span> <span class="kn">import</span> <span class="nn">TensorDataset</span>
<span class="mi">1</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">x_train</code> <code class="highlighter-rouge">y_train</code>可以被组合进一个<code class="highlighter-rouge">TensorDataset</code>中，这会使得迭代切片更加简单。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>之前我们获取数据的方法如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>现在我们可以使用更简单的方法：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="o">+</span><span class="n">bs</span><span class="p">]</span>
<span class="mi">1</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bs</span><span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">+</span> <span class="n">bs</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">12345678910111213</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<h1 id="引入dataloader加载数据">引入DataLoader加载数据</h1>

<p><code class="highlighter-rouge">DataLoader</code> 用于批量加载数据，你可以用他来加载任何来自 <code class="highlighter-rouge">Dataset</code>的数据，它使得数据的批量加载十分容易。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>之前我们读取数据的方式：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="o">+</span><span class="n">bs</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>现在使用dataloader加载数据：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">12</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
<span class="mi">123456789101112</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)
1
</code></pre></div></div>

<p>目前为止训练模型部分我们就已经完成了，通过使用<code class="highlighter-rouge">nn.Module</code>, <code class="highlighter-rouge">nn.Parameter</code>, <code class="highlighter-rouge">DataLoader</code>, 我们的训练模型以及得到了很大的改进。接下来让我们开始模型的测试部分。</p>

<h1 id="添加测试集">添加测试集</h1>

<p>在前一部分，我们尝试了使用训练集训练网络。实际工作中，我们还会使用测试集来观察训练的模型是否过拟合。</p>

<p>打乱数据的分布有助于减小每一批(batch)数据间的关联，有利于模型的泛化。但对于测试集来说，是否打乱数据对结果并没有影响，反而会花费多余的时间，所以我们没有必要打乱测试集的数据。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">bs</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
<span class="mi">12345</span>
</code></pre></div></div>

<p>在每训练完一轮数据（epoch）后我们输出测试得到的损失值。
(注：如下代码中，我们调用<code class="highlighter-rouge">model.train()</code>和<code class="highlighter-rouge">model.eval</code>表示进入训练模式与测试模式，以保证模型运行的准确性)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">,</span><span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoches</span><span class="p">):</span>
	<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
	<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
		<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
		
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

	<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
		<span class="n">valid_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">)</span>

	<span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">valid_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">))</span>
<span class="mi">1234567891011121314151617</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 tensor(0.3456)
1 tensor(0.2988)
12
</code></pre></div></div>

<h1 id="创建-fit-和-get_data-优化代码">创建 fit() 和 get_data() 优化代码</h1>

<p>我们再继续做一点改进。因为我们再计算训练损失和验证损失时执行了两次相同的操作，所以我们用一个计算每一个batch损失的函数封装这部分代码。</p>

<p>我们为训练集添加优化器，并执行反向传播。对于训练集我们不添加优化器，当然也不会执行反向传播。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span> <span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
	<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">)</span>

	<span class="k">if</span> <span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
	
	<span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="mi">123456789</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">fit</code>执行每一个epoch过程中训练和验证的必要操作</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
		<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
			<span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
		
		<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
			<span class="n">losses</span><span class="p">,</span> <span class="n">nums</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
				<span class="o">*</span><span class="p">[</span><span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">]</span>
			<span class="p">)</span>
		<span class="n">val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span><span class="o">.</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">nums</span><span class="p">))</span>

		<span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">)</span>
<span class="mi">12345678910111213141516</span>
</code></pre></div></div>

<p>现在，获取数据加载模型进行训练的整个过程只需要三行代码便能实现了</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="n">fit</span><span class="p">(</span><span class="n">epoches</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">123</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.2961075816631317
1 0.28558296990394594
12
</code></pre></div></div>

<p>我们可以用这简单的三行代码训练各种模型。下面让我们看看怎么用它训练一个卷积神经网络。</p>

<h1 id="使用卷积神经网络">使用卷积神经网络</h1>

<p>现在我们用三个卷积层来构造我们的卷积网络。因为之前的实现的函数都没有假定模型形式，这儿我们依然可以使用它们而不需要任何修改。</p>

<p>我们pytorch预定义的<code class="highlighter-rouge">Conv2d</code>类来构建我们的卷积层。我们模型有三层，每一层卷积之后都跟一个 ReLU，然后跟一个平均池化层。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
		<span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="mi">12345678910111213141516</span>
</code></pre></div></div>

<p>动量momentum是随机梯度下降的一个参数，它考虑到了之前的梯度值使得训练更快。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_CNN</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">1234</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.3829730714321136
1 0.2258522843360901
12
</code></pre></div></div>

<h1 id="使用-nnsequential-搭建网络">使用 nn.Sequential 搭建网络</h1>

<p><code class="highlighter-rouge">torch.nn</code>还有另外一个方便的类可以简化我们的代码：<code class="highlighter-rouge">Sequential</code>, 一个<code class="highlighter-rouge">Sequential</code>对象</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Lambda</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="mi">12345678910</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">Sequential</code>是一种简化代码的好方法。 一个<code class="highlighter-rouge">Sequential</code>对象按顺序执行包含在内的每一个module，使用它可以很方便地建立一个网络。</p>

<p>为了更好地使用<code class="highlighter-rouge">Sequential</code>模块，我们需要自定义 pytorch中没实现地module。例如pytorch中没有自带 改变张量形状地层，我们创建 <code class="highlighter-rouge">Lambda</code>层，以便在<code class="highlighter-rouge">Sequential</code>中调用。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="n">preprocess</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">123456789101112131415</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.32739396529197695
1 0.25574398956298827
12
</code></pre></div></div>

<h1 id="简易的dataloader">简易的DataLoader</h1>

<p>我们的网络以及足够精简了，但是只能适用于MNIST数据集，因为</p>

<ul>
  <li>网络默认输入为 28x28 的张量</li>
  <li>网络默认最后一个卷积层大小为 4x4 （因为我们的池化层大小为4x4）</li>
</ul>

<p>现在我们去除这两个假设，使得网络可以适用于所有的二维图像。首先我们移除最初的 <code class="highlighter-rouge">Lambda</code>层，用数据预处理层替代。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">WrappedDataLoader</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">dl</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

	<span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">batches</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
			<span class="k">yield</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="p">))</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="mi">12345678910111213141516171819</span>
</code></pre></div></div>

<p>然后，我们使用<code class="highlighter-rouge">nn.AdaptiveAvgPool2d</code>代替<code class="highlighter-rouge">nn.AvgPool2d</code>。它允许我们自定义输出张量的维度，而于输入的张量无关。这样我们的网络便可以适用于各种size的网络。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernal_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
	<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
	<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="mi">123456789101112</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.32888883714675904
1 0.31000419993400574
12
</code></pre></div></div>

<h1 id="使用gpu">使用GPU</h1>

<p>如果你的电脑有支持CUDA的GPU（你可以很方便地以 0.5美元/小时 的价格租到支持的云服务器），便可以使用GPU加速训练过程。首先检测设备是否正常支持GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ture
1
</code></pre></div></div>

<p>接着创建一个设备对象：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
	<span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>更新 <code class="highlighter-rouge">preprocess(x,y)</code>把数据移到GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="mi">123456</span>
</code></pre></div></div>

<p>最后移动网络模型到GPU：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="mi">12</span>
</code></pre></div></div>

<p>进行训练，能发现速度快了很多：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="mi">1</span>
</code></pre></div></div>

<p>out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 0.21190375366210937
1 0.18018000435829162
12
</code></pre></div></div>

<h1 id="总结">总结</h1>

<p>我们现在得到了一个通用的数据加载和模型训练方法，我们可以在pytorch种用这种方法训练大多的模型。想知道训练一个模型有多简单，回顾一下本次的代码便可以了。</p>

<p>当然，除此之外本篇内容还有很多需求没有讲到，比如数据增强，超参调试，数据监控(monitoring training),迁移学习等。这些特点都以与本篇教程相似的设计方法包含于 fastai库中。</p>

<p>本篇教程开头我们承诺将会通过例程解释 <code class="highlighter-rouge">torch.nn</code> <code class="highlighter-rouge">torch.optim</code> <code class="highlighter-rouge">Dataset</code> <code class="highlighter-rouge">DataLoader</code>等模块，下面我们就这些模型进行总结。</p>

<ul>
  <li>torch.nn
    <ul>
      <li>Module: 创建一个可以像函数一样调用地对象，包含了网络的各种状态，可以使用<code class="highlighter-rouge">parameter</code>方便地获取模型地参数，并有清零梯度，循环更新参数等功能。</li>
      <li>Parameter: 将模型中需要更新的参数全部打包，方便反向传播过程中进行更新。有 <code class="highlighter-rouge">requires_grad</code>属性的参数才会被更新。</li>
      <li>functional：通常导入为<code class="highlighter-rouge">F</code>，包含了许多激活函数，损失函数等。</li>
    </ul>
  </li>
  <li>torch.optim: 包含了很多诸如<code class="highlighter-rouge">SGD</code>一样的优化器，用来在反向传播中跟新参数</li>
  <li>Dataset: 一个带有 <code class="highlighter-rouge">__len__</code> <code class="highlighter-rouge">__getitem__</code>等函数的抽象接口。里面包含了 <code class="highlighter-rouge">TensorDataset</code>等类。</li>
  <li>DataLoader: 输入任意的 <code class="highlighter-rouge">Dataset</code> 并按批(batch)迭代输出数据。</li>
</ul>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/">爬虫环境安装</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-11
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#spider" title="Tag: spider" rel="tag">spider</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h4 id="spider-note">spider Note</h4>

<h5 id="python环境安装">python环境安装</h5>

<ul>
  <li>python3.7</li>
</ul>

<h5 id="请求库的安装">请求库的安装</h5>

<ul>
  <li>
    <p>请求库requests的安装：<code class="highlighter-rouge">conda install requests -n spider</code></p>
  </li>
  <li>
    <p>自动化测试工具Senlenium的安装：<code class="highlighter-rouge">conda install selenium -n spider</code></p>
  </li>
  <li>
    <p>senlenium驱动chrome浏览器工具chromedriver添加到PATH中：</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">在</span><span class="o">~/.</span><span class="n">bashrc</span><span class="err">最后添加：</span><span class="n">PATH</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">henry</span><span class="o">/</span><span class="n">opt</span><span class="p">:</span><span class="err">$</span><span class="n">PATH</span>
<span class="n">source</span> <span class="o">~/.</span><span class="n">bashrc</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>使用phantomJS实现后台的浏览器控制</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">下载</span><span class="n">phantomJS</span><span class="err">，并且将其路径添加到系统变量中</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装异步web请求库aiohttp</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">aiohttp</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="解析库的安装">解析库的安装</h5>

<ul>
  <li>
    <p>lxml的安装，支持HTML、XML的解析，支持XPath解析方式</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">lxml</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>beautifulsoup4的安装，其依赖lxml</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">beautifulsoup4</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>pyquery的安装</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">pyquery</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装验证码识别工具tesseract</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sudo</span> <span class="n">apt</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="n">tesseract</span><span class="o">-</span><span class="n">ocr</span> <span class="n">libtesseract</span><span class="o">-</span><span class="n">dev</span> <span class="n">libleptonica</span><span class="o">-</span><span class="n">dev</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="n">spider</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tesserocr</span> <span class="n">pillow</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="数据库的安装">数据库的安装</h5>

<ul>
  <li>
    <p>mysql的安装</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sudo</span> <span class="n">apt</span> <span class="n">update</span>
<span class="n">sudo</span> <span class="n">apt</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="n">mysql</span><span class="o">-</span><span class="n">server</span> <span class="n">mysql</span><span class="o">-</span><span class="n">client</span>
<span class="c1">#mysql启动、关闭、重启命令
#sudo service mysql start
#sudo service mysql stop
#sudo service mysql restart
</span></code></pre></div>    </div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mysql5</span><span class="mf">.7</span><span class="err">以上不会有密码设置过程，需要手动配置</span>
<span class="err">$</span> <span class="n">mysql</span> <span class="o">-</span><span class="n">u</span> <span class="n">debian</span><span class="o">-</span><span class="n">sys</span><span class="o">-</span><span class="n">maint</span> <span class="o">-</span><span class="n">p</span>
<span class="c1">#密码在/etc/mysql/debian.cnf文件中可查看(V3XKquzHqnW18GWc)，在mysql命令行中执行下列语句
</span>  
<span class="n">show</span> <span class="n">databases</span><span class="err">；</span>
<span class="n">use</span> <span class="n">mysql</span><span class="p">;</span>
<span class="n">update</span> <span class="n">user</span> <span class="nb">set</span> <span class="n">authentication_string</span><span class="o">=</span><span class="n">PASSWORD</span><span class="p">(</span><span class="s">"zhou"</span><span class="p">)</span> <span class="n">where</span> <span class="n">user</span><span class="o">=</span><span class="s">'root'</span><span class="p">;</span>
<span class="n">update</span> <span class="n">user</span> <span class="nb">set</span> <span class="n">plugin</span><span class="o">=</span><span class="s">"mysql_native_password"</span><span class="p">;</span>
<span class="n">flush</span> <span class="n">privileges</span><span class="p">;</span>
<span class="n">quit</span><span class="p">;</span>
</code></pre></div>    </div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#完全卸载mysql的方法
</span><span class="n">sudo</span> <span class="n">apt</span> <span class="n">purge</span> <span class="n">mysql</span><span class="o">-*</span>
<span class="n">sudo</span> <span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">mysql</span><span class="o">/</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">mysql</span>
<span class="n">sudo</span> <span class="n">apt</span> <span class="n">autoremove</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装redis</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sudo</span> <span class="n">apt</span> <span class="n">install</span> <span class="n">redis</span><span class="o">-</span><span class="n">server</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="存储库的安装">存储库的安装</h5>

<ul>
  <li>
    <p>安装pymysql存储库以使用python和mysql交互</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">pymysql</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装redis_py</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">redis</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="web库的安装">Web库的安装</h5>

<ul>
  <li>
    <p>安装Flask web库来搭建一些API接口，供爬虫使用，后面会利用Flask+Redis维护动态代理池和Cookies池</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">tornado</span> <span class="o">-</span><span class="n">n</span> <span class="n">spider</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>安装tornado，后面会使用tornado+redis来搭建一个ADSL拨号代理池</p>
  </li>
</ul>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/11/markdown_Guide/">Markdown_Guide</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-11
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#markdown" title="Tag: markdown" rel="tag">markdown</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h1 id="欢迎使用markdown编辑器写博客">欢迎使用Markdown编辑器写博客</h1>

<p>本Markdown编辑器使用<a href="https://github.com/benweet/stackedit">StackEdit</a>修改而来，用它写博客，将会带来全新的体验哦：</p>

<ul>
  <li><strong>Markdown和扩展Markdown简洁的语法</strong></li>
  <li><strong>代码块高亮</strong></li>
  <li><strong>图片链接和图片上传</strong></li>
  <li><strong><em>LaTex</em>数学公式</strong></li>
  <li><strong>UML序列图和流程图</strong></li>
  <li><strong>离线写博客</strong></li>
  <li><strong>导入导出Markdown文件</strong></li>
  <li><strong>丰富的快捷键</strong></li>
</ul>

<hr />

<h2 id="快捷键">快捷键</h2>

<ul>
  <li>加粗    <code class="highlighter-rouge">Ctrl + B</code></li>
  <li>斜体    <code class="highlighter-rouge">Ctrl + I</code></li>
  <li>引用    <code class="highlighter-rouge">Ctrl + Q</code></li>
  <li>插入链接    <code class="highlighter-rouge">Ctrl + L</code></li>
  <li>插入代码    <code class="highlighter-rouge">Ctrl + K</code></li>
  <li>插入图片    <code class="highlighter-rouge">Ctrl + G</code></li>
  <li>提升标题    <code class="highlighter-rouge">Ctrl + H</code></li>
  <li>有序列表    <code class="highlighter-rouge">Ctrl + O</code></li>
  <li>无序列表    <code class="highlighter-rouge">Ctrl + U</code></li>
  <li>横线    <code class="highlighter-rouge">Ctrl + R</code></li>
  <li>撤销    <code class="highlighter-rouge">Ctrl + Z</code></li>
  <li>重做    <code class="highlighter-rouge">Ctrl + Y</code></li>
</ul>

<h2 id="markdown及扩展">Markdown及扩展</h2>

<blockquote>
  <p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。    —— <a href="https://zh.wikipedia.org/wiki/Markdown" target="_blank"> [ 维基百科 ]</a></p>
</blockquote>

<p>使用简单的符号标识不同的标题，将某些文字标记为<strong>粗体</strong>或者<em>斜体</em>，创建一个<a href="http://www.csdn.net">链接</a>等，详细语法参考帮助？。</p>

<p>本编辑器支持 <strong>Markdown Extra</strong> , 　扩展了很多好用的功能。具体请参考<a href="https://github.com/jmcmanus/pagedown-extra" title="Pagedown Extra">Github</a>.</p>

<h3 id="表格">表格</h3>

<p><strong>Markdown　Extra</strong>　表格语法：</p>

<table>
  <thead>
    <tr>
      <th>项目</th>
      <th>价格</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Computer</td>
      <td>$1600</td>
    </tr>
    <tr>
      <td>Phone</td>
      <td>$12</td>
    </tr>
    <tr>
      <td>Pipe</td>
      <td>$1</td>
    </tr>
  </tbody>
</table>

<p>可以使用冒号来定义对齐方式：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">项目</th>
      <th style="text-align: right">价格</th>
      <th style="text-align: center">数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Computer</td>
      <td style="text-align: right">1600 元</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: left">Phone</td>
      <td style="text-align: right">12 元</td>
      <td style="text-align: center">12</td>
    </tr>
    <tr>
      <td style="text-align: left">Pipe</td>
      <td style="text-align: right">1 元</td>
      <td style="text-align: center">234</td>
    </tr>
  </tbody>
</table>

<p>###定义列表</p>

<dl>
  <dt><strong>Markdown　Extra</strong>　定义列表语法：</dt>
  <dt>项目１</dt>
  <dt>项目２</dt>
  <dd>定义 A</dd>
  <dd>定义 B</dd>
  <dt>项目３</dt>
  <dd>定义 C</dd>
  <dd>
    <p>定义 D</p>

    <blockquote>
      <p>定义D内容</p>
    </blockquote>
  </dd>
</dl>

<h3 id="代码块">代码块</h3>
<p>代码块语法遵循标准markdown代码，例如：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">requires_authorization</span>
<span class="k">def</span> <span class="nf">somefunc</span><span class="p">(</span><span class="n">param1</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">param2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">'''A docstring'''</span>
    <span class="k">if</span> <span class="n">param1</span> <span class="o">&gt;</span> <span class="n">param2</span><span class="p">:</span> <span class="c1"># interesting
</span>        <span class="k">print</span> <span class="s">'Greater'</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">param2</span> <span class="o">-</span> <span class="n">param1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">None</span>
<span class="k">class</span> <span class="nc">SomeClass</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">message</span> <span class="o">=</span> <span class="s">'''interpreter
... prompt'''</span>
</code></pre></div></div>

<p>###脚注
生成一个脚注[^footnote].
[^footnote]: 这里是 <strong>脚注</strong> 的 <em>内容</em>.</p>

<h3 id="目录">目录</h3>
<p>用 <code class="highlighter-rouge">[TOC]</code>来生成目录：</p>

<p>[TOC]</p>

<h3 id="数学公式">数学公式</h3>
<p>使用MathJax渲染<em>LaTex</em> 数学公式，详见<a href="http://math.stackexchange.com/">math.stackexchange.com</a>.</p>

<ul>
  <li>行内公式，数学公式为：$\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$。</li>
  <li>块级公式：</li>
</ul>

<script type="math/tex; mode=display">x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a}</script>

<p>更多LaTex语法请参考 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">这儿</a>.</p>

<h3 id="uml-图">UML 图:</h3>

<p>可以渲染序列图：</p>

<pre><code class="language-sequence">张三-&gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&gt;张三: 忙得吐血，哪有时间写。
</code></pre>

<p>或者流程图：</p>

<pre><code class="language-flow">st=&gt;start: 开始
e=&gt;end: 结束
op=&gt;operation: 我的操作
cond=&gt;condition: 确认？

st-&gt;op-&gt;cond
cond(yes)-&gt;e
cond(no)-&gt;op
</code></pre>

<ul>
  <li>关于 <strong>序列图</strong> 语法，参考 <a href="http://bramp.github.io/js-sequence-diagrams/">这儿</a>,</li>
  <li>关于 <strong>流程图</strong> 语法，参考 <a href="http://adrai.github.io/flowchart.js/">这儿</a>.</li>
</ul>

<h2 id="离线写博客">离线写博客</h2>

<p>即使用户在没有网络的情况下，也可以通过本编辑器离线写博客（直接在曾经使用过的浏览器中输入<a href="http://write.blog.csdn.net/mdeditor">write.blog.csdn.net/mdeditor</a>即可。<strong>Markdown编辑器</strong>使用浏览器离线存储将内容保存在本地。</p>

<p>用户写博客的过程中，内容实时保存在浏览器缓存中，在用户关闭浏览器或者其它异常情况下，内容不会丢失。用户再次打开浏览器时，会显示上次用户正在编辑的没有发表的内容。</p>

<p>博客发表后，本地缓存将被删除。　</p>

<p>用户可以选择 <i class="icon-disk"></i> 把正在写的博客保存到服务器草稿箱，即使换浏览器或者清除缓存，内容也不会丢失。</p>

<blockquote>
  <p><strong>注意：</strong>虽然浏览器存储大部分时候都比较可靠，但为了您的数据安全，在联网后，<strong>请务必及时发表或者保存到服务器草稿箱</strong>。</p>
</blockquote>

<h2 id="浏览器兼容">浏览器兼容</h2>

<ol>
  <li>目前，本编辑器对Chrome浏览器支持最为完整。建议大家使用较新版本的Chrome。</li>
  <li>IE９以下不支持</li>
  <li>IE９，１０，１１存在以下问题
    <ol>
      <li>不支持离线功能</li>
      <li>IE9不支持文件导入导出</li>
      <li>IE10不支持拖拽文件导入</li>
    </ol>
  </li>
</ol>

<hr />


                </div>
                <div class="read-all">
                    <a  href="/2019/03/11/markdown_Guide/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/11/Pytorch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Pytorch环境搭建</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-11
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#pytorch" title="Tag: pytorch" rel="tag">pytorch</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p><strong>Pytorch 环境搭建</strong></p>

<p>PyTorch 的安装十分简单，根据 PyTorch 官网，对系统选择和安装方式等灵活选择即可。 这里以 anaconda 为例，简单的说一下步骤和要点。 国内安装 anaconda 建议使用清华或者中科大 [http://mirrors.ustc.edu.cn/help/anaconda.html] 镜像，快的不是一点半点。</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/11/Pytorch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/">GPT2.0笔记以及对NLP领域趋势的思考</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-11
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#spider" title="Tag: spider" rel="tag">spider</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h5 id="gpt10">GPT1.0</h5>

<p>简述如下：GPT 1.0采取预训练+FineTuning两个阶段，它采取Transformer作为特征抽取器。预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。</p>

<p>从大框架上来说，Bert基本就是GPT 1.0的结构，除了预训练阶段采取的是“双向语言模型”之外，它们并没什么本质差异，其它的技术差异都是细枝末节，不影响大局，基本可忽略。</p>

<h5 id="gpt20">GPT2.0</h5>

<p>GPT2.0大框架其实还是GPT 1.0的框架，但是把第二阶段的Finetuning做有监督地下游NLP任务，换成了无监督地做下游任务。本质上，GPT2.0选择了这么一条路来强化Bert或者是强化GPT 1.0的第一个预训练阶段：就是说首先把Transformer模型参数扩容，常规的Transformer Big包含24个叠加的Block，GPT2.0将Transformer层数增加到48层，参数规模15亿。真正的目的是：GPT 2.0准备用更多的训练数据来做预训练，更大的模型，更多的参数，意味着更高的模型容量，所以先扩容，免得Transformer楼层不够多的房间（模型容量）容纳不下过多的住户（就是NLP知识）。</p>

<p>GPT2.0的语料：GPT2.0找了800万互联网网页作为语言模型的训练数据，它们被称为WebText，互联网网页的优点是覆盖的主题范围非常广，这样训练出来的语言模型，通用性好，覆盖几乎任何领域的内容，这意味着它可以用于任意领域的下游任务，有点像图像领域的Imagenet的意思。GPT 2.0论文其实更强调训练数据的通用性强这点。当然，除了量大通用性强外，数据质量也很重要，高质量的数据必然包含更好的语言及人类知识，所以GPT 2.0还做了数据质量筛选，过滤出高质量的网页内容来。</p>

<p>GPT 2.0用这些网页做“单向语言模型”，GPT 2.0没有像Bert或者1.0版本一样，拿这个第一阶段的预训练模型有监督地去做第二阶段的Finetuning任务，而是选择了无监督地去做下游任务。另外论文中提到的对Transformer结构的微调，以及BPE输入方式，我相信都是不太关键的改动，应该不影响大局。</p>

<p><strong>问题一：为什么GPT 2.0第二阶段不通过Finetuning去有监督地做下游任务呢？</strong>无监督地去做很多第二阶段的任务，只是GPT作者想说明在第一阶段Transformer学到了很多通用的包含各个领域的知识，第二部分各种实验是对这点的例证，如此而已。这是为何说第二阶段其实不重要，因为它不是论文的中心思想，而是说明中心思想的例子。</p>

<p><strong>问题二：在预训练阶段，为什么GPT 2.0仍然固执地用单向语言模型，而不是双向语言模型呢？</strong>Bert在论文的实验部分已经证明了：Bert的效果比GPT好主要归因于这个双向语言模型。也许GPT 作者只想强调他们想做语言模型这个事情，毕竟生成内容后续单词这种模式，单向语言模型更方便，这估计是真正原因。</p>

<p><strong>问题三：GPT 2.0 既然第二阶段是无监督的任务，而它不做Finetuning，那么你训练好一个语言模型，它当然会根据输入的一句话，给你蹦出后面可能紧跟那个单词，这是标准的语言模型过程，这个正常。但是如果这时候让它去做一个文本摘要任务，它怎么知道它现在在做什么事情呢，根据输入，应该输出什么东西呢？</strong>其实GPT 2.0在做下游无监督任务的时候，给定输入（对于不同类型的输入，加入一些引导字符，引导GPT正确地预测目标，比如如果做摘要，在输入时候加入“TL：DR”引导字符串），它的输出跟语言模型的输出是一样的，就是蹦出一个单词。那么问题来了：对于比如摘要任务，我们期待的输出结果是一句话或者几句话，你给我一个单词，有点太小气，那该怎么办？很简单，继续一个字一个字往出蹦，按照这些字从系统里蹦出来的时间顺序连起来，就是你想要的摘要结果，这种所有任务采取相同的往出蹦字的输出模式也是有点意思的。就是说，GPT2.0给出了一种新颖的生成式任务的做法，就是一个字一个字往出蹦，然后拼接出输出内容作为翻译结果或者摘要结果。传统的NLP网络的输出模式一般需要有个序列的产生结构的，而GPT 2.0完全是语言模型的产生结果方式：一个字一个字往出蹦，没有输出的序列结构。</p>

<h5 id="归纳">归纳</h5>

<p>我们可以从两个不同的角度来理解GPT 2.0。</p>

<p><strong>一个角度是把它看作采取类似Elmo/GPT/Bert的两阶段模型解决NLP任务的一种后续改进策略</strong>，这种策略可以用来持续优化第一阶段的预训练过程。通过现在的Transformer架构，采用更高质量的数据，采用更宽泛的数据（Web数据量大了估计包含任何你能想到的领域），采用更大量的数据（WebText，800万网页），Transformer采用更复杂的模型（最大的GPT2.0模型是Transformer的两倍层深），那么在Transformer里能学会更多更好的NLP的通用知识。如果我们第二阶段仍然采取Finetuning，对下游任务的提升效果是可以很乐观地期待的。</p>

<p><strong>另外一个角度也可以把GPT 2.0看成一个效果特别好的语言模型</strong>，可以用它来做语言生成类任务，比如摘要，QA这种，再比如给个故事的开头，让它给你写完后面的情节，目前看它的效果出奇的好。</p>

<h5 id="bert的另外一种改进模式">Bert的另外一种改进模式</h5>

<p>GPT2.0给出的思路是优化Bert的第一个预训练阶段，方向是扩充数据数量，提升数据质量，增强通用性，追求的是通过做大来做强。</p>

<p>另一个思路：机器学习里面还有有监督学习，NLP任务里也有不少有监督任务是有训练数据的，这些数据能用来改善Bert第二阶段学习各种知识的Transformer。这种做法一个典型的模型是最近微软推出的MT-DNN，核心思想：结构上底层就是标准的Bert Transformer，第一阶段采用Bert的预训练模型不动，在Finetuning阶段，在上层针对不同任务构造不同优化目标，所有不同上层任务共享底层Transformer参数，这样就强迫Transformer通过预训练做很多NLP任务，来学会新的知识，并编码到Transformer的参数中。</p>

<h5 id="nlp主流模型进化">NLP主流模型进化</h5>

<ul>
  <li>采取Bert的两阶段模式</li>
  <li>特征抽取器采用Transformer</li>
  <li>Bert两阶段模式中，第一个预训练阶段的两种改进方向：
    <ul>
      <li><strong>一种是强调通用性好以及规模大</strong>。加入越来越多高质量的各种类型的无监督数据，GPT 2.0指出了个明路，就是净化的高质量网页</li>
      <li><strong>第二种是通过多任务训练</strong>，加入各种新型的NLP任务数据，它的好处是有监督，能够有针对性的把任务相关的知识编码到网络参数里，所以明显的好处是学习目标明确，学习效率高；而对应的缺点是NLP的具体有监督任务，往往训练数据量少，于是包含的知识点少；而且有点偏科，学到的知识通用性不强。</li>
    </ul>
  </li>
  <li>GPT2.0<strong>采取超深层Transformer+更大量的网页数据去做更好的语言模型</strong>，并进而做各种生成式任务是很有研究应用前景。</li>
</ul>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文《MT-DNN》笔记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-10
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#paper" title="Tag: paper" rel="tag">paper</a>&nbsp;
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h2 id="论文multi-task-deep-neural-networks-for-natural-language-understanding笔记">论文《Multi-Task Deep Neural Networks for Natural Language Understanding》笔记</h2>

<blockquote>
  <p>论文地址：<a href="https://arxiv.org/pdf/1901.11504.pdf">微软MT-DNN论文《Multi-Task Deep Neural Networks for Natural Language Understanding》</a></p>
</blockquote>

<h4 id="mt-dnn简单介绍">MT-DNN简单介绍</h4>

<p>​	谷歌的<a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>在各个NLP任务（GLUE、SQuAD、命名实体识别、SWAG)上的表现都很好，但是BERT在词向量的预训练的第二阶段只使用了单个任务进行模型fine-tune，我们自然而然地会问：Fine-tune阶段使用多任务同时对网络参数进行微调效果会不会更好？。</p>

<p>​	微软研究院在2019年发布的论文《Multi-Task Deep Neural Networks for Natural Language Understanding》就做了这方面的实验。论文提出了一个假设：在单一领域的数据集上使用单一的任务训练模型限制了模型的泛化。MT-DNN提供的思路是：利用多任务之间的约束来避免单一任务上的过拟合问题，从而提高模型的泛化能力。文章中使用的多任务是相似的，作者任务机器能够像人一样在相似的任务中获取到相关的经验，比如会滑雪的人就能比较容易的学会滑冰，对机器来说也就是能够使用更少的训练数据是模型获得相同的效果。</p>

<h4 id="实验结果">实验结果</h4>

<p>​	<strong>(1)</strong>MT-DNN在8/9的GLUE<a href="Gerneral Lanuage Understanding Evaluation，是评估模型自然语言理解能力的最权威的指标">1</a>任务中取得了SOAT成绩，其中未达到SOAT成绩的原因是数据集存在问题。这８个数据集（任务）可以归纳分为以下四种类别：</p>

<table>
  <thead>
    <tr>
      <th>任务</th>
      <th>数据集</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Single sentence classification</td>
      <td>CoLA：情感分类<br />SST-2：判断句子是否符合语法要求</td>
    </tr>
    <tr>
      <td>Text similarity score</td>
      <td>STS-B：两句话的相似性</td>
    </tr>
    <tr>
      <td>Pairwise Text classification</td>
      <td>RET、MNLI：判断两句话的关系(emtaiment, controdictional, neutral)<br />QQP, MRPC：判断那两句话是否具有相同的语义</td>
    </tr>
    <tr>
      <td>Relevence ranking</td>
      <td>QNLI：判断问答句子对的相关性</td>
    </tr>
  </tbody>
</table>

<p>​	<strong>(2)</strong>通过这种多任务训练得到的模型能够很好的适用于其他未见过的相似任务，即使只有很少的带标注的数据。因为MT-DNN底层使用的是BERT(Base)的网络，所以这种相似任务之间的适用性的提高可以确定由多任务的fine-tune带来的。实验表明即使只使用原始数据集的0.1%、1%样本，同样能够获得不错的准确率。下面是MT-DNN模型和BERT两个模型在SNLI数据集上的表现：</p>

<table>
  <thead>
    <tr>
      <th>模型</th>
      <th>0.1%</th>
      <th>1％</th>
      <th>10%</th>
      <th>100%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BERT</td>
      <td>51%</td>
      <td>82%</td>
      <td>90%</td>
      <td>94%</td>
    </tr>
    <tr>
      <td>MT-DNN</td>
      <td>82%</td>
      <td>88%</td>
      <td>91%</td>
      <td>96%</td>
    </tr>
  </tbody>
</table>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <span class="previous disable"><i class="fa fa-angle-double-left"></i></span>
            <span class="previous disable"><i class="fa fa-angle-left"></i></span>
          
          <span class="page_number ">1/3</span>
          
            <a href="/page2" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page3" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/03/12/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90torch.nn/">深入解析torch.nn</a></li>
                    
                        <li><a href="/2019/03/11/spider%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/">爬虫环境安装</a></li>
                    
                        <li><a href="/2019/03/11/markdown_Guide/">Markdown_Guide</a></li>
                    
                        <li><a href="/2019/03/11/Pytorch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Pytorch环境搭建</a></li>
                    
                        <li><a href="/2019/03/11/GPT2.0%E5%8F%8A%E5%AF%B9NLP%E9%A2%86%E5%9F%9F%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E7%9A%84%E6%80%9D%E8%80%83/">GPT2.0笔记以及对NLP领域趋势的思考</a></li>
                    
                        <li><a href="/2019/03/10/%E8%AE%BA%E6%96%87MT-DNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文《MT-DNN》笔记</a></li>
                    
                        <li><a href="/2019/03/06/%E4%BA%AC%E4%B8%9C-%E4%BD%95%E6%99%93%E4%B8%9C-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E5%89%8D%E8%A8%80%E6%8A%80%E6%9C%AF/">自然语言与多模态交互前沿技术</a></li>
                    
                        <li><a href="/2019/03/06/%E6%9C%80%E8%BF%91%E9%98%85%E8%AF%BB%E6%96%87%E7%AB%A0%E5%B0%8F%E6%8A%84/">最近阅读文章小抄</a></li>
                    
                        <li><a href="/2019/02/28/Transformer%E7%9A%84%E5%9C%A8NLP%E4%B8%AD%E5%BA%94%E7%94%A8/">Transformer在NLP词向量预训练中的应用</a></li>
                    
                        <li><a href="/2018/07/16/%E6%B5%81%E7%95%85%E7%9A%84python%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">流畅的python笔记</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">15</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#reproduction" class="categories-list-item" cate="reproduction">
                            <span class="name">
                                reproduction
                            </span>
                            <span class="badge">1</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 15pt; color: #333;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 12pt; color: #666;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 12pt; color: #666;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 12pt; color: #666;">pytorch</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 9pt; color: #999;">markdown</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
