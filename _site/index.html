<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>henryzhou</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/">
    <link rel="alternate" type="application/rss+xml" title="henryzhou" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?9f8941ee9d9cbc5007bd89d1d30eb03f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>





</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">henryzhou</a>
        <small>Make robot converse with human naturally</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a class="active" href="/">
                        
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
    <script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
    	tex2jax: {
      	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      	inlineMath: [['$','$']]
    	}
  	});
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
</header>


        <div class="page clearfix" index>
    <div class="left">
        <h1>Welcome to Henry's Blog!</h1>
        <small>这里记录着我的NLP学习之路</small>
        <hr>
        <ul>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/04/04/Get_To_The_Point_Summarization_with_Pointer-Generator_Networks%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">Get To The Point: Summarization with Pointer-Generator Networks论文笔记</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-04-04
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#NLP" title="Category: NLP" rel="category">NLP</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#summarization" title="Tag: summarization" rel="tag">summarization</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>﻿### 论文：<a href="http://arxiv.org/abs/1704.04368">Get To The Point: Summarization with Pointer-Generator Networks</a>解读</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/04/04/Get_To_The_Point_Summarization_with_Pointer-Generator_Networks%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/28/The_Evolved_Transformer_%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">《The Evolved Transformer》论文总结</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-28
                    </div>
                    <div class="label-card">
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#NLP" title="Category: NLP" rel="category">NLP</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#Feature Extractor" title="Tag: Feature Extractor" rel="tag">Feature Extractor</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h3 id="google-brain-the-evolved-transformer">Google Brain-<a href="http://arxiv.org/abs/1901.11117">The Evolved Transformer</a></h3>

<h4 id="论文创新点">论文创新点：</h4>

<p>​	使用神经架构搜索的方法，为 seq2seq 任务找到了一种比Transformer更好的前馈网络架构。架构搜索是基于Transformer进行演进，最终得到的Evolved Transformer 的新架构在四个成熟的语言任务（WMT 2014 英德、WMT 2014 英法、WMT 2014 英捷及十亿词语言模型基准（LM1B））上的表现均优于原版 Transformer。在用大型模型进行的实验中，Evolved Transformer 的效率（FLOPS）是 Transformer 的两倍，而且质量没有损失。在更适合移动设备的小型模型（参数量为 7M）中，Evolved Transformer 的 BLEU 值高出 Transformer 0.7。</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/28/The_Evolved_Transformer_%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/">《Pretraining-Based Natural Language Generation for Text Summarization》论文解读</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-21
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#paper" title="Tag: paper" rel="tag">paper</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h3 id="pretraining-based-natural-language-generation-for-text-summarization">《Pretraining-Based Natural Language Generation for Text Summarization》</h3>

<p>国防科技大学 &amp; MSRA	2019-2-26</p>

<h4 id="问题定义">问题定义</h4>

<p>摘要是指： “一段从一份或多份文本中提取出来的文字，它包含了原文本中的重要信息，其长度不超过或远少于原文本的一半”。自动文本摘要旨在通过机器自动输出简洁、流畅、保留关键信息的摘要。</p>

<p>自动文本摘要通常可分为两类，分别是<strong>抽取式（extractive）</strong>和<strong>生成式（abstractive）</strong>。抽取式摘要判断原文本中重要的句子，抽取这些句子成为一篇摘要。而生成式方法则应用先进的自然语言处理的算法，通过转述、同义替换、句子缩写等技术，<strong>生成更凝练简洁的摘要</strong>。比起抽取式，生成式更接近人进行摘要的过程。历史上，抽取式的效果通常优于生成式。伴随深度神经网络的兴起和研究，基于神经网络的生成式文本摘要得到快速发展，并取得了不错的成绩。</p>

<h4 id="生成式文本摘要的问题">生成式文本摘要的问题</h4>

<p><strong>问题一</strong>：在监督式训练中，对一篇文本一般往往只提供一个参考摘要，基于 MLE 的监督式训练只鼓励模型生成一模一样的摘要，然而正如在介绍中提到的，对于一篇文本，往往可以有不同的摘要，因此监督式学习的要求太过绝对。</p>

<p>针对这个问题论文作者提出了两个解决问题：1.两阶段的summary生成方法；2.使用mixed objective优化误差。</p>


                </div>
                <div class="read-all">
                    <a  href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/20/%E6%96%87%E6%91%98/">文摘</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-20
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#digest" title="Category: digest" rel="category">digest</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#life" title="Tag: life" rel="tag">life</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <p>生命是一种长期而持续的累积过程，绝不会因为单一的事件而毁了一个人的一生，也不会因为单一的事件而救了一个人的一生。属于我们该得的，迟早会得到；属于我们不该得的，即使侥幸巧取也不可能长久保有。如果我们看清这个事实，许多所谓“人生的重大抉择”就可以淡然处之，根本无需焦虑。而所谓”人生的困境”，也往往当下就变得无足挂齿。</p>

<p>​																		——台湾清华彭明辉教授</p>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/20/%E6%96%87%E6%91%98/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/">chat-bot</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-16
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#notes" title="Category: notes" rel="category">notes</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <h5 id="聊天机器人">聊天机器人</h5>

<p>​	聊天机器人的应用场景十分丰富，包括用户服务或者在线自动回复。以往的聊天机器人是基于自动的检索系统，根据用户的输入自动选取最合适的回答。这种机制在需要多领域知识或领域知识快速迭代的应用场景就显得捉襟见肘。使用深度学习的方法，只要提供语料库，机器就能够从中学习到对话的能力。</p>

<ul>
  <li>项目重点：
    <ul>
      <li>对cornell Movie-Dialogs corpus语料库进行预处理，分割成成对的对话问答</li>
      <li>应用Luong attention mechanism构建seq2seq模型</li>
      <li>用mini-batch的数据训练encoder和decoder</li>
      <li>使用greedy-search方法生成回答</li>
      <li>与聊天机器人交流</li>
    </ul>
  </li>
</ul>

<ol>
  <li><strong>语料库主要使用两部分：movie_lines.txt、movie_conversations.txt</strong></li>
</ol>

<ul>
  <li>movie_lines包含一下信息：
    <ul>
      <li>LineID：唯一确定当前sentence在整个语料库中的位置</li>
      <li>userID：在某电影中角色的编号</li>
      <li>movieID：电影的编号</li>
      <li>user_name：电影中角色的名字</li>
      <li>text： 角色台词</li>
    </ul>
  </li>
  <li>movie_conversations包含一下信息：
    <ul>
      <li>character1ID：对话中角色1的编号</li>
      <li>character2ID：对话中角色2的编号</li>
      <li>movieID：电影的编号</li>
      <li>utterranceIDs：对话所在的行编号</li>
    </ul>
  </li>
</ul>

<ol>
  <li>
    <p><strong>解析文本</strong>：</p>

    <ol>
      <li>将语料库movie_lines.txt中每行解析并保存在字典中，key为<code class="highlighter-rouge">['lineID', 'characterID', 'movieID', 'characher', 'text']</code>；</li>
      <li>将语料库movie_lines.txt中每行解析并保存在字典中，key为<code class="highlighter-rouge">['character1ID', 'character2ID', 'movieID', 'utterranceIDs','lines']</code>，lines是根据uterranceIDs从第一步得到的字典中获取的台词。</li>
      <li>从第二步得到的conversations字典中生成问答对：pa_pairs，内容为<code class="highlighter-rouge">[inputLine, targetLine]</code>的列表。这里就得到了可以用于训练的原始问答对话qa_pairs。将qa_pairs写入datafile.csv文件，每行为一对问答。</li>
    </ol>
  </li>
  <li>
    <p><strong>创建一个Voc词汇类，功能包括：</strong></p>

    <ol>
      <li>往词汇表中添加词add_word()， 给词汇分配index并且计算词汇出现的次数</li>
      <li>添加句子中出现的所有词汇（使用add_word()的功能）</li>
      <li>去除不常出现的词汇（根据词频和词汇出现次数的阀值）trim()，只保留出现次数大于给定阀值的词汇，筛选结束后重新生成一次词汇表。</li>
    </ol>
  </li>
  <li>
    <p><strong>在将文本用于创建Voc词汇表之前，对文本进行编码转化、标准化和过滤</strong></p>

    <ol>
      <li>unicode2Ascii()，在需要比较字符串的程序中使用字符的多种表示会产生问题。 为了修正这个问题，你可以使用unicodedata模块先将文本标准化：<code class="highlighter-rouge">normalize()</code> 第一个参数指定字符串标准化的方式。 NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示。</li>
      <li>normalizeString()：在unicode2Ascii()基础上，将所有字符转化为小写（如果有的话），去除基本标点符号和字母以外的所有字符。</li>
      <li>filterPairs()：为了帮助模型收敛，剔除长度超过一定长度的句子</li>
      <li>使用经过上面三个预处理步骤的文本，初始化一个Voc词汇类，voc词汇类包含了word2index、word2count、index2word和num_words，为了进一步帮助模型收敛，使用voc的trim()函数剔除一些非常见的词汇，重新生成一次词汇表，并且剔除包含非常见词汇的句子，得到新的qa_pairs。</li>
    </ol>
  </li>
  <li>
    <p><strong>将文本pa_pair向量化，并通过pad方法成(max_length, batch_size)的形状</strong></p>

    <ol>
      <li>之所以要设置成（max_length, batch_size)的形状，是因为每一个batch中句子的长度不是对齐的，而训练模型网络需要固定输入输出以及隐藏层的形状。</li>
      <li>inputVar()：先将qa_pair中的question句子batch转化成indexes的batch，使用<code class="highlighter-rouge">itertools.zip(*l, fillvalue='PAD')</code>进行空白部分的填充。返回填充后的index_batch和batch中个句子的长度lengths</li>
      <li>outputVar：对qa_pair中的answer句子进行填充，过程和inputVar大致相同，不同点在于返回mask用于记录answer中非填充部分的位置，用于后面的损失函数的计算。</li>
      <li></li>
    </ol>
  </li>
  <li>
    <p><strong>聊天机器人的核心：seq2seq</strong></p>

    <ol>
      <li>seq2seq模型包含两个模块：encoder和decoder。encoder将不同长度的输入压缩成一个高维的上下文张量，这个上下文张量包含着从所有输入语句中学习到的语义信息。decoder可以根据模块的输入和隐藏状态（hidden-state）生成预测的输出。</li>
      <li>encoder使用了双向的GRU，从输入语句的双向学习隐藏的信息，在双向GRU层学习输入的表示之前，我们在输入和GRU层之间夹一层Embedding层。Embedding层可以将输入重新表示到一个新的空间中，好处时可以使得模型学习到一部分语义上的信息，比如语义相近的词可能被映射到相似的Embedding空间中。
        <ul>
          <li>经过第5步后，pa_pair变成了使用padding的方式进行长度对齐的文本，为了方便将训练数据输入到LSTM模型进行训练，同时为了保证模型训练的精度，应该同时告诉LSTM相关padding的情况，此时，pytorch中的pack_padded_sequence就有了用武之地，pack_padded_sequence按序列长度的长短排序，长的在前，短的在后。pad_packed_sequence的作用时将index_batch重新打包成包含0填充的序列。 <strong>当使用双向RNN的时候, 必须要使用 pack_padded_sequence !!</strong> .否则的话, pytorch 是无法获得 序列的长度, 这样也无法正确的计算双向 <code class="highlighter-rouge">RNN/GRU/LSTM</code>的结果.</li>
          <li>Encoder部分的计算图如下所示：
            <ul>
              <li>将词的index转化为词嵌入embedding</li>
              <li>对input进行pack压缩后feed给RNN模块</li>
              <li>向前传播GRU层（embedding层-&gt;GRU）</li>
              <li>对ouput重新填充0</li>
              <li>对双向GRU输出求和</li>
              <li>返回输出和隐藏状态</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Decoder通过隐藏状态和上下文张量对输入响应进而生成一个一个词的输出，直到生成EOS结束符。只使用seq2seq模型可能对于长句子，对整个句子做编码会出现信息丢失的问题。注意力机制使得decoder只关注输入句子的特定部分而不是整个句子。
        <ul>
          <li>注意力权重由decoder的hidden state和dcoder input共同计算得到。注意力权重和encoder的output乘积即可得到加持注意力后新的的encoder output</li>
          <li>global attention基于attention改进得到，我们的模型使用的就是这个Luong attention。与基本的attention的不同点在于：
            <ol>
              <li>global attention考虑了所有encoder的hidden state，而不是只关注当前step的encoder的hidden state</li>
              <li>global attention利用decoder当前step的hidden state计算attention weights，而不是前一step的hidden state</li>
              <li>global attention给出了计算attention energies或者得分函数的方法，包括dot、general、concat三种。$\overline{h}_s=all \  encoder states$， $h_t=current \ target \ decoder \ state$</li>
              <li>我们单独设置一个Attn子层，计算注意力权重</li>
            </ol>
          </li>
          <li>decoder层结构：embedding-&gt;dropout-&gt;GRU-&gt;Attn-&gt;concat(Linear)-&gt;out(Linear)-&gt;softmax
            <ol>
              <li>获得当前输入层词汇的embedding</li>
              <li>按照decoder层结构向前传播单向GRU</li>
              <li>基于第2步计算得到的GRU层的output计算attention weights</li>
              <li>将attention weigth应用到encoder的outout上得到新的encoder output</li>
              <li>合并当前GRU输出和应用attention weight后的新的encoder output得到concateed_output</li>
              <li>将concated_output通过两层全连接层，得到softmax概率output</li>
              <li>返回output和hidden</li>
            </ol>
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>定义训练步骤</strong></p>

    <ol>
      <li>遮掩损失：我们处理的是batch后的填充了0值的序列，但在计算损失的时候不能也将填充的部分也加入到损失的计算过程中。根据mask只计算非填充部分的交叉熵。</li>
      <li>定义单次训练的过程，这个过程我们使用了两个技巧来提高训练的效率。teacher forcing 也就是按照teacher_forcing_ratio设置的概率将本次训练的target代替decoder的输出（guess）作为decoder下一次的输入。gradient clipping可以将反向传播中计算得到的梯度限制到一个固定的区间内，可以解决梯度爆炸的问题。单次训练的计算图如下:
        <ol>
          <li>将整个batch在encoder中向前传播</li>
          <li>将decoder的input初始化为开始符SOS_token，将decoder的hidden state初始化为encoder最后的hidden state</li>
          <li>每次一步地向decoder传播batch sequence</li>
          <li>如果使用teacher forcing的话：将当前step的target设置为decoder的下一次输入，否则将当前step的output作为下一次decoder 的输入</li>
          <li>计算并且累计loss</li>
          <li>进行反向传播</li>
          <li>进行梯度截断</li>
          <li>更新encoder和decoder的模型参数，5-8四步就是为了进行梯度截断对优化器的工作过程的拆解。</li>
        </ol>
      </li>
      <li>定义n_iteration此迭代的训练过程。这个函数大部分的功能依赖于单次的训练。我们将模型保存为tar包，保存的参数包括encoder和decoder的state_dicts, optimizer的state_dicts，loss和当前的iteration。本函数的计算图如下：
        <ol>
          <li>从qa_pair中选取出batch_size大小的qa对组成batch，进而制作iteration大小的batches</li>
          <li>进行单次训练，每次训练完都打印当前iter的loss，每500次iter保存一次模型参数。</li>
          <li>加入loadFilename值有效则从文件中恢复模型，并从当前的iter开始训练。</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>定义输入的evaluate函数，对输入的句子生成回答</strong></p>

    <ol>
      <li>定义一个<strong>greedySearcher</strong>函数，根据输入的input_seq、lengths、max_lengths生成最后的回答序列，greedySearch的计算图如下：
        <ol>
          <li>将input_seq通过encoder模型</li>
          <li>将encoder的最后step的hideen layer作为decoder的第一个hidden input</li>
          <li>将decoder的首次输入初始化为SOS_token</li>
          <li>初始化一个tensor用来追加decoder得到的单词</li>
          <li>每次得到一个decoder的输出：
            <ul>
              <li>在decoder中向前传播一次</li>
              <li>获得可能性最高的下一个输出的token</li>
              <li>记录token和score</li>
              <li>将当前的token作为decoder的下一输入</li>
            </ul>
          </li>
          <li>返回decoder预测的单词token集合和scores</li>
        </ol>
      </li>
      <li>对于输入的句子我们可以将其看作是batch_size=1的测试数据，计算lengths，最后使用greedySearcher()生成回答。</li>
    </ol>
  </li>
  <li>
    <p><strong>整个训练过程整合</strong></p>

    <ol>
      <li>
        <p>超参数设置:</p>

        <table>
          <thead>
            <tr>
              <th>model_name</th>
              <th>attn_model</th>
              <th>hidden_size</th>
              <th>encoder_n_layers</th>
              <th>decoder_n_layers</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>cb_model</td>
              <td>dot</td>
              <td>500</td>
              <td>2</td>
              <td>2</td>
            </tr>
            <tr>
              <td>dropout</td>
              <td>batch_size</td>
              <td>checkpoint_iter</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>0.1</td>
              <td>64</td>
              <td>400</td>
              <td> </td>
              <td> </td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>初始化embedding层:nn.Embedding(voc.num_words, hidden_size)</p>
      </li>
      <li>
        <p>初始化encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)</p>
      </li>
      <li>
        <p>初始化decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, , voc.num_words, decoder_n_layers, dropout)</p>
      </li>
      <li>
        <p>训练模型</p>

        <ol>
          <li>设置训练的超参数</li>
        </ol>

        <table>
          <thead>
            <tr>
              <th>clip</th>
              <th>teacher_forcing_ratio</th>
              <th>decoder_learning_ratio</th>
              <th>n_iteration</th>
              <th>print_every</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>50</td>
              <td>1.0</td>
              <td>5.0</td>
              <td>4000</td>
              <td>1</td>
            </tr>
            <tr>
              <td>save_every</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>500</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
            </tr>
          </tbody>
        </table>

        <ol>
          <li>将encoder和decoder设置为训练模式</li>
          <li>设定encoder和decoder的优化器为adam</li>
          <li>调用trainIters函数进行训练</li>
        </ol>
      </li>
      <li>
        <p>预测</p>

        <ol>
          <li>将encoder和decoder设定为预测模型</li>
          <li>使用GreedySearchDecoder生成一个输入框，对输入进行预测</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
              <li>
                <h2>
                  <a class="post-link" href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/">自然语言处理的是个发展趋势</a>
                </h2>
                <div class="label">
                    <div class="label-card">
                        <i class="fa fa-calendar"></i>2019-03-13
                    </div>
                    <div class="label-card">
                        <i class="fa fa-user"></i>Henryzhou
                        
                    </div>
                    <div class="label-card">
                        
                    </div>

                    <div class="label-card">
                    


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#reproduction" title="Category: reproduction" rel="category">reproduction</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


                    </div>

                    <div class="label-card">
                    
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <a href="/tag/#nlp" title="Tag: nlp" rel="tag">nlp</a>
    
  

</span>

                    </div>
                </div>
                <div class="excerpt">
                    <blockquote>
  <p>转载自：https://blog.csdn.net/cf2suds8x8f0v/article/details/78588562</p>
</blockquote>

<p><em>概要：<a href="https://www.baidu.com/s?wd=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd">哈尔滨工业大学</a>刘挺教授在第三届中国人工智能大会上对自然语言处理的发展趋势做了一次精彩的归纳。</em></p>

<p>哈尔滨工业大学刘挺教授在第三届中国人工智能大会上对自然语言处理的发展趋势做了一次精彩的归纳。</p>

<p><strong>趋势 1：语义表示——从符号表示到分布表示</strong></p>

<p><img src="https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4YziaJDxJpJg67Pd7zXAYNn6e6YdsRMv8Dtc3o4A3QFtoNaRFp8gMCwQ/640.png?wxfrom=5&amp;wx_lazy=1" alt="" /></p>

<p>自然语言处理一直以来都是比较抽象的，都是直接用词汇和符号来表达概念。但是使用符号存在一个问题，比如两个词，它们的词性相近但词形不匹配，计算机内部就会认为它们是两个词。举个例子，荷兰和苏格兰这两个国家名，如果我们在一个语义的空间里，用词汇与词汇组合的方法，把它表示为连续、低维、稠密的向量的话，就可以计算不同层次的语言单元之间的相似度。这种方法同时也可以被神经网络直接使用，是这个领域的一个重要的变化。</p>

<p>从词汇间的组合，到短语、句子，一直到篇章，现在有很多人在做这个事，这和以前的思路是完全不一样的。</p>

<p>有了这种方法之后，再用深度学习，就带来了一个很大的转变。原来我们认为自然语言处理要分成几个层次，但是就句法分析来说，它是人为定义的层次，那它是不是一定必要的？这里应该打一个问号。</p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4aKz9C7DBSAn4Low4lPLlPkJ79MyMRVbgRVdfXv5YibaibG0b6Jvx73Gg/640.png?" alt="" /></p>

<p>实际工作中，我们面临着一个课题——信息抽取。我之前和一个单位合作，初衷是我做句法分析，然后他们在我的基础上做信息抽取，相互配合，后来他们发表了一篇论文，与初衷是相悖的，它证明了没有句法分析，也可以直接做端到端的直接的实体关系抽取，这很震撼，不是说现在句法分析没用了，而是我们认为句法分析是人为定义的层次，在端到端的数据量非常充分，可以直接进行信息抽取的时候，那么不用句法分析，也能达到类似的效果。当端到端的数据不充分时，才需要人为划分层次。</p>

<p><strong>趋势 2：学习模式——从浅层学习到深度学习</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4ycXUk346vPicxYGTVMDAL2xHbyjeBaUmY6a7v5rmRxsgvQayJuxORNQ/640.png?" alt="" /></p>

<p>浅层到深层的学习模式中，浅层是分步骤走，可能每一步都用了深度学习的方法，实际上各个步骤是串接起来的。直接的深度学习是一步到位的端到端，在这个过程中，我们确实可以看到一些人为贡献的知识，包括该分几层，每层的表示形式，一些规则等，但我们所谓的知识在深度学习里所占的比重确实减小了，主要体现在对深度学习网络结构的调整。</p>

<p><strong>趋势 3：NLP 平台化——从封闭走向开放</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4ibccaicSI5x2B4oJ9bjaAS6MYfSfqJVocr40priccbiaNlfxNjtibY1ZBLA/640.png?" alt="" /></p>

<p>以前我们搞研究的，都不是很愿意分享自己的成果，像程序或是数据，现在这些资料彻底开放了，无论是学校还是大企业，都更多地提供平台。NLP 领域提供的开放平台越来越多，它的门槛也越来越降低。</p>

<p>语音和语言其实有很大的差别，我认识的好几位国内外的进入 NLP 的学者，他们发现 NLP 很复杂，因为像语音识别和语音合成等只有有限的问题，而且这些问题定义非常清晰。但到了自然语言，要处理的问题变得纷繁复杂，尤其是 NLP 和其他的领域还会有所结合，所以问题非常琐碎。</p>

<p><strong>趋势 4：语言知识——从人工构建到自动构建</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh43Jqs2UNCJmKZu2GtqBLT5mdVia7FuhKMA6xNpKeWDDMNeyGkmf0iaicyw/640.png?" alt="" /></p>

<p>AlphaGo 告诉我们，没有围棋高手介入他的开发过程, 到 AlphaGo 最后的版本，它已经不怎么需要看棋谱了。所以 AlphaGo 在学习和使用过程中都有可能会超出人的想像，因为它并不是简单地跟人学习。</p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh43msIxqIoexKb9LtSub344I2eURu3h8JHS789bx4iaysdSMKmLZDticuw/640.png?" alt="" /></p>

<p>美国有一家<a href="https://www.baidu.com/s?wd=%E6%96%87%E8%89%BA%E5%A4%8D%E5%85%B4&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd">文艺复兴</a>公司，它做金融领域的预测，但是这个公司不招金融领域的人，只是招计算机、物理、数学领域的人。这就给了我们一个启发，计算机不是跟人的顶级高手学，而是用自己已有的算法，去直接解决问题。</p>

<p>但是在自然语言处理领域，还是要有大量的显性知识的，但是构造知识的方式也在产生变化。比如，现在我们开始用自动的方法，自动地去发现词汇与词汇之间的关系，像毛细血管一样渗透到各个方面。</p>

<p><strong>趋势 5：对话机器人——从通用到场景化</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh46EvxzHuNlTXUhULHaUgM5picTuMANE71icvwWpNvTDuVuTiaI4XTiaQiafg/640.png?" alt="" /></p>

<p>最近出现了各种图灵测试的翻版，就是做知识抢答赛来验证人工智能，从产学研应用上来讲就是对话机器人，非常有趣味性和实用价值。</p>

<p>这块的趋势在哪里？我们知道，从 Siri 刚出来，国内就开始做<a href="https://www.baidu.com/s?wd=%E8%AF%AD%E9%9F%B3%E5%8A%A9%E6%89%8B&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd">语音助手</a>了，后来语音助手很快下了马，因为它可以听得到但是听不懂，导致后面的服务跟不上。后来国内把难度降低成了聊天，你不是调戏 Siri 吗，我就做小冰就跟你聊。但是难度降低了，实用性却跟不上来，所以在用户的留存率上，还是要打个问号。</p>

<p>现在更多的做法和场景结合，降低难度，然后做任务执行，即希望做特定场景时的有用的人机对话。在做人机对话的过程中，大家热情一轮比一轮高涨，但是随后大家发现，很多问题是由于自然语言的理解没有到位，才难以产生真正的突破。</p>

<p><strong>趋势 6：文本理解与推理——从浅层分析向深度理解迈进</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4k8hlENsHFjvA5cMV24rT7YCJZXNtnqpiaxPLLggVxDfT32h0ialybqFw/640.png?" alt="" /></p>

<p>Google 等都已经推出了这样的测试机——以阅读理解作为一个深入探索自然语言理解的平台。就是说，给计算机一篇文章，让它去理解，然后人问计算机各种问题，看计算机是否能回答，这样做是很有难度的，因为答案就在这文章里面，人会很刁钻地问计算机。所以说阅读理解是现在竞争的一个很重要的点。</p>

<p><strong>趋势 7：文本情感分析——从事实性文本到情感文本</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4fWCCQhEic2cZGick6kqY2pOwiaMvRf9s0D015yIV0mU6znWo7BWnaWbTw/640.png?" alt="" /></p>

<p>多年以前，很多人都在做新闻领域的事实性文本，而如今，搞情感文本分析的似乎更受群众欢迎，这一块这在商业和政府舆情上也都有很好地应用。</p>

<p><strong>趋势 8：社会媒体处理——从传统媒体到社交媒体</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4y2fSUTeCq0n2esoHFCRFtO6nSfILZo3cZskt2nzBmmdgEsywxiadOCA/640.png?" alt="" /></p>

<p>相应的，在社会媒体处理上，从传统媒体到社交媒体的过渡，情感的影响是一方面，大家还会用社交媒体做电影票房的预测，做股票的预测等等。</p>

<p>但是从长远的角度看，社会、人文等的学科与计算机学科的结合是历史性的。比如，在文学、历史学等学科中，有相当一部分新锐学者对本门学科的计算机的大数据非常关心，这两者在碰撞，未来的前景是无限的，而自然语言处理是其中重要的、基础性的技术。</p>

<p><strong>趋势 9：文本生成——从规范文本到自由文本</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4DMOVFIpoaUia4e7AOGhPscOEG32ibzVPSjgepfiaDE6JNKfx4ibRPYudvQ/640.png?" alt="" /></p>

<p>文本生成这两年很火，从生成古诗词到生成新闻报道到再到写作文。这方面的研究价值是很大的，它的趋势是从生成规范性的文本到生成自由文本。比如，我们可以从数据库里面生成一个可以模板化的体育报道，这个模板是很规范的。然后我们可以再向自由文本过渡，比如写作文。</p>

<p><strong>趋势 10：NLP + 行业——与领域深度结合，为行业创造价值</strong></p>

<p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwHFTtg54MNxIEIq5hqKmh4rrmP7ibtNSdFpXmagGrJPM1MVD9nicNKloiaIUP0XjW8sfCvaYERD0xyA/640.png?" alt="" /></p>

<p>最后是谈与企业的合作。现在像银行、电器、医药、司法、教育、金融等的各个领域对 NLP 的需求都非常多。</p>

<p>我预测 NLP 首先是会在信息准备的充分的，并且服务方式本身就是知识和信息的领域产生突破。还比如司法领域，它的服务本身也有信息，它就会首先使用 NLP。NLP 最主要将会用在以下四个领域，医疗、金融、教育和司法。</p>

                </div>
                <div class="read-all">
                    <a  href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/"><i class="fa fa-newspaper-o"></i>Read All</a>
                </div>
                <hr>
              </li>
            
        </ul>



        <!-- Pagination links -->
        <div class="pagination">
          
            <span class="previous disable"><i class="fa fa-angle-double-left"></i></span>
            <span class="previous disable"><i class="fa fa-angle-left"></i></span>
          
          <span class="page_number ">1/6</span>
          
            <a href="/page2" class="next"><i class="fa fa-angle-right"></i></a>
            <a href="/page6" class="next"><i class="fa fa-angle-double-right"></i></a>
          
        </div>
    </div>
    <!-- <button class="anchor"><i class="fa fa-anchor"></i></button> -->
    <div class="right">
        <div class="wrap">
            <div class="side">
                <div>
                    <i class="fa fa-pencil-square-o" aria-hidden="true"></i>
                    Recent Posts
                </div>
                <ul class="content-ul" recent>
                    
                        <li><a href="/2019/04/04/Get_To_The_Point_Summarization_with_Pointer-Generator_Networks%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">Get To The Point: Summarization with Pointer-Generator Networks论文笔记</a></li>
                    
                        <li><a href="/2019/03/28/The_Evolved_Transformer_%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">《The Evolved Transformer》论文总结</a></li>
                    
                        <li><a href="/2019/03/21/Pretraining-Based-Natural-Language-Generation-for-Text-Summarization/">《Pretraining-Based Natural Language Generation for Text Summarization》论文解读</a></li>
                    
                        <li><a href="/2019/03/20/%E6%96%87%E6%91%98/">文摘</a></li>
                    
                        <li><a href="/2019/03/16/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/">chat-bot</a></li>
                    
                        <li><a href="/2019/03/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%98%AF%E4%B8%AA%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/">自然语言处理的是个发展趋势</a></li>
                    
                        <li><a href="/2019/03/13/%E5%8D%8E%E4%B8%BA%E6%9D%8E%E8%88%AA-NLP%E6%9C%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%81%9A%E7%9A%84%E5%BE%88%E5%A5%BD/">华为李航-NLP有个基本问题，深度学习4个做的很好</a></li>
                    
                        <li><a href="/2019/03/13/%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Google-Tensor2Tensor-%E7%B3%BB%E7%BB%9F/">全面解析Tensor2Tensor系统</a></li>
                    
                        <li><a href="/2019/03/13/%E4%BD%BF%E7%94%A8pytorch%E8%AF%86%E5%88%ABmnist/">使用pytorch识别mnist</a></li>
                    
                        <li><a href="/2019/03/13/pytorch%E4%B8%ADcnn%E7%9A%84%E4%BD%BF%E7%94%A8/">pytorch中cnn的使用</a></li>
                    
                </ul>
            </div>

            <!-- Content -->
            <div class="side ">
                <div>
                    <i class="fa fa-th-list"></i>
                    Categories
                </div>
                <ul class="content-ul" cate>
                    
                    <li>
                        <a href="/category/#notes" class="categories-list-item" cate="notes">
                            <span class="name">
                                notes
                            </span>
                            <span class="badge">17</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#reproduction" class="categories-list-item" cate="reproduction">
                            <span class="name">
                                reproduction
                            </span>
                            <span class="badge">11</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#digest" class="categories-list-item" cate="digest">
                            <span class="name">
                                digest
                            </span>
                            <span class="badge">1</span>
                        </a>
                    </li>
                    
                    <li>
                        <a href="/category/#NLP" class="categories-list-item" cate="NLP">
                            <span class="name">
                                NLP
                            </span>
                            <span class="badge">2</span>
                        </a>
                    </li>
                    
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <div class="side">
                <div>
                    <i class="fa fa-tags"></i>
                    Tags
                </div>
                <div class="tags-cloud">
                    
                    
                    
                    

                    

                    
                      
                      
                      
                      
                      
                      <a href="/tag/#linux" style="font-size: 11pt; color: #777;">linux</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#jekyll" style="font-size: 9pt; color: #999;">jekyll</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#paper" style="font-size: 11pt; color: #777;">paper</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#deep_learning" style="font-size: 9pt; color: #999;">deep_learning</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#python" style="font-size: 9pt; color: #999;">python</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#nlp" style="font-size: 18pt; color: #000;">nlp</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#spider" style="font-size: 10pt; color: #888;">spider</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#pytorch" style="font-size: 14pt; color: #444;">pytorch</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#markdown" style="font-size: 9pt; color: #999;">markdown</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cnn" style="font-size: 9pt; color: #999;">cnn</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#cv" style="font-size: 9pt; color: #999;">cv</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#tensorflow" style="font-size: 9pt; color: #999;">tensorflow</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#life" style="font-size: 9pt; color: #999;">life</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#Feature Extractor" style="font-size: 9pt; color: #999;">Feature Extractor</a>
                    
                      
                      
                      
                      
                      
                      <a href="/tag/#summarization" style="font-size: 9pt; color: #999;">summarization</a>
                    
                </div>
            </div>

            <!-- <div class="side">
                <div>
                    <i class="fa fa-external-link"></i>
                    Links
                </div>
                <ul  class="content-ul">

                </ul>
            </div> -->
        </div>
    </div>
</div>
<!-- <script src="/js/scroll.min.js " charset="utf-8"></script> -->
<!-- <script src="/js/pageContent.js " charset="utf-8"></script> -->


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             本站记录我NLP之旅的沿途风景！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/henryzhou1113" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
        </p>
        <p>
本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次 -->
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
